{"Job_ID":"85795244","Role":"Data Engineer","Company":"SMRT Corporation Ltd","Location":"Bishan","Publish_Time":"2025-07-16 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85795244","job_desc":"Company description:; ; SMRT Corporation Ltd is a public transport service provider. Our primary business is to manage and operate train services on the North-South Line, East-West Line, the Circle Line, the Thomson-East Coast Line and Bukit Panjang Light Rail Transit. This is complemented by our bus, taxi and private hire vehicle services.; ; Our core values are Respect, Integrity, Safety and Service, and Excellence. We are committed to provide safe, reliable and comfortable service for all our commuters.; ; ; Job description:; ; Job Purpose; The Data Engineer will be part of the team to develop operation & maintenance decision-support tools to enhance train reliability and maintenance efficiency. This position involves designing, developing, and maintaining data pipelines, APIs, and cloud infrastructure for various rail-oriented applications. The ideal candidate will have expertise in data analysis, transformation, ingestion, database design, API development, and preferably, cloud infrastructure setup. Collaborating closely with software engineers, data scientists, and frontend developers, the Data Engineer will contribute to building efficient, scalable, and reliable systems.; ; Responsibilities; The duties and responsibilities for Data Engineer, are as listed below. The list is not comprehensive and related duties and responsibilities may be assigned from time to time.; Data Engineering & Processing:; Develop and maintain data pipelines for efficient data ingestion and transformation.; Work with structured and unstructured data to ensure optimal storage and retrieval.; Perform data analysis and report on results.; Database Design & Management:; Design and implement relational and NoSQL database schemas for scalability.; Optimize database performance through indexing, partitioning, and query tuning.; Implement data security and compliance best practices.; API Development & Backend Engineering:; Design and develop APIs for data access and application integration.; Implement authentication, authorization, and API security best practices.; Cloud Infrastructure & Deployment (Supporting Role):; Assist in design Azure cloud architectures; Work with IT infrastructure team to set up cloud infrastructure for application hosting, data storage and processing.; Collaboration & Best Practices:; Collaborate with internal stakeholders to understand their business needs.; Work with software engineers, data scientist, frontend developer to understand the data requirement and design architecture of the data platform.; Implement CI\/CD pipelines for automated testing, deployment and monitoring.; Write testable and maintainable code and documentation to deploy to production.; Engage continuously with end-user for feedback and improvements.; Qualifications & Work Experience; Degree in Science, Technology, Engineering or Mathematics (STEM); Previous experience as a data engineer or in a similar role; Data engineering certification is a plus; Knowledge of security best practices in cloud and database management is a plus; Skills; Technical skills include:; Programming and Data processing: MATLAB, Python, SQL, or similar languages.; Databases: My SQL, SQL Server, MongoDB or similar.; Cloud Platforms: Azure; DevOps & CI\/CD: Git Lab CI\/CD, Docker; Generic skills include:; Strong inclination and eager for continual learning and development; Strong team player; Critical thinking and problem-solving skills; Ability to understand and explain complex data and effective interactions with the stakeholders; Ability to think independently and actively propose solutions to the team.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85818029","Role":"Data Engineer","Company":"Nearsource","Location":"West Region","Publish_Time":"2025-07-16 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85818029","job_desc":"Key Responsibilities:; Design, build, and optimize scalable data pipelines for data extraction, transformation, and loading (ETL\/ELT).; Develop and maintain data architectures, databases, and data warehouses.; Collaborate with data scientists and analysts to understand data needs and deliver clean, structured datasets.; Monitor and improve the performance of data systems.; Implement and enforce data quality, security, and governance practices.; Work with cloud platforms (e.g., AWS, Azure, GCP) for data storage and processing.; Automate data processes and workflows using orchestration tools like Airflow or similar.; Maintain documentation of data models, schemas, and systems.; Required Skills and Qualifications:; Bachelor's degree in Computer Science, Engineering, Information Technology, or related field.; Proven experience as a Data Engineer or in a similar role.; Proficient in programming languages such as Python, Scala, or Java.; Strong SQL skills and experience with relational and NoSQL databases (e.g., PostgreSQL, MySQL, MongoDB).; Hands-on experience with ETL tools and data pipeline frameworks (e.g., Apache Spark, Kafka, Airflow).; Experience with cloud data platforms like AWS (Redshift, Glue, S3), Azure (Data Factory, Synapse), or GCP (BigQuery, Dataflow).; Familiarity with data modeling, data warehousing concepts, and data lakes.; Strong problem-solving skills and attention to detail.; Preferred Qualifications:; Experience with big data technologies like Hadoop, Hive, or Presto.; Knowledge of CI\/CD and DevOps practices in data engineering.; Experience with version control tools (e.g., Git).; Understanding of data privacy and compliance standards (e.g., GDPR, HIPAA).","salary":"","work_type":"Kontrak\/Temporer","country":"singapore"}
{"Job_ID":"85817969","Role":"Data Engineer","Company":"TECHNOPALS CONSULTANTS PTE. LTD.","Location":"West Region","Publish_Time":"2025-07-16 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85817969","job_desc":"Key Responsibilities:; Design, build, and optimize scalable data pipelines for data extraction, transformation, and loading (ETL\/ELT).; Develop and maintain data architectures, databases, and data warehouses.; Collaborate with data scientists and analysts to understand data needs and deliver clean, structured datasets.; Monitor and improve the performance of data systems.; Implement and enforce data quality, security, and governance practices.; Work with cloud platforms (e.g., AWS, Azure, GCP) for data storage and processing.; Automate data processes and workflows using orchestration tools like Airflow or similar.; Maintain documentation of data models, schemas, and systems.; Required Skills and Qualifications:; Bachelor's degree in Computer Science, Engineering, Information Technology, or related field.; Proven experience as a Data Engineer or in a similar role.; Proficient in programming languages such as Python, Scala, or Java.; Strong SQL skills and experience with relational and NoSQL databases (e.g., PostgreSQL, MySQL, MongoDB).; Hands-on experience with ETL tools and data pipeline frameworks (e.g., Apache Spark, Kafka, Airflow).; Experience with cloud data platforms like AWS (Redshift, Glue, S3), Azure (Data Factory, Synapse), or GCP (BigQuery, Dataflow).; Familiarity with data modeling, data warehousing concepts, and data lakes.; Strong problem-solving skills and attention to detail.; Preferred Qualifications:; Experience with big data technologies like Hadoop, Hive, or Presto.; Knowledge of CI\/CD and DevOps practices in data engineering.; Experience with version control tools (e.g., Git).; Understanding of data privacy and compliance standards","salary":"","work_type":"Kontrak\/Temporer","country":"singapore"}
{"Job_ID":"85789645","Role":"DATA Engineer","Company":"KRISE SOLUTIONS PTE. LTD.","Location":"Central Region","Publish_Time":"2025-07-15 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85789645","job_desc":"Job Summary:; We are looking for a skilled and detail-oriented Data Engineer to join our team. The ideal candidate will be responsible for designing, building, and maintaining scalable data pipelines and infrastructure. You will work closely with data scientists, analysts, and business teams to ensure the availability, reliability, and accuracy of data.; Key Responsibilities:; Design, build, and optimize scalable data pipelines for data extraction, transformation, and loading (ETL\/ELT).; Develop and maintain data architectures, databases, and data warehouses.; Collaborate with data scientists and analysts to understand data needs and deliver clean, structured datasets.; Monitor and improve the performance of data systems.; Implement and enforce data quality, security, and governance practices.; Work with cloud platforms (e.g., AWS, Azure, GCP) for data storage and processing.; Automate data processes and workflows using orchestration tools like Airflow or similar.; Maintain documentation of data models, schemas, and systems.; Required Skills and Qualifications:; Bachelor's degree in Computer Science, Engineering, Information Technology, or related field.; Proven experience as a Data Engineer or in a similar role.; Proficient in programming languages such as Python, Scala, Java.; Strong SQL skills and experience with relational and NoSQL databases (e.g., PostgreSQL, MySQL, MongoDB).; Hands-on experience with ETL tools and data pipeline frameworks (e.g., Apache Spark, Kafka, Airflow).; Experience with cloud data platforms like AWS (Redshift, Glue, S3), Azure (Data Factory, Synapse), or GCP (BigQuery, Dataflow).; Familiarity with data modeling, data warehousing concepts, and data lakes.; Strong problem-solving skills and attention to detail.; Experience with big data technologies like Hadoop, Hive, or Presto.; Knowledge of CI\/CD and DevOps practices in data engineering.; Experience with version control tools (e.g., Git).; Understanding of data privacy and compliance standards (e.g., GDPR, HIPAA).","salary":"","work_type":"Kontrak\/Temporer, Full time","country":"singapore"}
{"Job_ID":"85742262","Role":"Big Data Engineer - TikTok","Company":"TikTok Pte. Ltd.","Location":"Central Region","Publish_Time":"2025-07-14 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85742262","job_desc":"About TikTok; TikTok is the leading destination for short-form mobile video. At TikTok, our mission is to inspire creativity and bring joy. TikTok's global headquarters are in Los Angeles and Singapore, and we also have offices in New York City, London, Dublin, Paris, Berlin, Dubai, Jakarta, Seoul, and Tokyo.; Why Join Us; Inspiring creativity is at the core of TikTok's mission. Our innovative product is built to help people authentically express themselves, discover and connect \u2013 and our global, diverse teams make that possible. Together, we create value for our communities, inspire creativity and bring joy - a mission we work towards every day.; We strive to do great things with great people. We lead with curiosity, humility, and a desire to make impact in a rapidly growing tech company. Every challenge is an opportunity to learn and innovate as one team. We're resilient and embrace challenges as they come. By constantly iterating and fostering an \"Always Day 1\" mindset, we achieve meaningful breakthroughs for ourselves, our company, and our users. When we create and grow together, the possibilities are limitless. Join us.; Diversity & Inclusion; TikTok is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives. Our platform connects people from across the globe and so does our workplace. At TikTok, our mission is to inspire creativity and bring joy. To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach. We are passionate about this and hope you are too.; Job highlights; Positive team atmosphere, Career growth opportunity, Paid leave, 100+ mil users, Meals provided; Responsibilities; Team Introduction; The mission of the Data Platform Singapore Business Partnering (DPSG BP) team is to empower the TikTok Business with data. Our goal is to build a Data Warehouse that can cater to batch and streaming data, Data Products that provide useful information to build efficient data metrics & dashboards which will be used to make smarter business decisions to support business growth. If you're looking for a challenging ground to push your limits, this is the team for you!; Responsibilities:; - Translate business requirements & end to end designs into technical implementations and responsible for building batch and real-time data warehouse.; - Manage data modeling design, writing, and optimizing ETL jobs.; - Collaborate with the business team to building data metrics based on data warehouse.; - Responsible for building and maintaining data products.; - Involvement in rollouts, upgrades, implementation, and release of data system changes as required for streamlining of internal practices.; - Develop and implement techniques and analytics applications to transform raw data into meaningful information using data-oriented programming languages and visualisation software.; - Apply data mining, data modelling, natural language processing, and machine learning to extract and analyse information from large structured and unstructured datasets.; - Visualise, interpret, and report data findings and may create dynamic data reports as well.; Qualifications; Minimum Qualifications:; - At least 5 years in software engineering and 2 years of relevant experience in data engineering.; - Proficient in creating and maintaining complex ETL pipelines end-to-end while maintaining high reliability and security.; - Familiar with data warehouse concept and have production experience in modeling design.; - Familiar with at least 1 distributed computing engine (e.g. Hive, Spark, Flink).; - Familiar with at least 1 NoSQL database is a plus (e.g. HBase).; Preferred Qualifications:; - Excellent interpersonal and communication skills with the ability to engage and manage internal and external stakeholders across all levels of seniority.; - Strong collaboration skills with the ability to build rapport across teams and stakeholders.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85843199","Role":"Backend Data Engineer","Company":"Whitehall Resources","Location":"Singapore","Publish_Time":"2025-07-17 12:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85843199","job_desc":"Backend Data Engineer; Backend Data Engineer required by Whitehall Resources for a 9-12 Month extendable project assignment for our client headquartered in Germany.; Please note, we are looking for candidates that can either work in CET or APAC time zones. Candidates based in offshore locations such as APAC are preferred.; Start Date: ASAP; Duration: 9-12 Months extendable; Delivery: Fully Remote; Capacity: Full Time; Overview:; We are seeking a skilled Backend Data Engineer to design, build, and maintain scalable data pipelines and infrastructure that support advanced analytics and business intelligence. You will work closely with data scientists, analysts, and platform teams to ensure efficient, secure, and reliable data delivery.; Key Responsibilities:; - Develop and maintain ETL\/ELT pipelines using tools such as Apache Spark, Airflow, or dbt; - Design and optimize data warehouses and data lakes (e.g., Snowflake, BigQuery, Redshift); - Integrate data from various sources including APIs, databases, and streaming platforms; - Ensure data quality, consistency, and security through validation and monitoring; - Collaborate with DevOps to automate deployments and CI\/CD workflows; Key Skills & Technologies:; - Proficient in Python, SQL, and working with relational & NoSQL databases; - Experience with cloud platforms (AWS, GCP, or Azure); - Knowledge of data modeling, partitioning, and performance tuning; - Familiarity with Kafka, Spark, or other big data technologies; - Understanding of data governance, security, and compliance (GDPR, etc.); Start Date is ASAP!; Please Apply Now!; All of our opportunities require that applicants are eligible to work in the specified country\/location, unless otherwise stated in the job description.; Whitehall Resources are an equal opportunities employer who value a diverse and inclusive working environment. All qualified applicants will receive consideration for employment without regard to race, religion, gender identity or expression, sexual orientation, national origin, pregnancy, disability, age, veteran status, or other characteristics.","salary":"","work_type":"Kontrak\/Temporer","country":"singapore"}
{"Job_ID":"85074940","Role":"Senior Application & Data Engineer","Company":"BRC GLOBAL ROLLS PTE. LTD.-","Location":"Downtown Tanjong Pagar","Publish_Time":"2025-06-21 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85074940","job_desc":"The Senior Application & Data Engineer is responsible for Identifying, designing, and implementing process improvements that include building\/re-engineering data models, data architectures, pipelines, and data applications. Continuously look for data optimization processes and oversee data management, governance, security, and analysis.; Job Responsibilities:; Lead the development and optimization of our data pipelines, databases, and systems for serving data to our customers, ensuring scalability, efficiency, and reliability.; Work in close collaboration with stakeholders and analysts to design and implement robust data models.; Drive innovation by staying updated with the latest in data engineering practices, tools, and technologies, applying them to solve complex business and data challenges.; Design, construct, install, test and maintain a highly scalable data platform.; Analyze business requirements and create conceptual, logical, and physical data models.; Design database tables, columns, and relationships, and document data flow and dependencies.; Build high-performance algorithms, prototypes, models and proof of concepts.; Develop data set processes for data modeling, mining, and production.; Integrate new data management technologies and software engineering tools into existing structures.; Research opportunities for data acquisition and new uses for existing data for reporting.; Create custom software components and analytics applications.; Collaborate with IT team members on project and technology related goals.; Job Requirements:; Degree in Computer Science\/Information Technology or equivalent data-related fields, such as data science, data engineering, data management, data governance, data analytics etc; Minimum 5 years of relevant experience in areas such as data management, engineering, extract, transfer and load data.; Strong SQL skills, on MS SQL server environment, for querying and managing data.; Proficiency in Python and SQL.; Strong understanding of object-oriented programming (OOP) and design patterns.; Proficiency in programming languages such as .NET and Python.; Experience with software development frameworks and libraries.; Familiarity with version control systems such as Git or Azure Devops.; Knowledge of software testing and debugging methodologies.; Ability to write clean, maintainable, and efficient code.; Experience with agile development methodologies.; Skills in systems problem-solving and conflict resolution.; Ability to work as part of a team, independently and make decisions.; Artificial Intelligence on LLM\/RAG knowledge will be an advantage.; Ethical and able to organize and complete tasks to expected standards and on-time.; Trustworthy and accountable to deliver quality results.; Adaptability to changing requirements and circumstances.; Strong written and verbal communication skills; Ability to manage time effectively.; Ability to travel and take on short overseas assignments on an as needed basis.","salary":"$6,000 \u2013 $9,000 per month (SGD)","work_type":"Full time","country":"singapore"}
{"Job_ID":"85632080","Role":"Principal Big Data Engineer","Company":"LUMINARY SERVICE PTE. LTD.","Location":"Singapore","Publish_Time":"2025-07-10 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85632080","job_desc":"Design and implement scalable data pipelines to collect, process, and store large volumes of structured and unstructured data.; Ensure data quality, integrity, and consistency across various data sources.; Work closely with data scientists, analysts, and other stakeholders to understand data requirements and deliver solutions that meet business needs.; Integrate data from multiple sources, including databases, APIs, and third-party data providers.; Build and maintain big data infrastructure using technologies; Optimize data storage and retrieval processes to improve performance and reduce costs.; Proven experience as a Big Data Engineer or similar role.; Experience with big data technologies such as Hadoop, Spark, Kafka, Hive, HBase, etc; Proficiency in programming languages such as Python, Java, or Scala.; Strong knowledge of SQL and NoSQL databases.; Knowledge of machine learning and data analytics.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85840938","Role":"Data Engineer - Growth","Company":"TikTok Pte. Ltd.","Location":"Singapore","Publish_Time":"2025-07-17 10:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85840938","job_desc":"Responsibilities; The Growth team plays a core role in acquisition, activation, and retention of billions of users, through our globally popular products such as TikTok, Lemon8, etc. We are building platform foundations, leveraging data and ML models, and providing end-to-end solutions to power global growth of products. You will:; - Build data pipelines to portray business status, based on a deep understanding of our fast changing business and data-driven approach; - Extract information and signals from a broad range of data and build hierarchies to accomplish analytical and mining goals for \u201cPackaged Business Capability\u201d such as user-growth, gaming and searching; - Keep improving the integrity of data pipelines to provide a comprehensive data service.; Qualifications; Minimum Qualifications\uff1a; Bachelor's degree in Computer Science, Statistic, Data Science or a related field;; Skilled in SQL and additional object-oriented programming language (e.g. Scala, Java, or Python);; Experience in issue tracking and problem solving on data pipelines;; Fast business understanding and collaborative in teamwork.; Preferred Qualification\uff1a; Industry experience working with user growth.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85703612","Role":"Big Data Engineer (Libra) - Data Platform","Company":"TikTok Pte. Ltd.","Location":"Central Region","Publish_Time":"2025-07-13 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85703612","job_desc":"About TikTok; TikTok is the leading destination for short-form mobile video. At TikTok, our mission is to inspire creativity and bring joy. TikTok's global headquarters are in Los Angeles and Singapore, and we also have offices in New York City, London, Dublin, Paris, Berlin, Dubai, Jakarta, Seoul, and Tokyo.; Why Join Us; Inspiring creativity is at the core of TikTok's mission. Our innovative product is built to help people authentically express themselves, discover and connect \u2013 and our global, diverse teams make that possible. Together, we create value for our communities, inspire creativity and bring joy - a mission we work towards every day.; We strive to do great things with great people. We lead with curiosity, humility, and a desire to make impact in a rapidly growing tech company. Every challenge is an opportunity to learn and innovate as one team. We're resilient and embrace challenges as they come. By constantly iterating and fostering an \"Always Day 1\" mindset, we achieve meaningful breakthroughs for ourselves, our company, and our users. When we create and grow together, the possibilities are limitless. Join us.; Diversity & Inclusion; TikTok is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives. Our platform connects people from across the globe and so does our workplace. At TikTok, our mission is to inspire creativity and bring joy. To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach. We are passionate about this and hope you are too.; Job highlights; Positive team atmosphere, Career growth opportunity, Meals provided; Responsibilities; About the team; Libra is a large-scale online one-stop A\/B testing platform developed by TikTok Data Platform. Some of its features include:; - Provides experimental evaluation services for all product lines within the company, covering solutions for complex scenarios such as recommendation, algorithm, function, UI, marketing, advertising, operation, social isolation, causal inference, etc.; - Provides services throughout the entire experimental lifecycle from experimental design, experimental creation, indicator calculation, statistical analysis to final evaluation launch.; - Supports the entire company's business on the road of rapid iterative trial and error, boldly assuming and carefully verifying.; Responsibilities; - Responsible for data system of experimentation platform operation and maintenance.; - Construct PB-level data warehouses, participate in and be responsible for data warehouse design, modeling, and development, etc.; - Build ETL data pipelines and automated ETL data pipeline systems.; - Build an expert system for metric data processing that combines offline and real-time processing.; Qualifications; Minimum Qualifications; - Bachelor's degree in Computer Science, a related technical field involving software or systems engineering, or equivalent practical experience.; - Proficiency with big data frameworks such as Presto, Hive, Spark, Flink, Clickhouse, Hadoop, and have experience in large-scale data processing.; - Minimum 1 year of experience in Data Engineering.; - Experience writing code in Java, Scala, SQL, Python or a similar language.; - Experience with data warehouse implementation methodologies, and have supported actual business scenarios.; Preferred Qualifications; - Knowledge about a variety of strategies for ingesting, modeling, processing, and persisting data, ETL design, job scheduling and dimensional modeling.; - Expertise in designing, analyzing, and troubleshooting large-scale distributed systems is a plus (Hadoop, M\/R, Hive, Spark, Presto, Flume, Kafka, ClickHouse, Flink or comparable solutions).; - Work\/internship experience in internet companies, and those with big data processing experience are preferred.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85784716","Role":"Data Engineer","Company":"Helius Technologies Pte Ltd","Location":"Central Region","Publish_Time":"2025-07-15 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85784716","job_desc":"\ud83d\udd0d Key Responsibilities:; 6+ Years in Design, develop, and maintain end-to-end data lakes and data warehouse solutions; Work with Microsoft Fabric, Azure Data Factory (ADF), or Informatica IDMC for data integration and orchestration; Build scalable data pipelines using Python\/PySpark; Develop and maintain data models for both transactional and analytical systems; Tune and optimize complex queries and analyze performance for large-scale datasets; Collaborate in master data management (MDM) initiatives, preferably in customer domains; (Optional) Contribute to Big Data and DataSecOps initiatives for enterprise-grade solutions; \u2705 Key Requirements:; Strong hands-on experience in Azure Data Services, ideally Microsoft Fabric; Proficiency in Python \/ PySpark scripting; Solid background in data modeling, database design, and SQL performance tuning; Experience with large datasets and real-time data environments; Exposure to MDM tools\/processes, particularly in customer data domains; Advantageous to have familiarity with Big Data tools and DataSecOps practices; Thanks, and Best Regards; Karanam Vijaya Kiran; (EA Registration no: R1443178); HP: +65 92333815; Email: vijay@helius-tech.com; Recruitment Manager; Helius Technologies Pte Ltd (EA Licence No: 11C3373)","salary":"$7,500 \u2013 $8,500 per month (SGD)","work_type":"Full time","country":"singapore"}
{"Job_ID":"85763140","Role":"Data Engineer - Contract","Company":"Manpower Staffing Services (S) Pte Ltd - Head Office","Location":"Central Region","Publish_Time":"2025-07-15 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85763140","job_desc":"About the Role; As a Senior Data Engineer, you will focus on designing, building, and maintaining scalable data solutions that support business needs and AI applications. You will play a key role in:; Data Pipeline & Architecture Development; Build and optimize automated data pipelines for ingesting, transforming, and processing large datasets.; Design efficient data architectures that support analytics, machine learning, and real-time applications.; Cloud Migration & AI Enablement; Support cloud migration efforts, transitioning on-premises data workflows to cloud-based platforms like Databricks.; Collaborate with data scientists to improve feature selection, feature engineering, and enable end-to-end AI workflows from model training to deployment and monitoring.; CI\/CD & Automation; Develop CI\/CD pipelines to streamline data pipeline deployments and ensure stable, automated workflows.; Improve monitoring and observability to maintain system reliability.; Collaboration & Business Impact; Work with data scientists, product teams, and platform engineers to align data solutions with business objectives.; Ensure data quality, security, and compliance with industry standards.; Contribute to best practices in data governance, documentation, and automation.; Qualifications & Skills; Degree holder of Information Technology, Mathematics or Statistics with least 3 years of experience in data engineering.; Expert in Python, Java, SQL, Linux Shell.; Experience in UNIX environment, Git Flow, CI\/CD automation, Jenkins, Bitbucket.; Hands-on experience with Spark, Hadoop platforms & tools (Hive, Impala, Airflow, NiFi), Spring Boot, etc. to build big data products & platforms.; Proficiency with a modern cloud or hybrid-cloud stack (AWS, Databricks, Cloudera, etc).; Experience in building and deploying production-level data-driven applications and data processing workflows or pipelines.; Interested candidates may send in their resume and cover letter directly to gem.cabria@manpower.com.sg (R1434374), stating the position as the subject title in the email.; Jireli Gem Mejia Cabria EA License No.: 02C3423 Personnel Registration No.: R1434374; Please note that your response to this advertisement and communications with us pursuant to this advertisement will constitute informed consent to the collection, use and\/or disclosure of personal data by ManpowerGroup Singapore for the purpose of carrying out its business, in compliance with the relevant provisions of the Personal Data Protection Act 2012. To learn more about ManpowerGroup's Global Privacy Policy, please visit https:\/\/www.manpower.com.sg\/privacy-policy","salary":"$8k - $10k p.m. (SGD)","work_type":"Kontrak\/Temporer","country":"singapore"}
{"Job_ID":"85435173","Role":"Big Data Engineer - TikTok","Company":"TikTok Pte. Ltd.","Location":"Singapore","Publish_Time":"2025-07-03 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85435173","job_desc":"Responsibilities; The mission of the Data Platform Singapore Business Partnering (DPSG BP) team is to empower the TikTok Business with data. Our goal is to build a Data Warehouse that can cater to batch and streaming data, Data Products that provide useful information to build efficient data metrics & dashboards which will be used to make smarter business decisions to support business growth. If you're looking for a challenging ground to push your limits, this is the team for you! Responsibilities:; Translate business requirements & end to end designs into technical implementations and responsible for building batch and real-time data warehouse.; Manage data modeling design, writing, and optimizing ETL jobs.; Collaborate with the business team to building data metrics based on data warehouse.; Responsible for building and maintaining data products.; Involvement in rollouts, upgrades, implementation, and release of data system changes as required for streamlining of internal practices.; Develop and implement techniques and analytics applications to transform raw data into meaningful information using data-oriented programming languages and visualisation software.; Apply data mining, data modelling, natural language processing, and machine learning to extract and analyse information from large structured and unstructured datasets.; Visualise, interpret, and report data findings and may create dynamic data reports as well.; Qualifications; Minimum Qualifications:; At least 5 years in software engineering and 2 years of relevant experience in data engineering.; Proficient in creating and maintaining complex ETL pipelines end-to-end while maintaining high reliability and security.; Familiar with data warehouse concept and have production experience in modeling design.; Familiar with at least 1 distributed computing engine (e.g. Hive, Spark, Flink).; Familiar with at least 1 NoSQL database is a plus (e.g. HBase).; Preferred Qualifications:; Excellent interpersonal and communication skills with the ability to engage and manage internal and external stakeholders across all levels of seniority.; Strong collaboration skills with the ability to build rapport across teams and stakeholders.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85113297","Role":"Data Engineer (Cloudera)","Company":"NCS Pte Ltd","Location":"Ang Mo Kio Town Centre","Publish_Time":"2025-06-23 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85113297","job_desc":"Company Description; NCS is a leading technology services firm that operates across the Asia Pacific region in over 20 cities, providing consulting, digital services, technology solutions, and more. We believe in harnessing the power of technology to achieve extraordinary things, creating lasting value and impact for our communities, partners, and people. Our diverse workforce of 13,000 has delivered large-scale, mission-critical, and multi-platform projects for governments and enterprises in Singapore and the APAC region.; Job Description; Responsibilities:; 1. Cloudera Private Cloud Deployment & Management:; Install, configure, and maintain Cloudera Private Cloud (CDP) clusters.; Manage cluster services (HDFS, YARN, Spark, Hive, Impala, Kafka, etc.).; Monitor cluster health, performance, and capacity planning.; 2. Security & Access Control:; Configure and manage Kerberos, LDAP\/Active Directory integration, and TLS\/SSL encryption. Implement Role-Based Access Control (RBAC) for users and services.; Ensure compliance with security best practices and audit logging.; 3. Performance Tuning & Troubleshooting:; Optimize cluster performance by tuning configurations (YARN, HDFS, Spark).; Diagnose and resolve issues related to storage, compute, and networking.; Analyze logs and metrics using Cloudera Manager and other monitoring tools.; 4. Data Processing & Optimization:; Develop, optimize, and maintain Spark, Hive, Impala, Nifi, and Kafka applications.; Write efficient SQL queries and optimize Hive\/Impala performance.; Build data pipelines for batch and real-time processing; Requirements:; Must-Have Skills:; Linux & Command Line Proficiency: o Strong experience with Linux commands (ls, grep, awk, sed, chmod, ssh).; Managing file permissions, ownership, and shell scripting (Bash).; Programming Languages:; Python (for scripting and automation).; SQL (for database queries and management).; Good-to-Have Skills:; Programming Languages:; Java\/Scala (nice to have for application support).; Networking & Security:; Understanding of TLS\/SSL, certificates, and data-in-transit encryption.; Knowledge of networking protocols (HTTPS, DNS, NTP) and port management.; Familiarity with Microsoft Active Directory and LDAP authentication.; Scripting & Automation:; Experience with Ansible\/Chef\/Puppet for infrastructure automation. o Knowledge of GitLab\/Bamboo for DevOps and CI\/CD pipelines.; Academic Qualifications:; 1. Bachelor\u2019s Degree in:; Computer Science \/ Information Technology; Software Engineering; Data Analytics \/ Big Data Technologies.; OR; Diploma in:; Computer Science \/ Information Technology; Software Engineering o Data Analytics \/ Big Data Technologies; We are driven by our AEIOU beliefs\u2014Adventure, Excellence, Integrity, Ownership, and Unity\u2014and we seek individuals who embody these values in both their professional and personal lives. We are committed to our Impact: Valuing our clients, Growing our people, and Creating our future.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85510030","Role":"Lead Big Data Engineer","Company":"TECH-HIRE (S) PTE. LTD.","Location":"Kampong Ubi","Publish_Time":"2025-07-05 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85510030","job_desc":"Experience : 10+ years; (SQL Server \/ Oracle \/ DB2 \/ Netezza) \u2013 at-least good working knowledge in 2 of these DBApache Spark Streaming or Apache Flink; Kafka; NOSQL databases - Cosmos DB, Document DB; Spark,Dataframe API; Hive (HQL); Scripting language \u2013 Shell or bash; CI CD; Experience with at least one Cloud Infra provider (Azure\/AWS); Good to have Skills :; Certifications related to Data and Analytics","salary":"","work_type":"Kontrak\/Temporer, Full time","country":"singapore"}
{"Job_ID":"85271772","Role":"Big Data Development Engineer","Company":"Tencent International Service Pte. Ltd.","Location":"Singapore","Publish_Time":"2025-06-30 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85271772","job_desc":"A bachelor's degree or above, with more than 2 years of data development experience.; Proficient in big data processing systems, familiar with big data technologies and distributed computing, such as Hadoop, Spark, Flink, Clickhouse, etc.; Strong coding ability, familiar with at least one programming language, such as Scala, C++, Go, or Java.; Candidates with data development experience in advertising, recommendation, and search are preferred.; Be proactive, have strong work enthusiasm, stress resistance, and communication skills, as well as good learning ability and problem analysis and solving skills.; Possess bilingual ability (English\/Chinese) to interact with stakeholders from HQ and the international markets","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85034742","Role":"Junior Data Engineer (Python \/ JAVA \/ SQL)","Company":"Vanguard Software Pte Ltd","Location":"Central Region","Publish_Time":"2025-06-19 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85034742","job_desc":"JOB SUMMARY:; We\u2019re looking for a Data Engineer to join our growing data team. This role is open to fresh graduates and early-career professionals (1\u20132 years of experience) who are excited about building data pipelines, transforming raw data into meaningful insights, and working with modern data technologies. You'll collaborate with data analysts, software engineers, and product teams to ensure that data flows smoothly across our systems and is reliable, secure, and accessible.; Whether you\u2019re just starting out or already have some experience, this is a great opportunity to develop your data engineering skills and contribute to impactful, data-driven decision-making.; ; KEY RESPONSIBILITIES:; Design, develop, and maintain scalable data pipelines and ETL\/ELT workflows; Clean, transform, and optimize raw data for storage and analysis; Work with structured and unstructured data from various sources (databases, APIs, files, etc.); Ensure data quality, accuracy, consistency, and availability; Support data infrastructure (e.g., data lakes, data warehouses) and performance tuning; Collaborate with analysts, data scientists, and backend teams; Document data models, processes, and technical decisions; What You\u2019ll Learn; Real-world data engineering with modern tools like Apache Spark\/Flink, Kafka, Airflow; Working with SQL\/NoSQL databases, data lakes, and cloud platforms (AWS, GCP, Azure); Building batch and streaming data pipelines; Data modeling, warehousing; Orchestration and monitoring of data workflows; Best practices in data governance, privacy, and security; Collaboration in agile, cross-functional teams with product, engineering, and analytics; JOB REQUIREMENTS:; \ud83c\udf93For Fresh Graduates; Bachelor\u2019s degree in Computer Science, Data Engineering, Software Engineering, or a related field (or graduating soon); Understanding of SQL and at least one programming language (Python preferred); Exposure to data concepts through coursework, internships, or projects; Eagerness to work with large datasets and cloud-based data platforms; Willingness to learn new tools and follow team best practices; \ud83d\udcbc For 1\u20132 Years Experience; 1\u20132 years of experience in data engineering, backend development, or analytics engineering; Proficient in SQL and Python; Familiar with ETL tools, data pipeline design, and version control (Git); Experience with cloud services (e.g., S3, Lambda, Cloud Functions, or GCP Dataflow); Able to troubleshoot data issues and build scalable data solutions; Nice to Have (For All Levels); Experience with data orchestration tools (Airflow, Prefect, Dagster, etc.); Familiarity with big data tools (Spark, Kafka, Hadoop); Exposure to data visualization tools (e.g., Looker, Tableau); Understanding of CI\/CD, containerization (Docker), and infrastructure-as-code; Contributions to personal or open-source data projects; Knowledge of data privacy and compliance (GDPR, HIPAA, etc.); Soft Skills; Analytical mindset and strong attention to detail; Team player with good communication skills; Open to feedback and continuous improvement; Responsible and proactive in solving data challenges; Eagerness to explore new tools and share knowledge; What We Offer; Structured onboarding and mentorship to grow your data skills; Opportunities to work on real-world data systems with production impact; A collaborative, knowledge-sharing team culture; Clear growth paths toward analytics engineering, senior data engineering, or data platform roles","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85182251","Role":"Big Data Engineer, Data Ingestion","Company":"TikTok Pte. Ltd.","Location":"Singapore","Publish_Time":"2025-06-26 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85182251","job_desc":"Responsibilities; About the Team The Data Management Suite team is building products that cover the whole lifecycle of data pipeline, including data ingestion and Integration, data development, data catalog, data security and data governance. These products support various businesses, so data engineers and data scientists could greatly boost their productivity.; As a software engineer in the data management suite team, you will have the opportunity to build, optimize and grow one of the largest data platforms in the world. You'll have the opportunity to gain hands-on experience on core systems in the data platform ecosystem. Your work will have a direct and huge impact on the company's core products as well as hundreds of millions of users.; Responsibilities:; Design, develop, and optimize the architecture of large-scale data ingestion systems to support real-time data pipelines, ensuring high throughput, low latency, and fault tolerance.; Enhance the performance, scalability, and reliability of data ingestion pipelines.; Develop and implement automated and intelligent operation and maintenance systems to monitor, diagnose, and ensure the stability and reliability of data ingestion pipelines.; Collaborate with cross-functional teams to deliver event data ingestion, transformation, and storage solutions that meet diverse business requirements.; Troubleshoot and resolve complex issues in production systems, ensuring minimal downtime and optimal performance.; Qualifications; Minimum Qualifications:; Bachelor's degree in Computer Science or equivalent practical experience;; 2+ years of experience in software development with proficiency in one or more programming languages, such as Java, Scala, Python, or; Go.; Strong understanding of data structures, algorithms, and distributed systems principles.; Hands-on experience with big data technologies, such as Hadoop, Flink, Kafka, or similar frameworks.; Preferred Qualifications:; 2+ years of experience in real-time data processing frameworks like Flink.; Contributions to open-source projects are a plus.; Experience in the event tracking domain, including event collection, real-time processing, governance, quality assurance and cost optimization.; Deep understanding of data lakehouse architectures and the integration of stream and batch processing.; Strong problem-solving skills and the ability to work in a fast-paced, collaborative environment.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85155271","Role":"Big Data Engineer - TikTok","Company":"TikTok Pte. Ltd.","Location":"Singapore","Publish_Time":"2025-06-17 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85155271","job_desc":"Responsibilities; TikTok is the leading destination for short-form mobile video. Our mission is to inspire creativity and bring joy. TikTok has global offices including Los Angeles, New York, London, Paris, Berlin, Dubai, Singapore, Jakarta, Seoul and Tokyo. Why Join Us Creation is the core of TikTok's purpose. Our platform is built to help imaginations thrive. This is doubly true of the teams that make TikTok possible. Together, we inspire creativity and bring joy; a mission we all believe in and aim towards achieving every day.; To us, every challenge, no matter how difficult, is an opportunity; to learn, to innovate, and to grow as one team. Status quo? Never. Courage? Always. At TikTok, we create together and grow together. That's how we drive impact; for ourselves, our company, and the communities we serve.; Join us. Team Introduction The mission of the Data Platform Singapore Business Partnering (DPSG BP) team is to empower the TikTok Business with data. Our goal is to build a Data Warehouse that can cater to batch and streaming data, Data Products that provide useful information to build efficient data metrics & dashboards which will be used to make smarter business decisions to support business growth. If you're looking for a challenging ground to push your limits, this is the team for you!; Translate business requirements & end to end designs into technical implementations and responsible for building batch and real-time data warehouse; Manage data modeling design, writing, and optimizing ETL jobs; Collaborate with the business team to building data metrics based on data warehouse; Responsible for building and maintaining data products; Involvement in rollouts, upgrades, implementation, and release of data system changes as required for streamlining of internal practices; Develop and implement techniques and analytics applications to transform raw data into meaningful information using data-oriented programming languages and visualisation software.; Apply data mining, data modelling, natural language processing, and machine learning to extract and analyse information from large structured and unstructured datasets.; Visualise, interpret, and report data findings and may create dynamic data reports as well.; Qualifications; At least 5 years in software engineering and 2 years of relevant experience in data engineering; Proficient in creating and maintaining complex ETL pipelines end-to-end while maintaining high reliability and security; Familiar with data warehouse concept and have production experience in modeling design; Familiar with at least 1 distributed computing engine (e.g. Hive, Spark, Flink); Familiar with at least 1 NoSQL database is a plus (e.g. HBase); Excellent interpersonal and communication skills with the ability to engage and manage internal and external stakeholders across all levels of seniority; Strong collaboration skills with the ability to build rapport across teams and stakeholders; TikTok is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives. Our platform connects people from across the globe and so does our workplace. At TikTok, our mission is to inspire creativity and bring joy. To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach. We are passionate about this and hope you are too.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85838975","Role":"Data Engineer - ETL (MOH ITDG)","Company":"Synapxe","Location":"One North","Publish_Time":"2025-07-17 09:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85838975","job_desc":"Company description:; ; Synapxe is the national HealthTech agency inspiring tomorrow's health. The nexus of HealthTech, we connect people and systems to power a healthier Singapore.; ; Together with partners, we create intelligent technological solutions to improve the health of millions of people every day, everywhere. Reimagine the future of health together with us at www.synapxe.sg; ; ; Job description:; ; Position Overview Role & Responsibilities; Develop TRUST data strategy:; Work with stakeholders to understand data analytics needs, data structure requirements (both in terms of scalability and accessibility), and translate this into a coherent near to long term data strategy for TRUST; Support translation of data business needs into technical system requirements for MCDR, in terms of collection, storage, batch -time processing, as well as analysis of information from structured and unstructured sources in a scalable, repeatable, and secure manner; Identify opportunities for improvements and optimisation e.g., Implement best practices and performance optimization on Big Data and Cloud to achieve the best data engineering outcomes; Oversee data preparation and data provisioning for TRUST:; Collaborate with data engineers to organise and prepare anonymised datasets in MCDR according to TRUST standards, and then providing the data in accordance with the approved TRUST Data Request. This involves working with the data engineers closely to ensure that the datasets meet the required standards and are made available as per the specific data request guidelines set by TRUST; Oversee implementation of common data model and data quality programme in TRUST and MCDR; Work with data analysts, data scientists, clinicians and other stakeholders to implement common data models to support analytics use cases; Design and implement tools to enhance the data strategy and enable seamless integration with the data, potentially leveraging API calls for efficient integration; Implement data management standards and practices; Requirements; Degree\/master's in computer science, Information Technology, Computer Engineering or equivalent; At least ten (10) years of relevant working experience in Data management \/ Integration \/ Modelling the data warehouse or advanced analytics solutions; Demonstrate good, in-depth knowledge in relevant Extract-Transform-Load (ETL) hardware\/software products, frameworks, and methodologies; Experience in designing and implementing cloud-based data solutions using cloud platforms (e.g., AWS cloud native tools); Databases (e.g., Oracle, MS SQL, MySQL, Teradata); Big data (e.g., Hadoop ecosystem); ETL development using ETL tools (e.g., Informatica, IBM DataStage, Talend); Data repository design (e.g., operational data stores, dimensional data stores, data marts); Experience in interacting with analytics stakeholders (economists, statisticians, clinicians, policy makers) on a business or domain level; Comfortable working independently to carry out data analysis, estimate data quality and sufficiency; Good interpersonal skills, a detail-oriented & flexible person who can work across different areas within the team; The following will be preferred: Some understanding of Singapore Healthcare System and healthcare data governance, management; and\/or familiarity with health informatics; Apply Now; NOTE: It only takes a few minutes to apply for a meaningful career in HealthTech - GO FOR IT!!; #LI-SYNX40","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85154949","Role":"Big Data Development Engineer (Tea) - Data Platform","Company":"TikTok Pte. Ltd.","Location":"Singapore","Publish_Time":"2025-06-17 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85154949","job_desc":"Responsibilities; About the team: Tea(Toutiao Event Analyze) is an in-house analytics platform that is heavily used by more than 10,000 employees globally. This is a data middle platform product built on user behavior data, providing data analysis services for various domestic and international businesses within ByteDance\/TikTok, including Douyin, Toutiao, Xigua, TikTok, and more. It possesses robust capabilities to support exabyte-level massive data, trillion-level event volume, and millisecond-level response time, offering users simple, flexible, and high-performance data analysis services.; We are the global engineering team for Tea. We are passionate about building the best data analytics platform in the world and are looking for top-notch software engineers to join the talented team. We deliver Tea products to over ten regions worldwide, optimizing product performance based on the unique data and cultural characteristics of each region.; Responsibilities:; Responsible for the deployment, configuration, monitoring, and maintenance of big data platforms.; Implement and optimize the infrastructure for big data storage, processing, and computation.; Responsible for architectural design, performance tuning, and troubleshooting of big data platforms.; Analyze and resolve complex system performance and stability issues to ensure system reliability and stability.; Write technical documentation to record system configurations and operational procedures.; Update software, enhances existing software capabilities and develops and direct software testing and validation procedures.; Qualifications; Minimum Qualifications:; Bachelor's Degree in Computer Science or related discipline with experience in software engineering, with 2 years of relevant experience.; 2 years experience in big data platform operations, familiarity with technologies such as Hadoop, Spark, Clickhouse, etc.; Solid programming skills, proficient in at least one programming language (e.g., Java, Scala, Python).; Strong foundation in computer science, familiarity with operating system principles, data structures, and algorithms.; Compliance Requirement: Familiarity with and adherence to international data protection regulations; ability to formulate and execute compliance-oriented strategies.; Global Multi-Environment Deployment and Operations: Experience in deploying and maintaining big data platforms across multiple global locations; familiarity with cross-geographical operational challenges.; Preferred Qualifications:; Excellent teamwork and communication skills, experience in project management.; Strong problem analysis and resolution skills, ability to respond quickly to emergencies.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"75168122","Role":"Senior\/ Data Engineer - DSC\/EZ","Company":"ST Engineering Mission Software & Services Pte Ltd","Location":"North-East Region","Publish_Time":"2025-07-06 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/75168122","job_desc":"About our Line of Business \u2013 Mission Software & Services; Our Mission Software & Services business provides leading-edge mission critical command, control, and communications (C3) systems with secured IT infrastructure and managed services. We support our client\u2019s innovation journey through design thinking, analytics, and AI-enabled decision support with our full suite of cloud computing solutions. We provide intelligent, actionable insights and sustainable solutions to our valued partners in diverse industries including defence, government, and commercial sectors.; ; Together, We Can Make A Significant Impact; The Data Engineer supports the design, implementation and maintenance of data flow channels and data processing systems that support the collection, storage, batch and real-time processing, and analysis of information in a scalable, repeatable and secure manner. The Data Engineer focuses on defining optimal solutions to data collection, processing and warehousing. He \/ She designs, codes and tests data systems and works on implementing those into the internal infrastructure.; Be Part of Our Success; Work with stakeholders including customers, partners and colleagues on data-related technical issues and support their data infrastructure needs;; Work closely with data scientists to solicit data requirements to support modeling works; Design, develop, document, manage and maintain data models, ETL processes, data warehouse, data management and pipeline solutions for large volume of structured\/unstructured data from disparate sources and with different latencies (e.g. on-demand, batch, real-time, near-real-time);; Define, monitor and report SLAs for data pipelines and data products;; Understand data security and governance standards or requirements to implement solutions that ensure adherence to these standards or meet such requirements;; Drive\/execute data quality assurance practices; and; Support data management solutions pre-sales initiatives, proposal development and provide post-sales support.; Qualities We Value; In-depth technical knowledge in:; Data Modelling;; Data Pipelines;; OLAP;; Data Ingestion & Integration techonlogies;; Query Optimisation; Technical expertise in:; Java, C\/C++, Python, Scala, SQL etc.; Big data technologies e.g. Hadoop, Spark, Hive, HBase etc.; Experience in master data management, data governance, data lifecycle management etc.;; Experience in designing, documenting, implementing and supporting data management solutions;; Experience in using software engineering best practices in development, programming, testing, version control etc.;; Knowledge of data privacy and security assurance;","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85155117","Role":"Big Data Engineer (Libra) - Data Platform","Company":"TikTok Pte. Ltd.","Location":"Singapore","Publish_Time":"2025-06-17 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85155117","job_desc":"Responsibilities; Libra is a large-scale online one-stop A\/B testing platform developed by TikTok Data Platform. Some of its features include:; Provides experimental evaluation services for all product lines within the company, covering solutions for complex scenarios such as recommendation, algorithm, function, UI, marketing, advertising, operation, social isolation, causal inference, etc.; Provides services throughout the entire experimental lifecycle from experimental design, experimental creation, indicator calculation, statistical analysis to final evaluation launch.; Supports the entire company's business on the road of rapid iterative trial and error, boldly assuming and carefully verifying.; Responsibilities; Responsible for data system of experimentation platform operation and maintenance.; Construct PB-level data warehouses, participate in and be responsible for data warehouse design, modeling, and development, etc.; Build ETL data pipelines and automated ETL data pipeline systems.; Build an expert system for metric data processing that combines offline and real-time processing.; Qualifications; Minimum Qualifications; Bachelor's degree in Computer Science, a related technical field involving software or systems engineering, or equivalent practical experience.; Proficiency with big data frameworks such as Presto, Hive, Spark, Flink, Clickhouse, Hadoop, and have experience in large-scale data processing.; Minimum 1 year of experience in Data Engineering.; Experience writing code in Java, Scala, SQL, Python or a similar language.; Experience with data warehouse implementation methodologies, and have supported actual business scenarios.; Preferred Qualifications; Knowledge about a variety of strategies for ingesting, modeling, processing, and persisting data, ETL design, job scheduling and dimensional modeling.; Expertise in designing, analyzing, and troubleshooting large-scale distributed systems is a plus (Hadoop, M\/R, Hive, Spark, Presto, Flume, Kafka, ClickHouse, Flink or comparable solutions).; Work\/internship experience in internet companies, and those with big data processing experience are preferred.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85632965","Role":"Data Engineer","Company":"MOURI TECH PTE. LTD.","Location":"North Region","Publish_Time":"2025-07-10 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85632965","job_desc":"You will be responsible for implementing advanced data transformation techniques, building analytical applications, and applying machine learning and natural language processing (NLP) to support business intelligence and decision-making.; \u2022 Bachelor\u2019s or Master\u2019s degree in Computer Science, Data Engineering, Information Systems, or a related field; \u2022 Proficiency in SQL, Python, or R for data transformation and analysis. Apply data mining, data modeling, machine learning, and NLP techniques on large structured and unstructured datasets.; \u2022 Experience with Databricks and\/or Azure data services (e.g., Azure Data Lake, Synapse, ADF). Design and maintain data workflows and orchestration using platforms like Azure Data Factory, Apache Airflow, or similar.; \u2022 Hands-on experience using dbt for data modeling and transformation. Build scalable ETL\/ELT pipelines using tools such as dbt, Databricks, or similar platforms; \u2022 Experience developing dashboards and reports using Qlik Sense, Power BI, or Tableau; \u2022 Solid understanding of data warehousing, data modeling, and performance optimization; \u2022 Collaborate with cross-functional teams to understand business needs and translate them into technical solutions; \u2022 Strong problem-solving and analytical skills","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85306740","Role":"Data Engineer","Company":"Singapore LNG Corporation Pte Ltd","Location":"Telok Blangah","Publish_Time":"2025-07-01 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85306740","job_desc":"Company description:; ; Singapore LNG Corporation Pte Ltd (SLNG) was incorporated by the Energy Market Authority of Singapore in June 2009 to build, own and operate Singapore's very first open-access, multi-user LNG Terminal. This is a key national infrastructure that supports Singapore's energy diversification strategy and future economic development in the energy sector.; ; ; With more than 95% of electricity in Singapore being generated using natural gas, the SLNG Terminal helps to enhance the country's energy security by enabling natural gas to be shipped to Singapore from anywhere in the world. It also serves as a platform to facilitate the development of new LNG-related businesses, thereby contributing to the growth of Singapore's energy industry and the creation of new job opportunities.; ; ; Job description:; ; Roles and Responsibilties; Build and maintain scalable and reliable analytical solutions and dashboards to support commercial operations, throughput monitoring, and performance analysis.; Design, develop, and maintain scalable data pipelines, ETL processes, and data integration frameworks from commercial, operational, and external sources.; Collaborate with Commercial and Operations teams to model, analyse, and forecast terminal usage, cargo schedules, and system capacity.; Support the development of real-time and batch analytics dashboards for terminal throughput, slot utilisation, and performance benchmarking.; Implement and maintain data quality and validation frameworks to ensure high-integrity inputs into business-critical decisions.; Optimise data structures for simulation tools, commercial scenario modelling, and throughput planning systems.; Assist in the development of predictive models and optimisation algorithms in collaboration with data scientists and business analysts.; Keep up with emerging technologies and recommend tools or solutions to improve data analytics capabilities.; Database Management; Administer and maintain Wallix MSSQL databases, including writing complex queries, stored procedures, indexing, and performance tuning.; Support data integration efforts during the terminal expansion project, including real-time and batch data processing.; Automation & Scheduling; Develop scripts and automation workflows (e.g., via SQL Agent, Python, or PowerShell) to streamline data loading and quality checks.; Troubleshoot data issues and optimize data workflows to enhance system performance and reliability.; Data Quality & Governance; Implement processes to validate, monitor, and improve data accuracy, completeness, and consistency across all sources.; Develop and maintain documentation for data flows, architecture, and metadata. Implement data best practices within the team and ensure compliance with data governance and cybersecurity policies.; Work Requirements; 3+ years of experience in a data engineering or data-intensive role, preferably in energy, utilities, shipping, or logistics.; Proven experience as a Data Engineer, preferably within the energy, LNG, or industrial sector.; Strong proficiency in SQL, Python, or similar programming languages for data processing.; Experience with data pipeline and workflow management tools.; Familiarity with cloud platforms and data warehousing solutions.; Technical Skills:; Proficient in SQL, Python, and modern ETL tools (e.g., Airflow, DBT).; Experience with database MSSQL management.; Familiarity with data modelling, API integration, and streaming technologies (Kafka, Spark, etc.) is advantageous.; Experience working with visualisation tools like Power BI, Tableau, or similar.; Familiarity with commercial or operational systems such as ERP, SCADA, TMS, or LNG scheduling tools is a plus.; Experience in machine learning techniques and frameworks (e.g., scikit-learn, TensorFlow, PyTorch) to develop predictive models and advanced analytics is desirable; Soft Skills:; Strong analytical mindset with excellent problem-solving abilities.; Ability to work cross-functionally and communicate effectively with both technical and non-technical stakeholders.; Comfortable in a fast-paced, evolving environment where innovation and initiative are valued.; Understanding of commercial operations and throughput metrics in terminal or energy infrastructure is advantageous.; Strong communication skills to translate technical concepts for non-technical stakeholders.; Education Requirements; Bachelor's degree in Computer Science, Data Engineering, Information Systems, or related discipline.; Why Join SLNG?; An opportunity to contribute to Singapore's energy security and the region's LNG ecosystem.; Work on high-impact projects at the intersection of commercial strategy and operational excellence.; Collaborative and innovative work culture with professional development opportunities.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85481336","Role":"Data Engineer - Financial Services","Company":"Bounteousxaccolite Singapore Pte. Ltd.","Location":"Central Region","Publish_Time":"2025-07-04 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85481336","job_desc":"Bounteous is a premier end-to-end digital transformation consultancy dedicated to partnering with ambitious brands to create digital solutions for today\u2019s complex challenges and tomorrow\u2019s opportunities. With uncompromising standards for technical and domain expertise, we deliver innovative and strategic solutions in Strategy, Analytics, Digital Engineering, Cloud, Data & AI, Experience Design, and Marketing.; One of our key Financial Services clients is looking for a Lead Data Engineer for a senior position based in Singapore.; ; Job Description; We seek individuals with highly developed conceptual, strategic, and analytical skills, capable of striking a balance between visionary thinking and practical solutions. The ability to comprehend, inspire, and mobilize others is crucial. A business-oriented mindset coupled with effective storytelling will drive your success. We are looking for self-starters ready to take on responsibilities with enthusiasm.; Your Role;  As a Lead Data Engineer, you will play a leading role in designing, building, and optimizing our data infrastructure, ensuring that it supports the advanced analytics need of the bank. You will oversee a team of data engineers, working closely with data analysts, DevOps team, infrastructure engineers, and other stakeholders to deliver high-quality data solution. ; Your main responsibilities will include:; Design, develop, and implement Spark Scala applications and data processing pipelines to process large volumes of structured and unstructured data.; Integrate Elasticsearch with Spark to enable efficient indexing, querying, and retrieval of data.; Optimize and tune Spark jobs for performance and scalability, ensuring efficient data processing and indexing in Elasticsearch.; Collaborate with data engineers, data scientists, and other stakeholders to understand requirements and translate them into technical specifications and solutions.; Design and deploy data engineering solutions on OpenShift Container Platform (OCP) using containerization and orchestration techniques.; Optimize data engineering workflows for containerized deployment and efficient resource utilization.; Collaborate with DevOps teams to streamline deployment processes, implement CI\/CD pipelines, and ensure platform stability.; Monitor and optimize data pipeline performance, troubleshoot issues, and implement necessary enhancements.; Implement monitoring and logging mechanisms to ensure the health, availability, and performance of the data infrastructure.; Document data engineering processes, workflows, and infrastructure configurations for knowledge sharing and reference.; Stay updated with emerging technologies, industry trends, and best practices in data engineering and DevOps.; Provide technical leadership, mentorship, and guidance to junior team members to foster a culture of continuous learning and innovation to the continuous improvement of the analytics capabilities within the bank.; Requirements; Bachelor\u2019s degree in computer science, Data Engineering, Information Technology, or a related field.; At least 10 years of experience as a Data Engineer, working with Hadoop, Spark, and data processing technologies in large-scale environments.; Strong expertise in designing and developing data infrastructure using Hadoop, Spark, and related tools (HDFS, Hive, Ranger, etc); Experience with containerization platforms such as OpenShift Container Platform (OCP) and container orchestration using Kubernetes.; Proficiency in programming languages commonly used in data engineering, such as Spark, Python, Scala, or Java.; Knowledge of DevOps practices, CI\/CD pipelines, and infrastructure automation tools (e.g., Docker, Jenkins, Ansible, BitBucket); Experience with Grafana, Prometheus, Splunk will be an added benefit; Strong problem-solving and troubleshooting skills with a proactive approach to resolving technical challenges.; Excellent collaboration and communication skills to work effectively with cross-functional teams.; Ability to manage multiple priorities, meet deadlines, and deliver high-quality results in a fast-paced environment.; Experience with cloud platforms (e.g., AWS, Azure, GCP) and their data services is a plus.;  What's on Offer; Bounteous brings together 5000+ employees spanning North America, APAC, and EMEA, and partnerships with leading technology providers. Through advanced digital engineering, technology solutions, and data-driven digital experiences, we create exceptional and efficient business impact and help our clients win.; Bounteous is focused on promoting an inclusive environment and is proud to be an equal opportunity employer. We celebrate the different viewpoints and experiences our diverse group of team members bring to Bounteous. Bounteous does not discriminate on the basis of race, religion, colour, gender identity, sexual orientation, age, physical or mental disability, national origin, or any other status protected under state, or local law.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85009910","Role":"Data Engineer","Company":"Keppel Management Ltd","Location":"Central Region","Publish_Time":"2025-06-18 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85009910","job_desc":"Job Description; Develop, maintain scalable data pipelines and build out new integrations to support continuing increases in data volume and complexity; Develop and maintain scalable, optimized data pipelines leveraging Python and AWS services to support increasing data volume and complexity, while ensuring seamless integration with AI platforms like Bedrock and Google. Further enhance data accessibility and drive data-driven decision making by collaborating with analytics and business teams to refine data models for business intelligence tools; Develop, maintain, and optimize scalable data pipelines using Python and AWS services (e.g., S3, Lambda, ECS, EKS, RDS, SNS\/SQS, Vector DB); Rapidly developing next-generation scalable, flexible, and high-performance data pipelines; Collaborate with analytics and business teams to create and improve data models for business intelligence; End-to-end ownership of data quality in our core datasets and data pipelines; Participate in code reviews and contribute to DevOps \/ DataOps \/ MLOps; Job Requirements; Bachelor's degree in Computer Science, Engineering, or a related field; 2-3 years of experience in data engineering or a similar role; Strong programming skills in Python, SQL, AWS and related tech stack; Experience with building scalable data pipelines with technologies such as Glue, Airflow, Kafka, Spark etc.; Experience using Snowflake, DBT, Bedrock is a plus; Good understanding of basic machine learning concepts (Sagemaker)","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85155164","Role":"Big Data Engineer - TikTok Recommendation Architecture","Company":"TikTok Pte. Ltd.","Location":"Singapore","Publish_Time":"2025-06-17 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85155164","job_desc":"Responsibilities; TikTok is the leading destination for short-form mobile video. Our mission is to inspire creativity and bring joy. TikTok has global offices including Los Angeles, New York, London, Paris, Berlin, Dubai, Singapore, Jakarta, Seoul and Tokyo. Why Join Us Creation is the core of TikTok's purpose. Our platform is built to help imaginations thrive. This is doubly true of the teams that make TikTok possible. Together, we inspire creativity and bring joy; a mission we all believe in and aim towards achieving every day.; To us, every challenge, no matter how difficult, is an opportunity; to learn, to innovate, and to grow as one team. Status quo? Never. Courage? Always. At TikTok, we create together and grow together. That's how we drive impact; for ourselves, our company, and the communities we serve.; Join us. About The Team; Our Recommendation Architecture Team is responsible for building up and optimizing the architecture for our recommendation system to provide the most stable and best experience for our TikTok users. The team is responsible for system stability and high availability, online services and offline data flow performance optimization, solving system bottlenecks, reducing cost overhead, building data and service mid-platform, realizing flexible and scalable high-performance storage and computing systems. Responsibilities; Design and implement a reasonable offline data architecture for large-scale recommendation systems; Design and implement flexible, scalable, stable and high-performance storage and computing systems; Trouble-shooting of the production system, design and implement the necessary mechanisms and tools to ensure the stability of the overall operation of the production system; Build industry-leading distributed systems such as storage and computing to provide reliable infrastructure for massive data and large-scale business systems; Develop and implement techniques and analytics applications to transform raw data into meaningful information using data-oriented programming languages and visualisation software; Apply data mining, data modelling, natural language processing, and machine learning to extract and analyse information from large structured and unstructured datasets; Visualise, interpret, and report data findings and may create dynamic data reports as well; Qualifications; 1. Bachelor's degree or above in computer science, software engineering, or a related field 2. Familiar with many open source frameworks in the field of big data, e.g. Hadoop, Hive,Flink, FlinkSQL,Spark, Kafka, HBase, Redis, RocksDB, ElasticSearch etc. 3. Familiar with Java, C ++ and other programming languages; 4. Strong coding and trouble shooting ability 5. Willing to challenge questions that have no obvious answers, and have a strong enthusiasm for learning new technologies 6. Experience of Peta Byte level data processing is a plus; 7. At least 3 years of relevant experience TikTok is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives. Our platform connects people from across the globe and so does our workplace.; At TikTok, our mission is to inspire creativity and bring joy. To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach. We are passionate about this and hope you are too.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85150830","Role":"Data Engineer","Company":"NEO GARDEN CATERING PTE. LTD.","Location":"Boon Lay","Publish_Time":"2025-06-25 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85150830","job_desc":"We\u2019re looking for a hands-on Data Engineer to lead the transformation of our data landscape. In this role, you\u2019ll drive the migration to Microsoft Fabric, improve data quality in our ERP system, and build robust data architecture to support advanced analytics and future AI initiatives. You\u2019ll work closely with cross-functional teams to enhance reporting, optimize sales operations, and shape our data-driven strategy.; ; Role Summary; Serve as primary data professional responsible for transforming existing data landscape into robust, scalable architecture; Address critical data quality challenges within current ERP system; Lead Microsoft Fabric migration initiative and establish foundation for AI applications; Build comprehensive data solutions while maintaining operational flexibility for sales optimization; Key Responsibilities; Data Infrastructure and Quality Management; Lead data quality remediation for account and items data within ERP system; Develop comprehensive data validation processes and governance standards; Design and implement robust ETL processes for in-house ERP system integration; Establish data integrity frameworks while preserving operational flexibility required for sales growth; Microsoft Fabric Migration and Architecture; Spearhead migration of company data to Microsoft Fabric platform; Design target architecture consolidating data from mixed POS systems and BCMS; Establish unified data models supporting current reporting and future AI applications; Advanced Analytics and AI Foundation; Prepare data infrastructure for AI solutions; Integrate user behavioral data from web analytics tools; Create customer segmentation and personalization data frameworks; Business Intelligence and Reporting Enhancement; Expand existing Power BI dashboard capabilities for comprehensive business insights; Work directly with stakeholders understanding reporting requirements across organizational levels; Establish data-driven decision-making frameworks; Required Qualifications; Educational Background; Bachelor's degree in Computer Science, Information Systems, Data Science, or related technical field; 3\u20135 years progressive experience in data engineering roles; Background working in small to medium enterprise environments with diverse responsibilities; Technical Expertise; Advanced proficiency in Microsoft ecosystem technologies including SQL Server, Azure data services, and Power BI; Preparation knowledge for Microsoft Fabric implementation and migration; Expertise in data modeling, ETL development, and database optimization; Experience with data quality assessment and remediation methodologies; Proficiency in Python or R for data processing and analysis; Experience integrating diverse data sources including ERP systems and web analytics platforms; Essential Competencies; Strategic and Technical Skills; Ability to design data architecture supporting immediate operational needs and long-term strategic objectives; Strong analytical thinking with proven problem-solving capabilities in complex data environments; Experience leading data transformation projects and managing competing priorities; Communication and Leadership; Excellent communication skills translating technical concepts into business value propositions; Ability to collaborate effectively with non-technical stakeholders across departments; Experience presenting data insights to management and training team members on data tools; Confidence making technical recommendations shaping organizational data strategy; Independent Work and Project Management; Strong project management capabilities with ability to prioritize multiple initiatives simultaneously; Proven ability to work autonomously while making sound technical decisions; Experience establishing data standards and implementing solutions independently; Track record of successful data transformation project leadership; Application Requirements; Resume demonstrating relevant data engineering experience and technical qualifications; Cover letter detailing experience with data quality remediation projects and Microsoft ecosystem implementations","salary":"$4,000 \u2013 $6,000 per month (SGD)","work_type":"Full time","country":"singapore"}
{"Job_ID":"85155236","Role":"Big Data Engineer, TikTok Ecommerce Recommendation Infrastructure","Company":"TikTok Pte. Ltd.","Location":"Singapore","Publish_Time":"2025-06-17 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85155236","job_desc":"Responsibilities; TikTok will be prioritizing applicants who have a current right to work in Singapore, and do not require TikTok's sponsorship of a visa. TikTok is the leading destination for short-form mobile video. Our mission is to inspire creativity and bring joy.; TikTok has global offices including Los Angeles, New York, London, Paris, Berlin, Dubai, Singapore, Jakarta, Seoul and Tokyo. Why Join Us Creation is the core of TikTok's purpose.; Our platform is built to help imaginations thrive. This is doubly true of the teams that make TikTok possible. Together, we inspire creativity and bring joy; a mission we all believe in and aim towards achieving every day.; To us, every challenge, no matter how difficult, is an opportunity; to learn, to innovate, and to grow as one team. Status quo? Never. Courage? Always. At TikTok, we create together and grow together. That's how we drive impact; for ourselves, our company, and the communities we serve.; Join us. About the Team E-commerce is a new and fast growing business that aims at connecting all customers to excellent sellers and quality products on TikTok Shop, through E-commerce live-streaming, E-commerce short videos, and commodity recommendation. Our E-ecommerce Recommendation Infra team is responsible for building up and optimizing the infrastructure for such recommendation systems, so as to provide the most stable and best experience for our users.; We work closely with applied machine learning engineers and build scalable systems to support all kinds of innovative algorithms and techniques. The team is responsible for system stability and high availability, online services and offline data flow performance optimization, solving system bottlenecks, reducing cost overhead, building data and service mid-platform, realizing flexible and scalable high-performance storage and computing systems. Responsibilities; Design and implement a reasonable offline data architecture for large-scale recommendation systems; Design and implement flexible, scalable, stable and high-performance storage and computing systems; Trouble-shooting of the production system, design and implement the necessary mechanisms and tools to ensure the stability of the overall operation of the production system; Build industry-leading distributed systems such as storage and computing to provide reliable infrastructure for massive data and large-scale business systems; Develop and implement techniques and analytics applications to transform raw data into meaningful information using data-oriented programming languages and visualisation software; Apply data mining, data modelling, natural language processing, and machine learning to extract and analyse information from large structured and unstructured datasets; Visualise, interpret, and report data findings and may create dynamic data reports as well; Qualifications; - Bachelor's degree or above in computer science, software engineering, or a related field - Familiar with many open source frameworks in the field of big data, e.g. Hadoop, Hive,Flink, FlinkSQL,Spark, Kafka, HBase, Redis, RocksDB, ElasticSearch etc. - Familiar with Java, C ++ and other programming languages; - Strong coding and trouble shooting ability TikTok is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives. Our platform connects people from across the globe and so does our workplace.; At TikTok, our mission is to inspire creativity and bring joy. To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach. We are passionate about this and hope you are too.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85155236","Role":"Big Data Engineer, TikTok Ecommerce Recommendation Infrastructure","Company":"TikTok Pte. Ltd.","Location":"Singapore","Publish_Time":"2025-06-17 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85155236","job_desc":"Responsibilities; TikTok will be prioritizing applicants who have a current right to work in Singapore, and do not require TikTok's sponsorship of a visa. TikTok is the leading destination for short-form mobile video. Our mission is to inspire creativity and bring joy.; TikTok has global offices including Los Angeles, New York, London, Paris, Berlin, Dubai, Singapore, Jakarta, Seoul and Tokyo. Why Join Us Creation is the core of TikTok's purpose.; Our platform is built to help imaginations thrive. This is doubly true of the teams that make TikTok possible. Together, we inspire creativity and bring joy; a mission we all believe in and aim towards achieving every day.; To us, every challenge, no matter how difficult, is an opportunity; to learn, to innovate, and to grow as one team. Status quo? Never. Courage? Always. At TikTok, we create together and grow together. That's how we drive impact; for ourselves, our company, and the communities we serve.; Join us. About the Team E-commerce is a new and fast growing business that aims at connecting all customers to excellent sellers and quality products on TikTok Shop, through E-commerce live-streaming, E-commerce short videos, and commodity recommendation. Our E-ecommerce Recommendation Infra team is responsible for building up and optimizing the infrastructure for such recommendation systems, so as to provide the most stable and best experience for our users.; We work closely with applied machine learning engineers and build scalable systems to support all kinds of innovative algorithms and techniques. The team is responsible for system stability and high availability, online services and offline data flow performance optimization, solving system bottlenecks, reducing cost overhead, building data and service mid-platform, realizing flexible and scalable high-performance storage and computing systems. Responsibilities; Design and implement a reasonable offline data architecture for large-scale recommendation systems; Design and implement flexible, scalable, stable and high-performance storage and computing systems; Trouble-shooting of the production system, design and implement the necessary mechanisms and tools to ensure the stability of the overall operation of the production system; Build industry-leading distributed systems such as storage and computing to provide reliable infrastructure for massive data and large-scale business systems; Develop and implement techniques and analytics applications to transform raw data into meaningful information using data-oriented programming languages and visualisation software; Apply data mining, data modelling, natural language processing, and machine learning to extract and analyse information from large structured and unstructured datasets; Visualise, interpret, and report data findings and may create dynamic data reports as well; Qualifications; - Bachelor's degree or above in computer science, software engineering, or a related field - Familiar with many open source frameworks in the field of big data, e.g. Hadoop, Hive,Flink, FlinkSQL,Spark, Kafka, HBase, Redis, RocksDB, ElasticSearch etc. - Familiar with Java, C ++ and other programming languages; - Strong coding and trouble shooting ability TikTok is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives. Our platform connects people from across the globe and so does our workplace.; At TikTok, our mission is to inspire creativity and bring joy. To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach. We are passionate about this and hope you are too.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85155045","Role":"Big Data Engineer Intern, Ads Data - 2025 Start","Company":"TikTok Pte. Ltd.","Location":"Singapore","Publish_Time":"2025-06-17 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85155045","job_desc":"Responsibilities; Monetization data SG team aims to empower TikTok monetization business through acquiring, building and managing all kinds of ad data as our data foundation, and further provide scalable solutions including dashboards, pipelines, data products and data services. We are partnership with business through a better understanding of business strategy and aligning OKRs with business to drive business growth as a stakeholder. At the same time, our team continues to explore the development of data engines, data warehouse methodology, BI tools, ML data technology to efficiently improve the efficiency of data development, and deeply mine data value to improve business growth.; We are looking for talented individuals to join us for an internship in 2025. Internships at TikTok aim to offer students industry exposure and hands-on experience. Watch your ambitions become reality as your inspiration brings infinite opportunities at TikTok.; Candidates can apply to a maximum of two positions and will be considered for jobs in the order you apply. The application limit is applicable to TikTok and its affiliates' jobs globally. Applications will be reviewed on a rolling basis - we encourage you to apply early.; Successful candidates must be able to commit to the following Internship: We will prioritize candidates who are able to commit to a minimum of 4 months and above, ideally for a minimum of 3 days a week. Responsibilities; - Responsible for developing and operating our internal BI platform to support various business of advertising and analytics. - Process data processing requests and execute data model design, implementation and maintenance;; Qualifications; Minimum Qualifications; Undergraduate, or Postgraduate who is currently pursuing a degree\/master in Computer Science, Computer Engineering, Information Systems or a related technical major;; Strong interest in computer science and internet technology;; Know relevant technologies of the Hadoop ecosystem, such as the principles of MapReduce, Spark, Hive, and Flink;; Familiar with SQL, be able to use SQL to perform data analysis, or proficient in Java, Python and Shell and other programming languages for data processing;; Good at communication, proactive in work, a strong sense of responsibility, and good teamwork ability.; By submitting an application for this role, you accept and agree to our global applicant privacy policy, which may be accessed here: https:\/\/careers.tiktok.com\/legal\/privacy. If you have any questions, please reach out to us at apac-earlycareers@tiktok.com","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85481896","Role":"Senior Data engineer","Company":"Flintex Consulting Pte Ltd","Location":"City Hall","Publish_Time":"2025-07-05 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85481896","job_desc":"Benefits: 13th Month Salary; Responsibilities; Integrate data from multiple sources, such as databases, APIs, or streaming platforms, to provide a unified view of the data; Implement data quality checks and validation processes to ensure the accuracy, completeness, and consistency of data; Identify and resolve data quality issues, monitor data pipelines for errors, and implement data governance and data quality frameworks; Enforce data security and compliance with relevant regulations and industry-specific standards; Implement data access controls, encryption mechanisms, and monitor data privacy and security risks; Optimise data processing and query performance by tuning database configurations, implementing indexing strategies, and leveraging distributed computing frameworks; Optimize data structures for efficient querying and develop data dictionaries and metadata repositories; Identify and resolve performance bottlenecks in data pipelines and systems; Collaborate with cross-functional teams, including data scientists, analysts, and business stakeholders; Document data pipelines, data schemas, and system configurations, making it easier for others to understand and work with the data infrastructure; Monitor data pipelines, databases, and data infrastructure for errors, performance issues, and system failures; Set up monitoring tools, alerts, and logging mechanisms to proactively identify and resolve issues to ensure the availability and reliability of data; It would be a plus if he has software engineering background; Requirements; Bachelor's or master's degree in computer science, information technology, data engineering, or a related field; Strong knowledge of databases, data structures, algorithms; Proficiency in working with data engineering tools and technologies including knowledge of data integration tools (e.g., Apache Kafka, Azure IoTHub, Azure EventHub), ETL\/ELT frameworks (e.g., Apache Spark, Azure Synapse), big data platforms (e.g., Apache Hadoop), and cloud platforms (e.g., Amazon Web Services, Google Cloud Platform, Microsoft Azure); Expertise in working with relational databases (e.g., MySQL, PostgreSQL, Azure SQL, Azure Data Explorer) and data warehousing concepts.; Familiarity with data modeling, schema design, indexing, and optimization techniques is valuable for building efficient and scalable data systems; Proficiency in languages such as Python, SQL, KQL, Java, and Scala; Experience with scripting languages like Bash or PowerShell for automation and system administration tasks; Strong knowledge of data processing frameworks like Apache Spark, Apache Flink, or Apache Beam for efficiently handling large-scale data processing and transformation tasks; Understanding of data serialization formats (e.g., JSON, Avro, Parquet) and data serialization libraries (e.g., Apache Avro, Apache Parquet) is valuable; Having experience in CI\/CD and GitHub that demonstrates ability to work in a collaborative and iterative development environment; Having experience in visualization tools (e.g. Power BI, Plotly, Grafana, Redash) is beneficial; Preferred Skills & Characteristics; Consistently display dynamic independent work habits, goal oriented, passionate in growth mindsets and self-motivated professional. Self-driven and proactive in keeping up with new technologies and programming","salary":"$6,000 \u2013 $9,000 per month (SGD)","work_type":"Full time","country":"singapore"}
{"Job_ID":"85833905","Role":"Data Engineer - Cloud Operations (Engineering & Ops)","Company":"Synapxe","Location":"One North","Publish_Time":"2025-07-17 06:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85833905","job_desc":"Company description:; ; Synapxe is the national HealthTech agency inspiring tomorrow's health. The nexus of HealthTech, we connect people and systems to power a healthier Singapore.; ; Together with partners, we create intelligent technological solutions to improve the health of millions of people every day, everywhere. Reimagine the future of health together with us at www.synapxe.sg; ; ; Job description:; ; Position Overview; This position will serve as primary support for cloud operations related to IDMC, Tableau, STATA, Sagemaker and Databricks. This role will be responsible for ensuring platforms operate reliably, securely and efficiently within the AWS environment. Responsibilities include maintaining operational excellence, monitoring and automation, managing incident response and performance optimization and ensure governance and cloud best practices.; Role & Responsibilities; Operational Architecture and Reliability; Design scalable, fault-tolerant, and high available AWS infrastructure; Define and implement operational best practices for cloud workloads (compute, storage, database); Monitoring and Logging; Build and maintain operational playbooks; Setup alerts, dashboards and logs to track health and performance of AWS workloads; Incident Management and Troubleshooting; Conduct Root Cause analysis and drive permanent fixes for recurring issues; Define and enforce incident response processes and escalation paths; Lead resolution of incidents; Requirements; Degree in Computer Science, Computer Engineering; Minimum 10-12 year working experience in system operations compliance and management areas; 5+ years of experience in cloud operations or cloud architecture; Must be cloud certified; Good in-depth understanding of data warehouse concepts, data profiling, data verification and advanced analytics techniques; Strong knowledge of monitoring, incident management, and clous cost control; Possess prior hands-on experience with technologies such as AWS, IDMC, Tableau, .NET, MS-SQL database, Oracle Database, Databricks, ML Ops, STATA, Sagemaker, Data Robot technologies.; Good interpersonal skills with the ability to work with different groups of stakeholders; Exposure to hospital information \/ clinical systems is an added advantage; Apply Now; NOTE: It only takes a few minutes to apply for a meaningful career in HealthTech - GO FOR IT!!; #LI-SYNX40","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85703055","Role":"Data Engineer","Company":"ONE NORTH AI PTE. LTD.","Location":"Singapore","Publish_Time":"2025-07-13 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85703055","job_desc":"One North, a Singapore based firm specializing in providing Technology Solutions is currently hiring Data Engineers with about 5~10 years of experience especially in Databricks as per details given below.; Job Description & Requirements: -; As Data Engineer, you will support Data Engineering team in setting up the Data Lake on Cloud and the implementation of standardized Data Model, single view of customer.; You will develop data pipelines for new sources, data transformations within the Data Lake , implementing GRAPHQL, work on no sql database, CI\/CD and data delivery as per the business requirements.; Job Description:; Build pipelines to bring in a wide variety of data from multiple sources within the organization as well as from social media and public data sources.; Collaborate with cross functional teams to source data and make it available for downstream consumption.; Work with the team to provide an effective solution design to meet business needs.; Ensure regular communication with key stakeholders, understand any key concerns in how the initiative is being delivered or any risks\/issues that have either not yet been identified or are not being progressed.; Ensure dependencies and challenges (risks) are escalated and managed. Escalate critical issues to the Sponsor and\/or Head of Data Engineering team.; Ensure timelines (milestones, decisions and delivery) are managed and achieved, without compromising quality and within budget.; Ensure an appropriate and coordinated communications plan is in place for initiative execution and delivery, both internal and external.; Ensure final handover of initiative to business-as-usual processes, carry out a post implementation review (as necessary) to ensure initiative objectives have been delivered, and any lessons learnt are included in future processes.; Who we are looking for:; Competencies & Personal Traits:-; Expertise in Databricks; Experience with at least one Cloud Infra provider (Azure\/AWS); Experience in building data pipelines using batch processing with Apache Spark (Spark SQL, Dataframe API) or Hive query language (HQL); Experience in building streaming data pipeline using Apache Spark Structured Streaming or Apache Flink on Kafka & Data Lake; Knowledge of NOSQL databases.; Expertise in Cosmos DB, Restful APIs and GraphQL; Knowledge of Big data ETL processing tools, Data modelling and Data mapping.; Experience with Hive and Hadoop file formats (Avro \/ Parquet \/ ORC); Basic knowledge of scripting (shell \/ bash); Experience of working with multiple data sources including relational databases (SQL Server \/ Oracle \/ DB2 \/ Netezza), NoSQL \/ document databases, flat files; Experience with CI CD tools such as Jenkins, JIRA, Bitbucket, Artifactory, Bamboo and Azure Dev-ops.; Basic understanding of DevOps practices using Git version control; Ability to debug, fine tune and optimize large scale data processing jobs; Excellent problem analysis skills; Working Experience; 7+ years (no upper limit) of experience working with Enterprise IT applications in cloud platform and big data environments.; Professional Qualifications; Certifications related to Data and Analytics would be an added advantage","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85009893","Role":"Senior Data Engineer","Company":"Keppel Management Ltd","Location":"Central Region","Publish_Time":"2025-06-18 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85009893","job_desc":"Job Description; Develop, maintain scalable data pipelines and build out new integrations to support continuing increases in data volume and complexity; Develop and maintain scalable, optimized data pipelines leveraging Python and AWS services to support increasing data volume and complexity, while ensuring seamless integration with AI platforms like Bedrock and Google; Further enhance data accessibility and drive data-driven decision making by collaborating with analytics and business teams to refine data models for business intelligence tools; Develop data models, schemas, and standards that ensure data integrity, quality, and accessibility; Develop, maintain, and optimize scalable data pipelines using Python and AWS services (e.g., S3, Lambda, ECS, EKS, RDS, SNS\/SQS, Vector DB); Build solutions with AI Services like Bedrock, Google etc.; Rapidly developing next-generation scalable, flexible, and high-performance data pipelines; Collaborate with analytics and business teams to create and improve data models for business intelligence; End-to-end ownership of data quality in our core datasets and data pipelines; Participate in code reviews and contribute to DevOps \/ DataOps \/ MLOps; Job Requirements:; Bachelor's degree in Computer Science, Engineering, or a related field; 5-6 years of experience in data engineering or a similar role; Strong programming skills in Python, SQL, AWS and related tech stack; Experience with building scalable data pipelines with technologies such as Glue, Airflow, Kafka, Spark etc.; Experience using Snowflake, DBT, Bedrock is a plus; Good understanding of basic machine learning concepts (Sagemaker)","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85688880","Role":"Data Engineer - Data Governance","Company":"TikTok Pte. Ltd.","Location":"Singapore","Publish_Time":"2025-07-12 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85688880","job_desc":"Responsibilities; About the team The success of TikTok's data business model hinges on the supply of a large volume of high quality labeled data that will grow exponentially as our business scales up. However, the current cost of data labeling is excessively high. The Data Solutions Team uses quantitative and qualitative data to guide and uncover insights, turning our findings into real products to power exponential growth. The Data Solutions Team responsibility includes infrastructure construction, recognition capabilities management, global labeling delivery management. About the role As Data Engineer, you will be working on cutting-edge challenges in the big data and Al industry which requires strong passion and capability of innovation. You will collaborate closely with cross-functional teams to understand business requirements and translate them into technical solutions. Responsibility 1. Design, build, and maintain robust, scalable, and efficient data pipelines for ingesting, processing, and transforming large volumes of data to meet both immediate and long-term business needs. 2. Define the technical strategy and roadmap for data engineering projects in alignment with business objectives, actively evaluate and bring in industry best practices and state-of-the-art technical approaches, and timely update the strategy according to the rapid change of the industry; 3. Own and drive data engineering projects by leveraging both internal and cross-functional resources, setting meaningful and challenging targets, and achieving them with innovative approaches; 4. Ensure data governance and quality, establishing processes to manage and protect the integrity of data across systems. 5. Translate business requirements into actionable data solutions, maintaining a strong understanding of business needs and using data to generate impactful insights for decision-making. 6. Support business intelligence efforts by designing and implementing data warehousing solutions and building interactive dashboards for real-time business analysis. Qualifications; Minimum Qualifications 1. Bachelor's or Master's degree in Computer Science, Engineering, or related field; 2. 3+ years of experience (or equivalent) in data engineering, with a strong track record of building and managing large-scale, production-grade data pipelines. 3. Proficiency in Python and SQL for data processing and automation; experience with Java or Scala is a plus. 4. Hands-on experience with big data frameworks such as Apache Spark, Hadoop, Kafka, or similar technologies. Experience in performance tuning and optimization of data pipelines and large-scale distributed systems. 5. Strong knowledge of database and data warehousing concepts, including star schema, dimensional modeling, ETL\/ELT frameworks, and schema design. 6. Familiarity with data governance, data quality frameworks, and best practices in data validation, lineage, and metadata management. 7. Experience using data visualization tools such as Tableau, Power Bl, or internal platforms to support business insights. Preferred Qualifications 1. Familiarity with real-time data processing technologies such as Flink and others. 2. Strong communication and interpersonal skills, with the ability to work effectively with both technical and non-technical stakeholders.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85072379","Role":"Data engineer - Azure","Company":"Flintex Consulting Pte Ltd","Location":"City Hall","Publish_Time":"2025-06-21 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85072379","job_desc":"Benefits: 13th Month Salary; Data engineer (Azure) \u2013 Synapse and Pyspark, Python,  Datawarehouse and Power BI , Azure Devops; Skills & Experience; Bachelor\u2019s Degree in Computer Science or Engineering with 3-5 years of experience in Azure Data engineering, Python, Pyspark or Big Data development; Sound Knowledge of Azure Synapse analytics for pipelines, orchestration, set up; 1-2  experience in Visualization design and development with Power BI. Knowledge on row-level security, access control; Sound experience in SQL, Datawarehouse, data marts, data ingestion with Pyspark and Python; Expertise in developing and maintaining ETL processing pipelines in cloud-based platforms such as AWS, Azure, etc. (Azure Synapse or data factory preferred); Team player with good interpersonal, communication, and problem-solving skills.; Job Scope; Design, review and development of Pyspark scripts. Testing, troubleshooting of data pipelines, orchestration; Designing and developing reports and dashboards in Power BI, setting up access control with row-level security, DAX query experience; Establishing connections to source data systems, including internal systems e.g. SAP, Historians, Data Lake, etc. as well as external systems such as Web APIs, etc; Managing the collected data in appropriate storage\/data-base solutions e.g. file systems, SQL servers, Big Data platforms such as Hadoop, HANA, etc. as required by the specific project requirements; Design, development of data marts and relevant data pipelines using pyspark, data copy activities for batch ingestion; Deployment of pipeline artifacts from one environment to the other using Azure Devops; Performing data integration e.g. using database table joins, or other mechanisms at an appropriate level as required by the analysis requirements of the project.; Good to have; Data catalog with Purview  enabling effective metadata management, lineage tracking, and data discovery; Candidates should demonstrate the ability to leverage Purview to ensure data governance, compliance, and efficient data exploration within Azure environments.; Others; Able to work independently on assignment according to agreed schedule without much supervision; Own assignment and take initiative to resolve issues hinder completion of assignment Proactively reach out for help\/guidance whenever required.","salary":"$7,000 \u2013 $10,000 per month (SGD)","work_type":"Full time","country":"singapore"}
{"Job_ID":"85782530","Role":"Junior Data Engineer ( ETL \/ Python) | Singaporean Only!","Company":"APBA TG Human Resource Pte Ltd","Location":"Central Region","Publish_Time":"2025-07-15 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85782530","job_desc":"Requirements:; Diploma in Computer Science, AI, Data Analytics, or related disciplines.; 1\u20132 years of relevant experience is an advantage .; Proficiency in Python, R, SQL, and data visualization tools (e.g., Tableau, Qlik) 2.; Familiarity with Windows and UNIX operating systems, networking, and system administration.; Ability to work independently and as part of a team.; Willingness to travel to sites and perform off-site standby duties.; Preferred Skills:; Experience with statistical packages, ETL tools, and scripting languages.; Knowledge of AI model development and implementation.; Class 3 driving license is a plus.;  Apply, please kindly email your updated resume to akshya.raman@tg-hr.com. ; Only shortlisted applicants will be notified. ; APBA TG Human Resource Pte Ltd (14C7275) || Akshya R (R24122440)","salary":"$3,000 \u2013 $3,500 per month (SGD)","work_type":"Kontrak\/Temporer","country":"singapore"}
{"Job_ID":"81745041","Role":"Software Engineer (Big Data Processing)","Company":"Csit","Location":"Singapore","Publish_Time":"2025-06-17 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/81745041","job_desc":"Responsibilities; Design and develop internal flagship big data analytics systems, applications and APIs that allow engineers and analysts to retrieve, triage and analyse information more efficiently; Work with product managers, engineering managers and key stakeholders to deliver impactful solutions that meet our business needs ; Manage enterprise system performance, reliability and sustainability through software quality control and optimisation of software products and technologies; Explore emerging technologies and deliver proof-of-concepts to the team and senior management; Work with the team or independently on the following:; (1) Architect solutions that can scale efficiently; (2) Write clean and maintainable code; (3) Write unit, functional and end-to-end tests; (4) Safely roll out mission-critical products that impact hundreds of analysts; (5) Identify system issues to provide timely resolution and recovery; (6) Perform system health monitoring and support patches, assist in capacity planning and performance tuning","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85789655","Role":"Data Analyst - Big Data & Cloud Platforms","Company":"TEAMLEASE DIGITAL CONSULTING PTE. LTD.","Location":"Boon Lay","Publish_Time":"2025-07-15 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85789655","job_desc":"We are seeking a passionate and experienced Data Analyst to join our team in delivering cutting-edge data solutions for enterprise clients. The successful candidate will be responsible for designing, building, and optimizing scalable data pipelines, implementing ETL frameworks, and contributing to enterprise-wide data management strategies across cloud platforms.; Key Responsibilities:; Design and implement robust data pipelines and ETL workflows using Apache Spark, Python, and Scala.; Lead performance optimization efforts for large-scale batch and real-time data processing.; Integrate and manage data from multiple enterprise systems including SAP ECC, MDG, Dynamics 365, and various flat file sources.; Develop and maintain scalable solutions on Google Cloud Platform (GCP) and Amazon Web Services (AWS).; Build data quality validation frameworks using tools like Great Expectations integrated with BigQuery.; Work collaboratively with cross-functional teams to ensure data reliability, consistency, and availability.; Orchestrate complex workflows using Apache Airflow or Cloud Composer.; Conduct performance tuning, troubleshooting, and documentation for production-grade data systems.; Requirements:; Bachelor\u2019s degree in Electronics, Computer Science, Engineering, Information Technology, Science or a related field.; Minimum 3 years of experience in big data roles.; Strong experience with Apache Spark, Hadoop ecosystem, Hive, and related tools.; Proficiency in Python and Scala.; Hands-on experience with Google Cloud (BigQuery, Cloud Composer) and\/or AWS.; Strong SQL skills and experience with relational and distributed databases.; Experience with data quality frameworks such as Great Expectations.; Excellent problem-solving skills and ability to work independently and in a team.; Nice to Have:; Experience in data harmonization across enterprise systems.; Exposure to data governance practices.; Google Cloud Professional Data Engineer certification or equivalent.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85155118","Role":"Software Engineer (Big Data) - Application Computing","Company":"TikTok Pte. Ltd.","Location":"Singapore","Publish_Time":"2025-06-17 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85155118","job_desc":"Responsibilities; TikTok is the leading destination for short-form mobile video. Our mission is to inspire creativity and bring joy. TikTok has global offices including Los Angeles, New York, London, Paris, Berlin, Dubai, Singapore, Jakarta, Seoul and Tokyo. Why Join Us Creation is the core of TikTok's purpose. Our platform is built to help imaginations thrive. This is doubly true of the teams that make TikTok possible. Together, we inspire creativity and bring joy; a mission we all believe in and aim towards achieving every day.; To us, every challenge, no matter how difficult, is an opportunity; to learn, to innovate, and to grow as one team. Status quo? Never. Courage? Always. At TikTok, we create together and grow together. That's how we drive impact; for ourselves, our company, and the communities we serve.; Join us. Team Introduction TikTok's Recommendation Architecture Team is responsible for real-time computing direction, handling the design and development of real-time computing systems for TikTok videos, live streams, e-commerce, and a billion-user product recommendation system. Their main focus is ensuring system stability and high availability.; They abstract general real-time computing systems, build a unified recommendation feature middleware, and implement a flexible and scalable high-performance storage system and computing model. This enables advanced real-time data systems for deduplication, counting, feature services, and other recommendation-related business needs.; Design and implement real-time (streaming computing) data systems for large-scale recommendation systems.; Create flexible, scalable, stable, and high-performance storage systems and computing models.; Troubleshoot production system failures, design and implement necessary mechanisms and tools to ensure overall stability of the production systems; Construct industry-leading streaming computing frameworks and other distributed systems to provide reliable infrastructure for massive data and large-scale business systems; Research, design, and develop computer and network software or specialised utility programs.; Analyse user needs and develop software solutions, applying principles and techniques of computer science, engineering, and mathematical analysis.; Update software, enhances existing software capabilities, and develops and direct software testing and validation procedures.; Work with computer hardware engineers to integrate hardware and software systems and develop specifications and performance requirements.; Qualifications; -Proficient in programming languages like Java, C++, Scala, Python.; Strong coding and troubleshooting skills.; At least 5 years of relevant experience; Deep understanding of streaming computing systems, with formal production experience in developing TB-level Flink real-time computing systems.; Proficient in modules like FlinkDataStream, FlinkSQL, FlinkCheckpoint, FlinkState, and preferably with experience in reading Flink source code.; Experience in data lake development is preferred.; Familiar with at least one data lake technology such as Hudi, Iceberg, DeltaLake, and preferably with experience in reading their source code.; Willingness to tackle problems without clear answers, with a strong passion for learning new technologies.; Experience in handling PB-level data is a plus.; Familiarity with other big data systems is preferred, including YARN, K8S, Spark, SparkSQL, Kudu, and others.; Experience in storage systems such as Hbase, Cassandra, RocksDB.; TikTok is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives. Our platform connects people from across the globe and so does our workplace. At TikTok, our mission is to inspire creativity and bring joy. To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach. We are passionate about this and hope you are too.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85694062","Role":"Data Analysis Engineer","Company":"Prestige Professions Pte Ltd","Location":"West Region","Publish_Time":"2025-07-12 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85694062","job_desc":"\u2022 Remuneration bonuses package; \u2022 Conducive working environment; \u2022 Promising career prospect and advancement; Job Scopes:; Design and implement data pipelines to extract, transform, and load (ETL) data from various sources to enable real-time visibility and decision-making; Automate data cleansing and validation processes to improve overall data quality ; Develop and maintain dashboards and reports using business intelligence tools; Establish data standards, governance frameworks, and best practices for data management; Identify data quality issues and implement solutions to enhance reliability; Job Requirements:; Diploma or degree qualification; 3-5 years of related experience; Microsoft Office, SQL, Power BI, Oracle; Analytical and problem-solving skills; Good communication and collaboration abilities; *** Sincere & Interested applicants, kindly forward your Updated resume (word doc format) to allan(a)prestigeprofessions.com.sg and CC: Allan (R1223894) **","salary":"$5,000 \u2013 $6,000 per month (SGD)","work_type":"Full time","country":"singapore"}
{"Job_ID":"85817933","Role":"Information Technology - Data Scientist (Data Science Track)","Company":"Singapore Airlines","Location":"Changi","Publish_Time":"2025-07-16 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85817933","job_desc":"Job Description; SIA has multiple positions for junior and senior data scientists to drive our AI, data science and business analytics initiatives.; Key Responsibilities include:; Member of an in-house AI and data science development team that works on AI (including areas in generative AI, autonomous agent, NLP, computer vision and recommender system), mathematical optimization, game theory, and experimental design.; Work closely with business stakeholders to create impactful and intelligent features\/services in AI, data science and data analytics. Propose and build scalable ML\/DL solutions. Deploy them as API microservices for use by software applications and business users for faster and more effective decision making.; Oversee the technical work of external technology partners and provide them datasets to deliver products\/services in AI and data science. Support business users in the assessment\/ validation of partner-supplied prediction models and in their deployment to production cloud.; Work closely with application development teams to operationalize and integrate AI\/ML capabilities in API microservices.; Note: You could be posted to any subsidiary in SIA Group.; Requirements; BS in Computer Science, Mathematics, Statistics, Physics or related discipline is required. PhD and MS degrees related to machine learning and other AI disciplines are preferred.; Intermediate programming skills in Python. Conversant with algorithm design\/analysis, data structure and SQL. Familiarity with functional\/object-oriented software development using modern programming languages such as Scala, TypeScript\/JavaScript, Java and C# is a plus.; Exposure in the use of workflow\/map-reduce and stream processing systems such as Spark and Kafka for big-data processing.; Relevant internship or industry experience as a hands-on data scientist\/AI engineer in shallow and\/or deep learning (you should be very comfortable with most topics covered in the free undergraduate textbook by Gareth James, \u201cAn Introduction to Statistical Learning \u2013 with Applications in Python\u201d, 2023). Additional exposure in some of the following areas is a plus:; Use of recent proprietary\/open-source LLMs through prompt engineering (such as OpenAI ChatGPT and Bing Chat). Familiarity with LLM application\/data frameworks (such as LlamaIndex and LangChain).; Experience of using GPU-accelerated deep learning frameworks (such as PyTorch and TensorFlow).; Familiarity with Bayesian statistics\/inference and Bayesian\/causal networks for probabilistic reasoning.; Some hands-on experience of AWS, Azure, GCP or other public cloud environment.; Good interpersonal and communication skills for working with both technical staff and non-technical business users.; Experience with Agile\/Scrum\/Kanban methodologies is a plus.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85182175","Role":"Big Data Engineer Intern - 2025 Start","Company":"TikTok Pte. Ltd.","Location":"Singapore","Publish_Time":"2025-06-26 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85182175","job_desc":"Responsibilities; About TikTok TikTok is the leading destination for short-form mobile video. At TikTok, our mission is to inspire creativity and bring joy. TikTok's global headquarters are in Los Angeles and Singapore, and its offices include; New York, London, Dublin, Paris, Berlin, Dubai, Jakarta, Seoul, and Tokyo. Why Join Us Creation is the core of TikTok's purpose.; Our products are built to help imaginations thrive. This is doubly true of the teams that make our innovations possible. Together, we inspire creativity and enrich life; a mission we aim towards achieving every day.; To us, every challenge, no matter how ambiguous, is an opportunity; to learn, to innovate, and to grow as one team. Status quo? Never.; Courage? Always. At TikTok, we create together and grow together.; That's how we drive impact-for ourselves, our company, and the users we serve. Join us. Team Introduction; Our Data Platform Business Partnering team is at the core of TikTok E-Commerce business, responsible for generating tremendous amount of data and providing accessing services and applications. We empower many business & engineering teams to analyze and develop innovative strategies and products driving business growth. We are looking for passionate and talented backend engineers to join us to drive the future of E-Commerce together.; As a project intern, you will have the opportunity to engage in impactful short-term projects that provide you with a glimpse of professional real-world experience. You will gain practical skills through on-the-job learning in a fast-paced work environment and develop a deeper understanding of your career interests. Successful candidates must be able to commit to a minimum of 3 months full-time.; Responsibilities; Translate business requirements & end to end designs into technical implementations and responsible for building batch and real-time data warehouse.; Manage data modeling design, writing, and optimizing ETL jobs.; Collaborate with the business team to building data metrics based on data warehouse.; Responsible for building and maintaining data products.; Involvement in rollouts, upgrades, implementation, and release of data system changes as required for streamlining of internal practices.; Develop and implement techniques and analytics applications to transform raw data into meaningful information using data-oriented programming languages and visualisation software.; Apply data mining, data modelling, natural language processing, and machine learning to extract and analyse information from large structured and unstructured datasets.; Visualise, interpret, and report data findings and may create dynamic data reports as well.; Qualifications; Minimum qualifications:; Final year or undergraduate with a background in Software Development, Computer Science, Computer Engineering, or a related technical discipline.; Solid computer basic knowledge (e.g. data structure & algorithms, SQL and networks).; Strong coding capabilities and mastering at least one programming language (e.g. C\/C++\/Java\/Python\/Golang).; Preferred qualifications:; Passionate about data warehouse, ETL development, data analysis and eCommerce.; Good communication skills and a fast learner of new business and technology knowledge.; Strong collaboration skills with the ability to build rapport across teams and stakeholders.; TikTok is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives. Our platform connects people from across the globe and so does our workplace. At TikTok, our mission is to inspire creativity and bring joy.; To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach. We are passionate about this and hope you are too. By submitting an application for this role, you accept and agree to our global applicant privacy policy, which may be accessed here: https:\/\/careers.tiktok.com\/legal\/privacy.; If you have any questions, please reach out to us at apac-earlycareers@tiktok.com","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85371151","Role":"Data Engineer (Azure, PySpark)","Company":"Sembcorp Industries Ltd","Location":"Central Region","Publish_Time":"2025-07-02 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85371151","job_desc":"About Sembcorp; Sembcorp is a leading energy and urban solutions provider headquartered in Singapore. Led by its purpose to drive energy transition, Sembcorp delivers sustainable energy solutions and urban developments by leveraging its sector expertise and global track record.; Purpose & Scope; We are seeking a highly skilled and self-driven Azure Data Engineer with expertise in PySpark, Python, and modern Azure data services including Synapse Analytics and Azure Data Explorer. The ideal candidate will design, develop, and maintain scalable data pipelines and architectures, enabling effective data management, analytics, and governance.; Key Roles and Responsibilities; Design, develop, and maintain scalable and efficient data pipelines (both batch and real-time streaming) using modern data engineering tools.; Build and manage data lakes, data warehouses, and data marts using Azure Data Services.; Integrate data from various sources including APIs, structured\/unstructured files, IoT devices, and real-time streams.; Develop and optimize ETL\/ELT workflows using tools such as Azure Data Factory, Databricks, and Apache Spark.; Implement real-time data ingestion and processing using Azure Stream Analytics, Event Hubs, or Kafka.; Ensure data quality, availability, and security across the entire data lifecycle.; Collaborate with analysts, data scientists, and engineering teams to deliver business-aligned data solutions.; Contribute to data governance efforts and ensure compliance with data privacy standards.; Establish and manage source system connectivity (on-prem, APIs, sensors, etc.).; Handle deployment and migration of data pipeline artifacts between environments using Azure DevOps.; Design, develop, and troubleshoot PySpark scripts and orchestration pipelines.; Perform data integration using database joins and other transformations aligned with project requirements.; Qualifications, Skills & Experience; Bachelor\u2019s Degree in Computer Science, Engineering, or related field; 3\u20135 years of experience in Azure-based data engineering, PySpark, and Big Data technologies; Strong hands-on experience with Azure Synapse Analytics for pipeline orchestration and data handling; Expertise in SQL, data warehousing, data marts, and ingestion using PySpark and Python; Solid experience building and maintaining cloud-based ETL\/ELT pipelines, especially with Azure Data Factory or Synapse; Familiarity with cloud data environments such as Azure and optionally AWS; Experience with Azure DevOps for CI\/CD and artifact deployment; Excellent communication, problem-solving, and interpersonal skills; 1\u20132 years of experience working with Azure Data Explorer (including row-level security and access controls).; Experience with Azure Purview for metadata management, data lineage, governance, and discovery; Ability to work independently and take full ownership of assignments; Proactive in identifying and resolving blockers and escalating when needed; Exposure to real-time processing with tools like Azure Stream Analytics or Kafka; Our Culture at Sembcorp; At Sembcorp, our culture is shaped by a strong set of shared behaviours that guide the way we work and uphold our commitment to driving the energy transition.; We foster an institution-first mindset, where the success of Sembcorp takes precedence over individual interests. Collaboration is at the heart of what we do, as we work seamlessly across markets, businesses, and functions to achieve our goals together. Accountability is a core principle, ensuring that we take ownership of our commitments and deliver on them with integrity and excellence. These values define who we are and create a workplace where our people can thrive while making a meaningful impact on driving energy transition.; Join us in making a real impact!","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"84574188","Role":"Cloudera \/ Data Engineer","Company":"NCS Hong Kong and Singapore","Location":"Ang Mo Kio","Publish_Time":"2025-06-17 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/84574188","job_desc":"Company Description; NCS is a leading technology services firm that operates across the Asia Pacific region in over 20 cities, providing consulting, digital services, technology solutions, and more. We believe in harnessing the power of technology to achieve extraordinary things, creating lasting value and impact for our communities, partners, and people. Our diverse workforce of 13,000 has delivered large-scale, mission-critical, and multi-platform projects for governments and enterprises in Singapore and the APAC region. ; Job Description; Cloudera \/ Data Engineer; As a Cloudera \/ Data Engineer, you will be responsible for designing, building, and maintaining scalable data pipelines and platforms, with a strong focus on the Cloudera Hadoop ecosystem. You will work closely with data analysts, scientists, and business stakeholders to ensure data accessibility, quality, and security.; What will you do?; Design, build, and manage the Cloudera Hadoop Distribution (CDH\/CDP).; Develop and maintain ETL pipelines using tools such as Apache NiFi, Hive, Spark, and Impala.; Manage and optimize HDFS, YARN, Kafka, HBase, and Oozie workflows.; Monitor and troubleshoot cluster performance and jobs with strong problem-solving and debugging skills.; Collaborate with DevOps and Data Science teams to integrate data platforms into applications and analytics workflows.; Ensure data governance, security, and compliance using tools like Apache Ranger, Atlas, and Kerberos.; Mentor and guide a team of data engineers to deliver robust data solutions.; Qualifications; The ideal candidate should possess:; 10+ years of experience in big data engineering, preferably with Cloudera.; Strong programming skills in Python, Java, and Spark.; Experience with Apache Spark, Hive, Impala, and Kafka.; Familiarity with Linux\/Unix and shell scripting.; Degree in Computer Science, Information Technology, or a related field.; Preferred Skills:; Cloudera Certified Professional (CCP) or Cloudera Data Platform certification.; Experience\/Knowledge on cloud platforms (AWS, Azure, or GCP) and hybrid deployments.; Familiarity with CI\/CD pipelines, Docker, or Kubernetes in a data context.; Additional Information; We are driven by our AEIOU beliefs - Adventure, Excellence, Integrity, Ownership, and Unity - and we seek individuals who embody these values in both their professional and personal lives. We are committed to our Impact: Valuing our clients, Growing our people, and Creating our future.  ; Together, we make the extraordinary happen.  ; Learn more about us at ncs.co and visit our LinkedIn career site.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"84289838","Role":"Staff Platform Engineer - Big Data Platform Management","Company":"Csit","Location":"Singapore","Publish_Time":"2025-06-17 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/84289838","job_desc":"Requirements (Minimum Qualifications); Bachelor's degree in Computer Science, Engineering, or a related field.; At least 8 years of experience in managing Big Data Platforms.  Proven experience with technologies like Spark, Kafka, Hadoop, Elastic Stack, Kudu, NiFi and Kafka.; Strong knowledge of Big Data architectures, including data lakes, data warehouses, and data pipelines.; Experience in troubleshooting and resolving issues of the Big Data stack, ranging from hardware, networking, operating system to software; Strong understanding of data storage systems, including HDFS, S3, and other object storage systems.; Experience with scripting languages, such as Python, Scala, or Java.; Experience in managing physical and virtual infrastructures.; Experience in supporting search workloads in multi-terabytes and billions of records.; Experience in distributed systems.; Proven experience in using DevOps tools and CI\/CD pipelines effectively.; Experience in leading engineering teams.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"84455983","Role":"Data Engineer","Company":"Engineers Gate","Location":"Singapore","Publish_Time":"2025-06-17 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/84455983","job_desc":"The Data Engineer will join Engineers Gate\u2019s Core Technology Team as a key player in building and scaling the company\u2019s data platform, one of the cornerstones of our research infrastructure. This individual will be responsible for creating new research datasets by cleaning, normalizing, and loading data into the platform, as well as enhancing the underlying platform itself. As part of a small, focused team, the Data Engineer will collaborate closely with team members and end users, gain full stack data experience, and have immediate firmwide impact.; Key Responsibilities; Develop efficient, scalable data and research infrastructure.; Implement robust workflows for processing structured and unstructured financial datasets.; Collaborate closely with quantitative portfolio managers and researchers to:; Understand their data needs; Solve data-related challenges; Propose potential data applications; Build impactful data products; Optimize existing data processes to accelerate research.; Analyze dataset coverage and quality; build data knowledge base; develop and maintain reusable libraries for data analysis.; Actively work with data vendors and brokers to sleuth and onboard new datasets.; Required Skills, Qualifications and Experience; Undergraduate degree in CS, EE, or related field.; Passion for data.; 2-5 years professional experience in software or data engineering.; Programming experience in Python.; Familiarity with data pipelines\/ETL tools, relational databases, and\/or SQL.; P.S. Mention Tech in Asia Jobs when you apply! Helps keep the good stuff coming \ud83d\ude09","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85038918","Role":"Contract - Data Engineer [AI Data Pipeline Dvt & Mgt] (1 yr)","Company":"Infineon Technologies","Location":"Kallang","Publish_Time":"2025-06-19 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85038918","job_desc":"#WeAreIn for driving decarbonization and digitalization.; As a global leader in semiconductor solutions in power systems and IoT, Infineon enables game-changing solutions for green and efficient energy, clean and safe mobility, as well as smart and secure IoT. Together, we drive innovation and customer success, while caring for our people and empowering them to reach ambitious goals. Be a part of making life easier, safer and greener.; Are you in?; ; We are on a journey to create the best Infineon for everyone.; This means we embrace diversity and inclusion and welcome everyone for who they are. At Infineon, we offer a working environment characterized by trust, openness, respect and tolerance and are committed to give all applicants and employees equal opportunities. We base our recruiting decisions on the applicant\u00b4s experience and skills.; Please let your recruiter know if they need to pay special attention to something in order to enable your participation in the interview process.; Click here for more information about Diversity & Inclusion at Infineon.; The Data Engineer will serve as a technical expert in the fields of design and develop AI data pipelines to manage both large unstructured and structured datasets, with a particular focus on GenAI RAG\/Agent solutions.; ; In your new role you will:; Working closely with data scientists and domain experts to design and develop AI data pipelines using agile development process.; Developing pipelines for ingesting and processing large unstructured and structured datasets from a variety of sources, ensure efficient and effective data processing.; Development of BIA solution using defined framework for Data Modelling; Data Profiling; Data Extraction, Transformation & Loading; Design and provide data\/information in form of reports, dashboards, scorecards and data storytelling using Visualization Tools such as Business Objects & Tableau.; Work with cloud technologies such as AWS to design and implement scalable data architectures; Supporting the operation of the data pipelines involves troubleshooting and bug fixing, as well as implementing change requests to ensure that the data pipelines continue to meet user requirements.; You are best equipped for this task if you have:; Master's or Bachelor's Degree in Computer Science\/Mathematics\/ Statistics or equivalent.; Minimum of 3 years of relevant work experience in data engineering, including in-depth technical knowledge of databases, BI tools, SQL, OLAP, ETL, RAG \/ Agentic Data pipeline.; Proficient in RDBMS: Oracle\/PL SQL; Extensive hands-on experience in conceptualising, designing, and implementing data pipelines. Proficiency in handling unstructured data formats (e.g., PPT, PDF, Docx), databases (RDMS, NoSQL such as Elasticsearch, MongoDB, Neo4j, CEPH) and familiarity with big data platforms (HDFS, Spark, Impala).; Experience in working with AWS technologies focusing on building scalable data pipelines.; Front-end Reporting & Dashboard and Data Exploration tools -Tableau; Strong background in Software Engineering & Development cycles (CI\/CD) with proficiency in scripting languages, particularly Python.; Good understanding and experience with Kubernetes \/ Openshift Platform.; Other Skills \/ Attributes:; Good understanding of data management, data governance, and data security practices.; Highly motivated, structured and methodical with high degree of self-initiative; Team player with good cross-cultural skills to work in an international team; Customer and result-oriented; This is a 12 months contract under 3rd party payroll partner and entitled to benefits according to partner company","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85646746","Role":"Data Engineer","Company":"StarHub Ltd","Location":"Singapore","Publish_Time":"2025-07-10 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85646746","job_desc":"The Customer Lifecycle Management (CLM) team at StarHub is dedicated to understanding, enhancing, and optimizing the customer journey. From acquisition to retention, the CLM team employs data-driven strategies to provide unparalleled customer experiences. Through a combination of data science, business intelligence, customer insights, NPS, and digital analytics, the CLM team ensures that StarHub's offerings are aligned with customer needs, leading to increased loyalty, satisfaction, and growth.; The Data Engineer plays a crucial role in the CLM team by designing, implementing, and maintaining the data infrastructure that supports the team's analytics and data science initiatives. This position is responsible for developing and optimizing data pipelines, ensuring data quality and accessibility, and collaborating with data scientists and analysts to enable efficient data-driven decision-making. The Data Engineer will work on integrating data from various sources, implementing data governance practices, and creating scalable solutions that support the CLM team's objectives in enhancing customer experiences and driving business growth.;   Data Pipeline Development : Design, implement, and maintain efficient ETL (Extract, Transform, Load) processes to integrate data from various sources. Optimize existing data pipelines for improved performance and scalability.; Data Warehouse Management: Develop and maintain the data warehouse architecture, ensuring it meets the needs of the CLM team. Implement data modeling techniques to optimize data storage and retrieval.; Data Quality Assurance: Implement data quality checks and monitoring systems to ensure the accuracy and reliability of data used in analytics and reporting. Develop and maintain data documentation and metadata.; Big Data Technologies: Utilize big data technologies (e.g., Hadoop, Spark) to process and analyze large volumes of customer data efficiently. Implement solutions for real-time data processing when required.; Data Governance: Collaborate with relevant stakeholders to implement data governance policies and procedures. Ensure compliance with data privacy regulations and internal data management standards.; Infrastructure Optimization: Continuously assess and optimize the data infrastructure to improve performance, reduce costs, and enhance scalability. Implement automation solutions to streamline data processes.;   Education Level:; Bachelor's degree in Computer Science, Information Systems, Data Engineering, or a related field. Master's degree in a relevant field is preferred.; Required Experience and Knowledge; 3-5 years of experience in data engineering or a related field.; Strong knowledge of data warehouse concepts, ETL processes, and data modeling techniques.; Experience with cloud-based data platforms (e.g., AWS, SnowFlake).; Proficiency in SQL and experience with NoSQL databases.; Experience with big data technologies such as Hadoop, Spark, or Kafka.; Knowledge of data governance principles and data privacy regulations.; Job-Specific Technical Skills:; Proficiency in Python or Scala for data processing and automation.; Experience with ETL tools (e.g., Apache NiFi, Talend, Informatica).; Knowledge of data visualization tools (e.g., Tableau, PowerBI) to support data quality checks and pipeline monitoring.; Familiarity with version control systems (e.g., Git) and CI\/CD practices.; Experience with container technologies (e.g., Docker) and orchestration tools (e.g., Kubernetes).; Understanding of data security best practices and implementation.; Behavioural Skills:; Strong problem-solving and analytical skills.; Excellent communication abilities to collaborate with technical and non-technical team members.; Proactive approach to identifying and resolving data-related issues.; Ability to manage multiple projects and priorities effectively.; Detail-oriented with a focus on data quality and system reliability.; Adaptability to work with evolving technologies and changing business requirements.; Strong teamwork skills and ability to work in a collaborative environment.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85576102","Role":"Data Engineer","Company":"RSK Group","Location":"Singapore","Publish_Time":"2025-07-08 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85576102","job_desc":"We are seeking a Data Engineer with experience or interest in IoT technologies and cloud-based data engineering to join our team. This role blends the management of high-volume data flows and IoT-specific analytics with general data engineering practices to deliver robust and scalable data solutions. The ideal candidate will balance technical skills and domain knowledge, enabling data-driven insights for utility operations, customer services, and infrastructure optimisation initiatives.; Key Responsibilities:; Design, develop, and maintain scalable and efficient data pipelines and ETL processes for IoT and enterprise data systems.; Implement data ingestion workflows from IoT devices and integrate with enterprise platforms using Azure Data Factory or similar tools.; Ensure data quality through validation, cleansing, and monitoring processes to address issues such as missing data, duplicates, and inconsistencies.; Define data attributes and formats for IoT device and network data to support seamless integration with existing systems and standards.; Optimise data storage solutions in Azure Data Lake Storage (or equivalent) for structured and unstructured data.; Develop APIs and data interfaces for real-time or near-real-time data transfer between IoT components and enterprise platforms.; Apply advanced analytics techniques to IoT data for performance monitoring, usage profiling, and network management.; Leverage BI tools (e.g., Power BI) to enable business intelligence and operational insights.; Implement robust data security and privacy measures, ensuring compliance with relevant regulations.; Collaborate with cross-functional teams to gather requirements and deliver high-quality, documented solutions.; Required Skills & Qualifications:; Bachelor\u2019s degree in Computer Science, Information Technology, or a related field. Advanced degrees or certifications are advantageous.; Academic or industrial experience in data engineering, including SQL Databases and cloud platforms (Azure, AWS, or GCP).; Experience or interest in IoT technologies and data systems.; Proficiency in data ingestion tools such as Azure Data Factory and ETL processes.; Strong programming skills in Python, SQL, and one or more of Java or Scala.; Familiarity with big data technologies (e.g., Hadoop, Kafka) and enterprise service buses (ESB).; Knowledge of data management systems and integration with enterprise applications.; Understanding of operational data flows, business cycles, and regulatory requirements.; Preferred Skills & Qualifications:; Familiarity with additional Azure ecosystem tools (e.g., Synapse Analytics, Event Hub, Stream Analytics).; Experience with APIs, data interfaces, and integrating IoT systems with enterprise data platforms.; Relevant certifications in Microsoft Azure or other recognised credentials.; Knowledge of DevOps practices and CI\/CD pipelines (e.g., Azure DevOps).; Familiarity with containerisation technologies such as Docker.; Background in implementing Change Data Capture (CDC) designs and scalable data architectures.; Personal Attributes:; Excellent communication and collaboration skills.; Strong analytical and problem-solving mindset.; Proactive approach to continuous professional development.; Ability to work effectively in dynamic, fast-paced environments.; This position offers an opportunity to work at the intersection of IoT data systems and modern cloud engineering, supporting innovation and operational excellence across industries.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85570215","Role":"Contract - Data Engineer [AI Data Pipeline] (1 year)","Company":"Infineon Technologies","Location":"Kallang","Publish_Time":"2025-07-08 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85570215","job_desc":"#WeAreIn for driving decarbonization and digitalization.; As a global leader in semiconductor solutions in power systems and IoT, Infineon enables game-changing solutions for green and efficient energy, clean and safe mobility, as well as smart and secure IoT. Together, we drive innovation and customer success, while caring for our people and empowering them to reach ambitious goals. Be a part of making life easier, safer and greener.; Are you in?; ; We are on a journey to create the best Infineon for everyone.; This means we embrace diversity and inclusion and welcome everyone for who they are. At Infineon, we offer a working environment characterized by trust, openness, respect and tolerance and are committed to give all applicants and employees equal opportunities. We base our recruiting decisions on the applicant\u00b4s experience and skills.; Please let your recruiter know if they need to pay special attention to something in order to enable your participation in the interview process.; Click here for more information about Diversity & Inclusion at Infineon.; The Data Engineer will serve as a technical expert in the fields of design and develop AI data pipelines to manage both large unstructured and structured datasets, with a particular focus on GenAI RAG\/Agent solutions.; ; In your new role you will:; Working closely with data scientists and domain experts to design and develop AI data pipelines using agile development process.; Developing pipelines for ingesting and processing large unstructured and structured datasets from a variety of sources, ensure efficient and effective data processing.; Development of BIA solution using defined framework for Data Modelling; Data Profiling; Data Extraction, Transformation & Loading; Design and provide data\/information in form of reports, dashboards, scorecards and data storytelling using Visualization Tools such as Business Objects & Tableau.; Work with cloud technologies such as AWS to design and implement scalable data architectures; Supporting the operation of the data pipelines involves troubleshooting and bug fixing, as well as implementing change requests to ensure that the data pipelines continue to meet user requirements.; You are best equipped for this task if you have:; Master's or Bachelor's Degree in Computer Science\/Mathematics\/ Statistics or equivalent.; Minimum of 3 years of relevant work experience in data engineering, including in-depth technical knowledge of databases, BI tools, SQL, OLAP, ETL, RAG \/ Agentic Data pipeline.; Proficient in RDBMS: Oracle\/PL SQL; Extensive hands-on experience in conceptualising, designing, and implementing data pipelines. Proficiency in handling unstructured data formats (e.g., PPT, PDF, Docx), databases (RDMS, NoSQL such as Elasticsearch, MongoDB, Neo4j, CEPH) and familiarity with big data platforms (HDFS, Spark, Impala).; Experience in working with AWS technologies focusing on building scalable data pipelines.; Front-end Reporting & Dashboard and Data Exploration tools -Tableau; Strong background in Software Engineering & Development cycles (CI\/CD) with proficiency in scripting languages, particularly Python.; Good understanding and experience with Kubernetes \/ Openshift Platform.; Other Skills \/ Attributes:; Good understanding of data management, data governance, and data security practices.; Highly motivated, structured and methodical with high degree of self-initiative; Team player with good cross-cultural skills to work in an international team; Customer and result-oriented; This is a 12 months contract under 3rd party payroll partner and entitled to benefits according to partner company","salary":"","work_type":"Kontrak\/Temporer","country":"singapore"}
{"Job_ID":"85146396","Role":"Data Engineer","Company":"BEAMO LABS PRIVATE LIMITED","Location":"Singapore","Publish_Time":"2025-06-24 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85146396","job_desc":"Job Description:; Design, build, and maintain scalable data pipelines and architecture to support our analytics, product, and business needs.; Develop reliable ELT\/ETL processes and data models for our data warehouse.; Partner with engineers to instrument and track key product and system events; Ensure data quality, consistency, and security across systems.; Build internal tools, dashboards, and data marts that empower teams to make data-driven decisions.; Help define and shape our overall data strategy, tools, and best practices.; Requirements:; Strong problem-solving skills and attention to detail.; Excellent communication skills \u2013 this is key for working closely with cross-functional teams and owning product decisions.; 5+ years of experience as a Data Engineer or in a similar backend\/data infrastructure role with strong SQL and Python skills.; Experience with modern data stack tools (e.g. dbt, Airflow, Snowflake, BigQuery, Redshift, etc.); Solid understanding of data modeling, warehousing concepts, and distributed systems; Experience in cryptocurrency or blockchain is a plus.; A passion for working in a dynamic startup environment and solving complex challenges.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85782700","Role":"Data engineer","Company":"UARROW PTE. LTD.","Location":"Singapore","Publish_Time":"2025-07-15 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85782700","job_desc":"We are looking for a Data engineer, who will be responsible for architecting and implementing very large scale data intelligence solutions around Snowflake Data Warehouse. A solid experience and understanding of architecting, designing and operationalization of large scale data and analytics solutions on Snowflake Cloud Data Warehouse is a must.; Developing ETL pipelines in and out of data warehouse using combination of Python and Snowflakes Snow SQL; Writing SQL queries against Snowflake.; Developing scripts Unix, Python etc. to do Extract, Load and Transform data; Provide production support for Data Warehouse issues such data load problems, transformation translation problems; Translate requirements for BI and Reporting to Database design and reporting design; Understanding data transformation and translation requirements and which tools to leverage to get the job done; Understanding data pipelines and modern ways of automating data pipeline using cloud based; Testing and clearly document implementations, so others can easily understand the requirements, implementation, and test conditions.; Good hands on experience in developing data integration jobs in Talend 7.3 (TMC) , Talend 8.0.; Extensive experience in developing packages using SQL Server Integration Services (SSIS).; Experience in creating Dimensional Model, Logical\/Physical model, and performing forward \/; Reverse Engineering using ERwin data modeller.; Certified Data Vault practitioner from data vault alliance.; Skilled in writing complex SQL queries with joins and sub queries.; Experience in Performance Tuning and Query Optimization.; Good Knowledge on Version control systems and working experience on tools like TFS for SSIS; and GIT for Talend.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85157406","Role":"GenAI Big Data Engineer (Financial Services) Manager, Technology Consulting","Company":"Ernst & Young Solutions LLP","Location":"Singapore","Publish_Time":"2025-06-17 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85157406","job_desc":"At EY, you\u2019ll have the chance to build a career as unique as you are, with the global scale, support, inclusive culture and technology to become the best version of you. And we\u2019re counting on your unique voice and perspective to help EY become even better, too. Join us and build an exceptional experience for yourself, and a better working world for all. ; We are the only professional services organization who has a separate business dedicated exclusively to the financial services marketplace. Join Financial Services (FSO) and you will work with multi-disciplinary teams from around the world to deliver a global perspective. Aligned to key industry groups including asset management, banking and capital markets, insurance and private equity, we provide integrated advisory, assurance, tax, and transaction services.; The Opportunity; As part of our Data and Analytics team of Financial Services Consulting practice you will work with multi-disciplinary teams to support clients in a wide range of big data initiatives aiming to generate and present new, useful and actionable insights. You will have the opportunity to work and take responsibilities in challenging engagements, gaining exposure to clients in various sectors both in Singapore and in the APAC region.; ; Your Key Responsibilities; Participation in large-scale client engagements.; Contribution towards, or even leading, the delivery of innovative and engaging big data solutions.; Understanding of business and technical requirements, provision of subject matter expertise, and implementation of big data engineering techniques.; Conducting of data discovery activities, performing root cause analysis, and making recommendations for the remediation of data quality issues.; Putting into practice good organizational and time management skills, with the ability to prioritize and complete multiple complex projects under tight deadlines.;   Skills and Attributes for Success; Leverage technology to continually learn, improve service delivery and maintain our leading-edge best practices; Strong presentation skills and proficiency in the use of PowerPoint, Word and Excel; Good understanding of financial services industry;   To Qualify for the role, you must have; Bachelor or Master\u2019s degree in computer science, Engineering, or other related fields. ; Minimally 6 years of relevant experience. Candidates with more than 9 years of relevant experience can be consider for Senior Manager position. ; Understanding or even practical experience of handling and manipulating semi-structured and unstructured data.; Deep understanding of big data technology, concepts, tools, features, functions and benefits of different approaches available.; Ability to deploy, manage, and administer Hadoop-based components.; Ability to design, build, install, configure and support Hadoop-based applications.; Experience with one of Java, Python, C# or C++.; Experience with ETL tools such as Talend, Informatica, AWS Glue, Azure Data Factory. Hands-on experience with Talend is a plus.; Hands-on experience with HiveQL.; Familiarity with data ingestion tools such as Kafka, Flume and Sqoop.; Knowledge of Hadoop related workflow\/scheduling tools such as Oozie.; Understanding of data modeling (ER models) techniques.; Experience with investigating and handling data quality issues.;    Ideally, you\u2019ll also have; Design or implementation experience of data models in physical form in one (or more) of the leading RDBMS platforms such as SQL Server, Oracle, IBM DB2\/Netezza, Teradata, etc.; Experience with Business Intelligence or statistical analysis tools and techniques.; Strong communication and business relationship skills to effectively explain analysis, both verbally and in writing, to others and translate analysis into a clear business plan.; Strong time management and organizational skills to gather and make use of data (both internal and external). ;   What we look for; Highly motivated individuals with excellent problem-solving skills and the ability to prioritize shifting workloads in a rapidly changing industry. An effective communicator, you\u2019ll be a confident team player that collaborates with people from various teams while looking to develop your career in a dynamic organization.; What we offer; Continuous learning: You\u2019ll develop the mindset and skills to navigate whatever comes next.; Success as defined by you: We\u2019ll provide the tools and flexibility, so you can make a meaningful impact, your way.; Transformative leadership: We\u2019ll give you the insights, coaching and confidence to be the leader the world needs.; Diverse and inclusive culture: You\u2019ll be embraced for who you are and empowered to use your voice to help others find theirs.;   If you can demonstrate that you meet the criteria above, please contact us as soon as possible.;   The exceptional EY experience. It\u2019s yours to build.;   Apply now.; EY | Shape the future with confidence;  ; EY exists to build a better working world, helping to create long-term value for clients, people and society and build trust in the capital markets.  ;  ; Enabled by data and technology, diverse EY teams in over 150 countries provide trust through assurance and help clients grow, transform and operate.  ;  ; Working across assurance, consulting, law, strategy, tax and transactions, EY teams ask better questions to find new answers for the complex issues facing our world today.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85207822","Role":"Senior Data Engineer","Company":"Razer Inc.","Location":"North Region","Publish_Time":"2025-06-25 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85207822","job_desc":"This role is required to design, build and maintain high volume big data processing systems that enable the organization to collect, manage, and convert raw data into usable information for data scientists and business analysts, and enabling the use of Artificial Intelligence (AI) capabilities. He \/ She is responsible for developing and maintaining data pipelines, data storage systems and cloud infrastructure, while working closely with data scientists, data analysts and internal stakeholders to utilize data for analytics and AI capabilities.; Essential Duties and Responsibilities; Owns and improve the data stack used in the team to enhance the data processing capabilities.; Design, develop and maintain data systems and data pipelines that enable the organization to store, process, and analyze large volumes of data. This involves developing data pipelines, designing data storage systems, and ensuring that data is integrated effectively to support of AI applications.; Manage data lakes and data warehouses by populating and operationalizing them. This involves creating and managing table schemas, views, materialized views, including tokenization and vectorization techniques for Gen AI.; Monitor and troubleshoot data workflows, ensuring timely resolution of failures and rerunning failed jobs to ensure data completeness.; Leverage modern build tools to enhance automation, data quality, testing, and deployment of data pipelines.; Design and build AI Powered and GenAI applications collaboratively with data scientists, data analysts, product managers and business users.; Develop and implement cloud infrastructure that are in line with company's security policies and practices, as well as cost optimization practices.; Manage and scope projects that involve collaboration with data scientists, data analysts and business users to understand the data needs of various stakeholders across the organization to implement appropriate solutions.; Mentor interns and junior engineers in the team; Qualifications and Skills; Degree in computer science, engineering, mathematics or equivalent experience.; 5+ years of relevant professional experience.; Ability to scope projects and effectively lead and mentor more junior colleagues.; Ability to write clean, maintainable, scalable and robust code using Python, SQL, Java.; Proven experience in building and maintaining pipelines in production for advanced analytics uses cases.; Experience with cloud data services such as AWS Redshift, Snowflake, BigQuery, or Azure Data Lake.; Experience using Infrastructure As Code tools such as Terraform, containerization tools like Dockers, container orchestration platforms like Kubernetes.; Experience using orchestration tools like Airflow, distributed computing framework like Spark or Dask, data transformation tool like Data Build Tool (DBT).; Experience with CI\/CD pipelines for data engineering workflows.; Experience with various data processing techniques (streaming, batch, event-based), managing and optimizing data storage (Data Lake, Data Warehouse, Vector Data Stores and Database, SQL, and NoSQL) is essential.; Excellent problem-solving and analytical skills, with an understanding of AI technologies and their applications; Excellent written and verbal communication skills for coordinating across teams.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85478295","Role":"Data Engineer - Applied AI - Data Cycling Centre","Company":"TikTok Pte. Ltd.","Location":"Singapore","Publish_Time":"2025-07-04 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85478295","job_desc":"Responsibilities; About the team The Machine Learning Engineering (MLE) team focused on the application of multimodal LLMs, unsupervised learning, and clustering algorithms. The ideal candidate will work closely with product, operations, and engineering teams to apply advanced natural language processing, computer vision, and deep learning technologies to solve business challenges and extract actionable insights. Responsibilities; Collaborate closely with data scientists, machine learning engineers, and software developers to build scalable data pipelines.; Support the development and deployment of machine learning models by ensuring high-quality, reliable data infrastructure.; Drive innovation in data processing, storage, and retrieval to optimize model training and inference.; Work in an agile environment focused on continuous integration and delivery of ML solutions.; Contribute to the design and implementation of data governance and security best practices.; Qualifications; Minimum Qualifications: 1. Bachelor\u2019s degree in Computer Science, Engineering, or a related field along with proven experience in data engineering, preferably within a machine learning or AI-focused team. 2. Strong proficiency in programming languages such as Python, Java, or Scala.; 3. Hands-on experience with big data technologies (e.g., Hadoop, Spark, Kafka) along with strong expertise in designing, building, and maintaining ETL\/ELT pipelines. 4. Familiarity with cloud platforms (AWS, GCP, or Azure) and their data services. 5. Solid understanding of database systems, both SQL and NoSQL.; 6. Experience optimizing performance in big data and fully understanding data skew along with familiarity with data governance, lineage, and real-time data processing practices. 7. Knowledge of machine learning workflows and data requirements along with excellent problem-solving skills and ability to work collaboratively in cross-functional teams.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85009869","Role":"Data Architect","Company":"Keppel Management Ltd","Location":"Central Region","Publish_Time":"2025-06-18 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85009869","job_desc":"Job Description; Provide strategic recommendations on leveraging data lakes, data meshes, and serverless architectures to optimize data processing and storage; Assess and recommend AI\/ML-enabled data architectures that facilitate scalable feature engineering and model training pipelines; Define and advise on the design of modern, future-ready data architectures that align with business goals and support AI, analytics, and automation; Provide thought leadership on best practices for data modeling, data warehousing, and lakehouse architectures to ensure scalability and performance; Develop data models, schemas, and standards that ensure data integrity, quality, and accessibility; Design & Build solutions with AI Services like Bedrock, Google etc.; Mentor and guide team members in best practices for data architecture and management; Participate in code reviews and contribute to the improvement of data engineering best practices; Job Requirements; Bachelor's degree in Computer Science, Engineering, or a related field; 4-5 years of experience in designing and building high-performance cloud-based data architectures; Experience in Data Engagements, especially leading data strategy, roadmap and implementation; Tech Stack \u2013 Python, SQL, AWS, Snowflake, DBT, Airflow, Bedrock, NoSQL; Knowledge of container-based big data architectures and Kubernetes; Familiarity with DevOps \/ DataOps \/ MLOps concepts","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85207630","Role":"Senior Data Engineer","Company":"Nodeflair","Location":"Singapore","Publish_Time":"2025-06-25 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85207630","job_desc":"We\u2019re partnering with a well-funded, fast-growing tech company to hire a Senior Data Engineer to join their Malaysia team.; What You\u2019ll Do; Design event-driven data systems and stream processing architectures; Build and maintain real-time data pipelines from sensor and control systems; Develop cloud-based analytics platforms and lakehouse solutions; Ensure data quality, automate validation, and manage metadata; Work closely with AI\/IoT teams to support model deployment and analytics; Continuously optimize data systems for speed, cost, and scalability; What We\u2019re Looking For; Experienced in large-scale data engineering; Strong in stream processing, scripting, and data querying; Familiar with modern lakehouse platforms and enterprise-scale data tools; Hands-on with BI dashboards and data visualization; Confident with cloud infrastructure, version control, and schema design; Reach out to Ayla at ayla@nodeflair.com for a confidential discussion.; EA License No: 19S9830","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85272022","Role":"(Sr.) Data Engineer","Company":"BTSE","Location":"Singapore","Publish_Time":"2025-06-30 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85272022","job_desc":"About BTSE:; DGTL SG is a specialized service provider dedicated to delivering a full spectrum of front-office and back-office support solutions, each of which are tailored to the unique needs of global financial technology firms. DGTL SG is engaged by BTSE Group to offer several key positions, enabling the delivery of cutting-edge technology and tailored solutions that meet the evolving demands of the fintech industry in a competitive global market. ; BTSE Group is a leading global fintech and blockchain company that is committed to building innovative technology and infrastructure. BTSE empowers businesses and corporate clients with the advanced tools they need to excel in a rapidly evolving and competitive market. BTSE has pioneered numerous trading technologies that have been widely adopted across the industry, setting new benchmarks for innovation, performance, and security in fintech. BTSE\u2019s diverse business lines serve both retail (B2C) customers and institutional (B2B) clients, enabling them to launch, operate, and scale fintech businesses. BTSE is seeking ambitious, motivated professionals to join our B2C and B2B teams.; About the opportunity:; The Data Architecture team is responsible for designing and implementing scalable data platforms and systems on AWS to support enterprise data warehousing and pipeline infrastructure. As a Data Engineer on this team, you will design, build, and maintain a robust data platform that enables reliable data integration, processing, and analytics across the organization.; Responsibilities:; Design, build, and maintain scalable data platforms and systems on AWS and Databricks to support analytics, reporting, and data processing workloads.; Work closely with the infrastructure team to build and operate the foundational components such as compute, storage, and networking that support data pipeline execution at scale.; Develop and maintain a reusable data job framework to enable efficient and scalable orchestration of data pipelines using PySpark and Databricks Workflow.; Optimize performance of distributed data processing systems, including Spark tuning and resource configuration, to ensure high efficiency and reliability.; Define and implement monitoring, alerting, and observability for the data platform infrastructure to maintain system health and support proactive issue resolution.; Collaborate cross-functionally with data engineers, analysts, and DevOps teams to deliver governed, high-quality data with strong platform-level reliability.; Requirements:; 3+ years of experience in data engineering or a related field, with hands-on experience in building cloud-based data systems (preferably AWS).; Strong proficiency in PySpark, SQL, and Python for large-scale data processing and performance tuning.; Hands-on experience with Databricks and orchestration tools such as Workflow or Apache Airflow, with a proven track record of designing reusable frameworks to run and manage data workflows.; Familiarity with CI\/CD practices and version control systems like GitLab.; Working knowledge of AWS services commonly used in data platforms, such as Amazon Glue, PostgreSQL (RDS or Aurora), and ElastiCache for Redis.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85252285","Role":"Data Engineer (Data visualization \/ETL \/ Python) | Singaporean Only!","Company":"APBA TG Human Resource Pte Ltd","Location":"Central Region","Publish_Time":"2025-06-30 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85252285","job_desc":"Requirements:; Diploma in Computer Science, AI, Data Analytics, or related disciplines.; 1\u20132 years of relevant experience is an advantage .; Proficiency in Python, R, SQL, and data visualization tools (e.g., Tableau, Qlik) 2.; Familiarity with Windows and UNIX operating systems, networking, and system administration.; Ability to work independently and as part of a team.; Willingness to travel to sites and perform off-site standby duties.; Preferred Skills:; Experience with statistical packages, ETL tools, and scripting languages.; Knowledge of AI model development and implementation.; Class 3 driving license is a plus.;  Apply, please kindly email your updated resume to akshya.raman@tg-hr.com. ; Only shortlisted applicants will be notified. ; APBA TG Human Resource Pte Ltd (14C7275) || Akshya R (R24122440)","salary":"$3,000 \u2013 $4,000 per month (SGD)","work_type":"Kontrak\/Temporer","country":"singapore"}
{"Job_ID":"85745919","Role":"Senior Data Analyst (GRM Data Mart & ERM Dashboard)","Company":"PERSOLKELLY Singapore Pte Ltd (Formerly Kelly Services Singapore Pte Ltd)","Location":"Raffles Place","Publish_Time":"2025-07-14 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85745919","job_desc":"Location: Raffles Place; ; Job Purpose; You will be part of the GRM Enterprise System Management team to drive positive change in the division through adoption of new technology and improvement of internal processes as well as continuous support of GRM enterprise systems\/solutions particularly our ERM Tableau Dashboards.; The Job; Assist to drive the adoption and implementation of new technologies and digital transformations (e.g. Data Analytics and Data Visualization) within Group Risk Management to improve effectiveness and efficiency.; Assist to carry out and support project management and BAU activities, including UAT and Production Implementation for initiatives relating to adoption of New Technology Solutions, particularly data analytics and visualization as well as existing applications.; Participate in user requirements gathering process for data analytics and data visualization requests and follow through on process for subsequent approval by the relevant authorities.; Work with business user and data engineer to identify required data for analytics and to carry out data assessment, clarification and data prep in GRM Data Mart for Tableau consumption.; Assist to design, develop and maintain Tableau data model for the visualization of risk data from our GRM Data Mart based on user requirements.; Work with our key stakeholders (teams from various risk pillars in GRM) to design, develop and maintain Enterprise Risk Management Dashboard in Tableau.; Perform critical evaluation and Quality Assurance of the Tableau data model and ERM dashboard development to ensure the non-functional and business requirements are being met.; Assist to manage user access to our Tableau ERM Dashboards based on User Access Matrix.; Assist to manage enhancement request to our Tableau data model and ERM dashboard.; Assist to manage and document Tableau data model\/ERM Dashboard set up in Tableau, including their related workflow and procedures.; Assist in provision of advisory to new users on data analytics and data visualization\u2019s related matters.; Assist in provision of technical training to new users on Tableau.; Carry out ad-hoc tasks\/ projects as required by Lead of GRM ESM and\/or Head of Group AML\/CFT Compliance & ESM.; Takes accountability in considering business and regulatory compliance risks and takes appropriate steps to mitigate the risks.; Maintains awareness of industry trends on risks, regulatory compliance, emerging threats and technologies in order to understand the risk and better safeguard the company.; Highlights any potential concerns \/risks and proactively shares best risk management practices.; ; Our Requirements; Bachelor Degree in Data Analytics or IT with minimum of 2-5 years working experience in the area of Data Analytics and\/or Data Visualization, for insurance-related companies.; Good working knowledge in database management, data analysis process and IT systems knowledge particularly data science\/analytics\/big data platform.; Ability to understand and\/or write IT programming language especially for analytics purposes (e.g. SQL, Python, R) and for data visualization tools (Tableau).; Strong working knowledge of Tableau covering the functions of data modelling, visualization and administration is a must.; Working knowledge in the area of risk management and compliance will be an added advantage.; Broad knowledge of new technology solutions and their application to business in financial services industries (either banks, insurance and capital market firms) will also be an added advantage.; Good interpersonal & communication skills, able to handle difficult situations, adaptable to fast-pace environment and able to work under pressure to meet tight deadlines.; Resourceful, proactive and able to work independently as well as in a team.; High level of integrity, takes accountability of work and good attitude over teamwork.; Takes initiative to improve current state of things and adaptable to embrace new changes.; Interested candidates, please click on the following link to begin your job search journey and submit your curriculum vitae (CV) directly through the official PERSOLKELLY job application platform - GO. ;  ; By sending us your personal data and CV, you are deemed to consent to PERSOLKELLY Singapore Pte Ltd and its affiliates to collect, use and disclose your personal data for account creation in GO and the purposes set out in the Privacy Policy https:\/\/www.persolkelly.com.sg\/policies. You acknowledge that you have read, understood, and agree with GO\u2019s Terms of Use https:\/\/go.persolkelly.com\/Tac and the Privacy Policy. If you wish to withdraw your consent, please email us at dataprotection@persolkelly.com. Please feel free to contact us if you have any queries.;   ; PERSOLKELLY Singapore Pte Ltd \u2022 UEN. 200007268E; EA License No. 01C4394 \u2022 EA Registration No. R1221966 (Evelyn Yang HuiXian)","salary":"$8,000 \u2013 $9,500 per month (SGD)","work_type":"Full time","country":"singapore"}
{"Job_ID":"85685007","Role":"Associate Data Engineer - Datawarehouse (Engineering & Ops)","Company":"Synapxe","Location":"One North","Publish_Time":"2025-07-11 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85685007","job_desc":"Position Overview; The Data Engineer supports the implementation of data structure and architecture, master\/meta-data management approach and data quality programme to facilitate access to data and information. He\/She support the design, implementation and maintenance of data flow channels and data processing systems that support the collection, storage, batch and real-time processing, and analysis of information from structured and unstructured sources in a scalable, repeatable and secure manner on on-premise or commercial cloud. He\/She implements data management standards and practices.; Role & Responsibilities; Manage and prioritize user queries and production issues for existing applications in Engineering and Operations; Track and resolve production support incidents; Attend user meetings to document and analyze change request requirements or conduct regular workgroup meetings with stakeholders; Perform data profiling and mapping to define data requirements for new projects or change requests; Provide support for production reports, dashboards, and metadata; Collaborate with vendors and developers to design, configure, and test enhancements per Synapxe project methodologies; Translate user requirements into analytics, reporting needs, and ETL rules for new data mart applications and enhancements; Identify and document business attributes and metrics by analyzing existing data and reporting requirements; Conduct technical data mapping for potential data warehouse sources; Execute testing phases (system integration testing, user acceptance testing) before implementation; Provide 24\/7 primary application maintenance support; Assist the Project Manager in assessing technical feasibility for cost evaluations; Requirements; Bachelor's degree in Computer Science, Information Technology, or a related field; At least 4 years of experience in the IT industry, including:; Development, implementation, and maintenance of IT systems, preferably in Data Warehousing, ETL rules, data modeling, and BI applications; Operations support and business analysis experience; Strong MS-SQL and Oracle Database scripting; Experience in diagnosing, troubleshooting, and performing root cause analysis; Ability to diagnose and troubleshoot problems with BI reports and ETL processes; Experience with AWS, Data Lake, Databricks, and the healthcare domain is a plus; Able to work independently and as an effective team player with a strong desire to deliver results; Adaptable, meticulous, and possess strong analytical skills; Good communication skills (both written and spoken); Strong team player; Apply Now; NOTE: It only takes a few minutes to apply for a meaningful career in HealthTech - GO FOR IT!!; #LI-SYNX40","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"84883591","Role":"Data Engineer","Company":"ExpressVPN","Location":"Singapore","Publish_Time":"2025-06-17 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/84883591","job_desc":"The Data Engineer will be responsible for designing, developing, and maintaining robust data pipelines and data warehouse architectures. The ideal candidate will have expertise in tools like AWS Redshift, Athena, Snowflake, and other leading databases, with a strong focus on building scalable and efficient data solutions.; What you\u2019ll do; Design and implement scalable and efficient data pipelines to ingest, process, and store large datasets from multiple sources.; Develop and maintain data warehouse solutions using AWS Redshift, Athena, Snowflake, and other modern data platforms.; Optimize data models and queries to ensure high performance and cost-effectiveness.; Collaborate with data analysts, scientists, and cross-functional teams to understand data requirements and translate them into technical solutions.; Ensure data integrity, security, and privacy throughout the pipeline and storage processes.; Monitor data workflows and troubleshoot issues to ensure reliability and accuracy.; Implement best practices for data governance and ensure compliance with data policies.; Stay updated with the latest trends and advancements in data warehousing technologies.; What you\u2019ll need to succeed; Bachelor\u2019s or Master\u2019s degree in Computer Science, Information Technology, or related field.; 5+ years of experience as a Data Engineer or similar role with a focus on data warehousing and pipeline development.; Strong proficiency with AWS Redshift, Athena, Snowflake, and other top-tier databases.; Experience with data pipeline tools and frameworks (e.g., Apache Airflow, Glue, or similar).; Proficiency in SQL and scripting languages such as Python or Shell.; Familiarity with cloud services (AWS preferred) and infrastructure management.; Experience with data modeling, schema design, and ETL processes.; Excellent problem-solving skills and attention to detail.; Strong communication skills and the ability to work collaboratively across teams.; Nice to Have:; Experience with BI tools like Tableau, Looker, or Power BI.; Knowledge of data security and privacy best practices.; Experience with DevOps tools and practices.; P.S. Mention Tech in Asia Jobs when you apply! Helps keep the good stuff coming \ud83d\ude09","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85176733","Role":"Data Engineer - Singapore","Company":"Shopline","Location":"Singapore","Publish_Time":"2025-06-17 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85176733","job_desc":"Data Engineer - SHOPLINE Singapore; SHOPLINE is Asia\u2019s largest smart commerce platform. With our customers in mind, we strive to deliver scalable commerce solutions to merchants of all sizes. We\u2019re a full-featured platform with services including online store opening, O2O solution, retail POS systems, advertising placement, business strategy consultation, marketing, and more to empower merchants to succeed in omnichannel retailing and cross-border commerce.; What you\u2019ll be doing:; Responsible for collecting, designing, storing and processing payments data in the eCommerce business. In charge of unifying and standardizing data to create holistic business digital assets.; Play a leading role in constructing business evaluation metrics, developing tactics in data services and creating data-driven tools in alignment with Products and Operations objectives.; Define underlying business data requirements. Build fitted models to enhance data quality and stability. Standardize and maintain data integrity to provide an efficient way in accessing data.; Who we are looking for:; 5+ years of experience as a data engineer with a bachelor or advanced in Computer Science. Proven track record in data warehousing.; Demonstrated strength in data modeling, development and governance. Preferred experience with building ETL pipelines.; Expert skills in SQL and Python. Familiarity with big data technologies and solutions (Spark, Hadoop, Hive, etc.). Nice to have a background in Machine Learning.; Proven success in communicating across different functions and synthesizing resources to push through projects in a dynamic environment.; Preferred experience in tech firms, especially in the payments industry.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"84520520","Role":"Data Engineer","Company":"Breeze","Location":"Singapore","Publish_Time":"2025-06-17 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/84520520","job_desc":"Are you passionate about solving complex challenges in the fintech space? We\u2019re looking for talented individuals to join our dynamic startup, backed by Sequoia Capital. We\u2019re building the universal payment layer to unify all currencies\u2014fiat and crypto\u2014so businesses and consumers can transact seamlessly. If you're passionate about creating innovative solutions in a dynamic, fast-paced environment, we want to talk to you.; What You\u2019ll Do:; Design, build, and maintain scalable data pipelines and architecture to support our analytics, product, and business needs.; Develop reliable ELT\/ETL processes and data models for our data warehouse.; Partner with engineers to instrument and track key product and system events.; Ensure data quality, consistency, and security across systems.; Build internal tools, dashboards, and data marts that empower teams to make data-driven decisions.; Help define and shape our overall data strategy, tools, and best practices.; What We're Looking For:; Strong problem-solving skills and attention to detail.; Excellent communication skills \u2013 this is key for working closely with cross-functional teams and owning product decisions.; 3+ years of experience as a Data Engineer or in a similar backend\/data infrastructure role with strong SQL and Python skills.; Experience with modern data stack tools (e.g. dbt, Airflow, Snowflake, BigQuery, Redshift, etc.); Solid understanding of data modeling, warehousing concepts, and distributed systems.; Experience in cryptocurrency or blockchain is a plus.; A passion for working in a dynamic startup environment and solving complex challenges.; We have a high hiring bar for engineers and are looking for top talent.; Our Culture:; Fast-paced and dynamic \u2013 We\u2019re a growing startup that moves quickly.; Tech-driven \u2013 We leverage technology to address our users\u2019 biggest challenges.; Ownership and communication \u2013 We value people who take full ownership and communicate well across teams.; Continuous learning \u2013 You\u2019ll have the opportunity to work alongside industry experts and enhance your expertise in risk management.; Why Join Us:; Competitive salary + equity plan; 21 days PTO; Benefits: annual medical allowance, productivity budget, gym membership & wellness subsidy; Annual team retreat trip; Be part of a team backed by Sequoia Capital with a healthy runway.; Requirements:; Familiarity with modern data stack tools is preferred.; Bachelor\u2019s degree.; Must be eligible to work in Singapore.; Work visa sponsorship available.; P.S. Mention Tech in Asia Jobs when you apply! Helps keep the good stuff coming \ud83d\ude09","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"84484732","Role":"Machine Learning Engineer (Data Mining) - Global E-Commerce","Company":"TikTok Pte. Ltd.","Location":"Singapore","Publish_Time":"2025-06-17 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/84484732","job_desc":"Responsibilities; About The Team The e-commerce industry has seen tremendous growth in recent years and has become a hotly contested space amongst leading Internet companies, and its future growth cannot be underestimated.; With millions of loyal users globally, we believe TikTok is an ideal platform to deliver a brand new and better e-commerce experience to our users. Our product engineering team is responsible for building an e-commerce ecosystem that is innovative, secure and intuitive for our users. We are looking for passionate and talented people to join us as we drive the future of e-commerce here at TikTok.; Responsibilities; Responsible for development of data-empowered capabilities for e-commerce platform, enable different e-commerce verticals to extract and apply key insights from oceans of data to meet their business needs.; Data mining for e-commerce, building tagging system based on massive data, analysing user, traffic, e-commerce and other data, combining with business to improve GMV.; Data analysis, mining, model specific productization; understanding of e-commerce business, able to work with PM to continuously optimize data products based on data-driven.; Develop data mining pipelines to extract data assets (eg. entity labels) from aggregated data and enable the extraction of actionable insights from large volumes of domain data.; Design and development of various strategies, algorithms and machine learning techniques to improve the accuracy and coverage of extracted data assets.; Analyze e-commerce data-related business problems and abstract them into data-related requirements that are then implemented to help resolve key business problems.; Qualifications; Minimum Qualifications; B. Sc or higher degree in Computer Science or related fields from accredited and reputable institutions.; 5 years experience in data structures and algorithms, familiar with one or more: machine learning, natural language processing, data mining.; Familiar with one of the Python\/Java\/Go\/C++ programming languages.; Familiar with common data processing technologies (Hadoop, Spark, Hive, Storm, Flink), with demonstrable coding skills.; Familiar with common data mining techniques and machine learning algorithms (GBDT, Xgboost, FM, DFM, etc.), and familiar with their principles and scope of application.; Proficiency in data mining, machine learning tools and common frameworks (TensorFlow, Pytorch).; Preferred Qualifications; Continuous learning, keeping up with the latest research directions and results in the field of machine learning, for the exploration of big data applications in various vertical areas.; In-depth research experience in analysis and mining of user behaviour data and construction of user portraits.; Data mining experience in vertical industries (e-commerce, finance, games, travel, real estate, etc.).; Development and optimisation of advertising and recommendation algorithms.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"84360660","Role":"Data Engineer","Company":"Avatar Techno Services","Location":"Singapore","Publish_Time":"2025-06-17 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/84360660","job_desc":"Company; Avatar Techno Services; avatartechno.com; Designation; Data Engineer; Date Listed; 20 May 2025; Job Type; Experienced \/ Senior Executive; Full\/Perm; Job Period; Immediate Start, Permanent; Profession; IT \/ Information Technology; Industry; Computer and IT; Location Name; Singapore; Allowance \/ Remuneration; $8,000 monthly; Company Profile; Avatar Techno Services is a global IT Solutions and Services Company established in 2019 with its corporate headquarters in Singapore. We continue to expand our global network while providing value-added cost-effective consulting services to our clients. By 2020 Avatar Techno Services became a leading provider of client-focused IT services and started focusing on IT solutions.; Our services are designed to drive innovation and expansion into new marketplaces while reducing overall costs.; In today\u2019s competitive global market, businesses need technology partners who can understand business strategy and deliver seamless solutions with emerging technologies.; Our team is committed in providing all IT activities right from outsourcing solutions, Infra-structure setup, security consultancy, maintenance, support, project management, software development, testing and much more, which can be tailored on a case-to-case basis.; Our business is committed to offer a resource pool of highly skilled, industry-savvy professionals with a wide range of experience to meet the client\u2019s project requirement or as permanent addition.; Job Description; Experience 7 + years; Builds and maintains data pipelines.; - Works related to extracting transactional data, transforming for enrichment\/cleansing, and loading into data warehouses or reconciliation tools.; - Works closely with TLM (Transaction Lifecycle Management) teams for accurate and timely data integration; Required Skills:; ETL Tools and Technologies: Proficiency in ETL tools (e.g., Talend, Informatica, SSIS), scripting languages (e.g., Python, SQL), and data warehouse technologies (e.g., Snowflake, Redshift, BigQuery).; Data Modeling: Understanding of data modeling principles and techniques, including relational and dimensional modeling.; Data Quality and Governance: Knowledge of data quality principles and data governance frameworks.; Database Management: Experience with database systems (e.g., Oracle, SQL Server, MySQL) and SQL.; Big Data Technologies: Familiarity with big data technologies such as Hadoop, Spark, Cloud-based platforms.; Programming and Scripting: Proficiency in programming languages like Python or Scala.; Communication and Collaboration: Strong communication and collaboration skills to work effectively with cross-functional teams.; Experience:; Seven years of experience in data engineering, with a focus on ETL processes in a banking or financial services environment.; Experience with data warehousing, data lakes, or big data platforms.; Experience with data modeling and database design.; Experience with ETL tools and technologies.; Experience with big data technologies (e.g., Hadoop, Spark).; Application Instructions; Please apply for this position by submitting your text CV using InternSG.; Kindly note that only shortlisted candidates will be notified.; Apply for this position","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85810846","Role":"Data Engineer, Global E-Commerce (Governance Service - Risk Control Platform)","Company":"TikTok Pte. Ltd.","Location":"Singapore","Publish_Time":"2025-07-16 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85810846","job_desc":"Responsibilities; Global E-Commerce is a content e-commerce business with international short video product as the carrier. It is committed to becoming the first choice for users to discover and purchase good products with affordable prices. Global E-Commerce business team hopes to provide users with more tailored and efficient consumption experience, enabling merchants to receive reliable platform services in different scenarios such as live e-commerce, short video content e-commerce, thereby making more affordable and high-quality products easily accessible and improving lives.; The Global E-Commerce Risk Control Team is committed to combating risks with groups in the underground black industry, building a risk protection system for the platform, identifying current and future risk trends through human-machine collaboration, and promoting the prevention and control of business to support the rapid development of TikTok's e-commerce business. We are seeking passionate data engineers with strong problem-solving abilities to work hand in hand with talented cross-functional partners (business operations, data science, engineering, and product management) to efficiently solve some highly challenging data development and construction tasks. In this position, you will contribute to the company's core business, and your daily work will directly impact the company's and customers' financial security and business strategic planning.; Responsibilities:; Collaborate closely with product managers, strategic operations, internal engineering teams, etc. to understand data requirements and provide data solutions that meet business needs.; Evaluate, implement, and maintain data infrastructure tools and technologies to support efficient data processing, storage, and querying.; Design, build, and optimise scalable data pipelines to ingest, process, and transform large volumes of data to support complex analytical queries and reporting requirements.; Ensure data integrity, accuracy, and consistency by implementing data quality checks, validation processes, and monitoring mechanisms.; Continuously optimise data pipelines, queries, and processes to improve performance, reduce latency, and enhance scalability.; Build and maintain the data assets for the risk control business at a high level, serving as the foundation for engineering and product applications.; Qualifications; Minimum Qualifications:; Bachelor's or higher degree in Computer Science, Information Technology, Programming & System Analysis, Science (Computer Studies) or related discipline.; Have experience as a data engineer or similar role supporting data-centric businesses.; Solid knowledge of SQL and work experience with relational and non-relational databases.; Proficiency in at least one programming language such as Python, Java, Go, etc.; Solid mastery of data modeling and data warehouse concepts, data integration, and ETL\/ELT technologies.; Effective communication skills and the ability to collaborate effectively with cross-functional teams.; Excellent problem-solving skills, attention to detail, and the ability to thrive in a fast-paced environment.; Preferred Qualifications:; Experience in using big data technologies (such as Apache Hadoop, Spark, Kafka, Flink) and working experience in handling data ranging from TB to PB levels.; Experience in data governance, e-commerce data, data privacy, and compliance.; Experience in the cross-border\/payment\/e-commerce\/risk control industries.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85267346","Role":"Senior Big Data Developer","Company":"Peoplebank Singapore Pte Ltd","Location":"Central Region","Publish_Time":"2025-06-30 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85267346","job_desc":"Job Title: Senior Big Data Developer; Location: Singapore; Job Type: Full-Time \/ Contract; Industry: Information Technology \/ Financial Services; Experience Level: Senior; ; OPEN TO SINGAPOREANS ONLY!; ; ; ; Job Overview: We are seeking a skilled and experienced Senior Big Data Developer to join our technology team. The ideal candidate will be responsible for designing, developing, and maintaining high-performance big data solutions, while working collaboratively within a structured Agile development environment.; ; ; Key Responsibilities:; Analyze functional specifications and provide constructive technical feedback; Design and implement robust, scalable, and high-quality software solutions; Provide effort estimations and technical input for project planning; Develop solutions using Test-Driven Development (TDD) and Behavior-Driven Development (BDD) methodologies; Perform non-functional testing, including performance and scalability assessments; Review design documents and code produced by team members; Maintain comprehensive technical documentation; Support functional and user acceptance testing (UAT); Provide L3 production support, including investigation and resolution of technical issues; Required Qualifications & Experience:; At least 8 years of hands-on experience in Java\/J2EE development; Practical experience in big data technologies (e.g., Spark, Hadoop); Degree or Diploma in Computer Science, Information Systems, or related field (or equivalent professional experience); Experience in Agile\/Scrum development methodologies; Solid understanding of full software development lifecycle and large-scale system design; Technical Skills:; Strong knowledge of design patterns, system performance tuning, and optimization; Proficiency in Java 1.8+, including multithreading and concurrency concepts; Hands-on experience with Apache Spark, Hadoop, and big data processing frameworks; Familiarity with HBase, Elasticsearch, and large-scale distributed data systems; Experience with messaging tools such as IBM MQ and Apache Kafka; Skilled in using DevOps tools including Maven, Gradle, Jenkins, Git, GitLab; Familiarity with quality and testing tools: JUnit, Cucumber, SonarQube; Experience with Spring Framework, RESTful APIs, gRPC, Microservices, and ORM frameworks; Working knowledge of monitoring tools like Kibana, Spark UI, and the ELK Stack; Experience with relational databases such as Oracle or MySQL is a plus; Desirable Skills:; Exposure to banking or financial domains, particularly corporate banking or cash management; Familiarity with secure coding practices and structured development processes; Strong analytical, communication, and collaboration skills; Ability to work well in team settings and share technical knowledge; ; How to Apply: Interested applicants, please click on the \u201cApply Now\u201d to submit your updated resume.  ; ; Please note: Due to the anticipated high volume of applications, only shortlisted candidates will be contacted. All information provided will be treated with strict confidentiality and used solely for recruitment purposes.;  ;  ; Siti Zuriana Bee D\/O Mohamed Yusoff  ; Team Lead \u2013 IT & Digital; EA Personnel No: R23112335; Peoplebank Singapore Pte Ltd | EA Licence No: 08C5248","salary":"SGD 7800 - 9900 per month (SGD)","work_type":"Kontrak\/Temporer","country":"singapore"}
{"Job_ID":"85788052","Role":"Data Engineer Project Intern (Data Platform, TikTok) - 2025 Start (BS\/MS)","Company":"TikTok Pte. Ltd.","Location":"Singapore","Publish_Time":"2025-07-16 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85788052","job_desc":"Responsibilities; About the team The mission of the Data Platform Singapore Business Partnering (DPSG BP) team is to empower the TikTok Business with data. Our goal is to build a Data Warehouse that can cater to batch and streaming data, Data Products that provide useful information to build efficient data metrics & dashboards which will be used to make smarter business decisions to support business growth.; If you're looking for a challenging ground to push your limits, this is the team for you! As a project intern, you will have the opportunity to engage in impactful short-term projects that provide you with a glimpse of professional real-world experience. You will gain practical skills through on-the-job learning in a fast-paced work environment and develop a deeper understanding of your career interests.; Applications will be reviewed on a rolling basis; we encourage you to apply early.; Successful candidates must be able to commit to at least 3 months long internship period. Responsibilities:; Design and implement data warehouse architectures, data modeling, and ETL pipeline development.; Optimize data ETL process and resolve technical issues related to large-scale data ETL.; Participate in data governance under complex data link dependencies and diverse data content ecosystems.; Implement data solutions for business scenarios, leveraging ByteDance's robust mid-tier architecture and product systems.; Qualifications; Minimum Qualifications: - Proficiency in data warehouse implementation methodologies, with in-depth understanding of data warehouse systems and experience supporting business scenarios. - Strong coding skills, with proficiency in multiple tools\/languages including SQL, Python, Java, Hive, Spark, Kafka and Flink.; Preferred Qualifications: - Data-sensitive, meticulous and adept at identifying anomalies in data. - Strong communication skills and excellent ability in integrating technical expertise with business needs.; By submitting an application for this role, you accept and agree to our global applicant privacy policy, which may be accessed here: https:\/\/careers.tiktok.com\/legal\/privacy If you have any questions, please reach out to us at apac-earlycareers@tiktok.com.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"84707702","Role":"Senior Data Engineer (2 year contract)","Company":"StarHub Ltd","Location":"Singapore","Publish_Time":"2025-06-17 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/84707702","job_desc":"Senior Data Engineer; You will design, develop, and deploy AI-powered, cloud-based products. As a Data Engineer, you\u2019ll work with large-scale, heterogeneous datasets and hybrid cloud architectures to support analytics and AI solutions. Collaborate with data scientists, infra engineers, sales specialists, and stakeholders to ensure data quality, build scalable pipelines, and optimize performance. Your work will integrate telco data with other verticals (retail, healthcare), automate DataOps\/MLOps\/LLMOps workflows, and deliver production-grade systems.; As a Data Engineer, you will:; \u00b7 Ensure Data Quality & Consistency \u2022 Validate, clean, and standardize data (e.g., geolocation attributes) to maintain integrity.; \u2022 Define and implement data quality metrics (completeness, uniqueness, accuracy) with automated checks and reporting.; \u00b7 Build & Maintain Data Pipelines \u2022 Develop ETL\/ELT workflows (PySpark, Airflow) to ingest, transform, and load data into warehouses (S3, Postgres, Redshift, MongoDB).; \u2022 Automate DataOps\/MLOps\/LLMOps pipelines with CI\/CD (Airflow, GitLab CI\/CD, Jenkins), including model training, deployment, and monitoring.; \u00b7 Design Data Models & Schemas \u2022 Translate requirements into normalized\/denormalized structures, star\/snowflake schemas, or data vaults.; \u2022 Optimize storage (tables, indexes, partitions, materialized views, columnar encodings) and tune queries (sort\/distribution keys, vacuum).; \u00b7 Integrate & Enrich Telco Data \u2022 Map 4G\/5G infrastructure metadata to geospatial context, augment 5G metrics with legacy 4G, and create unified time-series datasets.; \u2022 Consume analytics\/ML endpoints and real-time streams (Kafka, Kinesis), designing aggregated-data APIs with proper versioning (Swagger\/OpenAPI).; \u00b7 Manage Cloud Infrastructure \u2022 Provision and configure resources (AWS S3, EMR, Redshift, RDS) using IaC (Terraform, CloudFormation), ensuring security (IAM, VPC, encryption).; \u2022 Monitor performance (CloudWatch, Prometheus, Grafana), define SLAs for data freshness and system uptime, and automate backups\/DR processes.; \u00b7 Collaborate Cross-Functionally & Document; \u2022 Clarify objectives with data owners, data scientists, and stakeholders; partner with infra and security teams to maintain compliance (PDPA, GDPR).; \u2022 Document schemas, ETL procedures, and runbooks; enforce version control and mentor junior engineers on best practices.; Qualifications; \u00b7 Bachelor\u2019s or Master\u2019s in Computer Science, Software Engineering, Data Science, or equivalent experience; \u00b7 4+ years in data engineering, analytics, or related AI\/ML role; \u00b7 Proficient in Python for ETL\/data engineering and Spark (PySpark) for large-scale pipelines; \u00b7 Experience with Big Data frameworks and SQL engines (Spark SQL, Redshift, PostgreSQL) for data marts and analytics; \u00b7 Hands-on with Airflow (or equivalent) to orchestrate ETL workflows and GitLab CI\/CD or Jenkins for pipeline automation; \u00b7 Familiar with relational (PostgreSQL, Redshift) and NoSQL (MongoDB) stores: data modeling, indexing, partitioning, and schema evolution; \u00b7 Proven ability to implement scalable storage solutions: tables, indexes, partitions, materialized views, columnar encodings; \u00b7 Skilled in query optimization: execution plans, sort\/distribution keys, vacuum maintenance, and cost-optimization strategies (cluster resizing, Spectrum); \u00b7 Experience with cloud platforms (AWS): S3\/EMR\/Glue, Redshift and containerization (Docker, Kubernetes); \u00b7 Infrastructure as Code using Terraform or CloudFormation for provisioning and drift detection; \u00b7 Knowledge of MLOps\/LLMOps: auto-scaling ML systems, model registry management, and CI\/CD for model deployment; \u00b7 Strong problem-solving, attention to detail, and the ability to collaborate with cross-functional teams; Nice to Have; \u00b7 Exposure to serverless architectures (AWS Lambda) for event-driven pipelines; \u00b7 Familiarity with vector databases, data mesh, or lakehouse architectures; \u00b7 Experience using BI\/visualization tools (Tableau, QuickSight, Grafana) for data quality dashboards; \u00b7 Hands-on with data quality frameworks (Deequ) or LLM-based data applications (NL-->SQL generation); \u00b7 Participation in GenAI POCs (RAG pipelines, Agentic AI demos, geomobility analytics); \u00b7 Client-facing or stakeholder-management experience in data-driven\/AI projects","salary":"","work_type":"Kontrak\/Temporer","country":"singapore"}
{"Job_ID":"85572857","Role":"Snowflake Data Engineer (6 Months Extendable)","Company":"Robert Half International Pte Ltd","Location":"Central Region","Publish_Time":"2025-07-08 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85572857","job_desc":"The Company; ; We are seeking an experienced Snowflake Data Engineer to support our client within the Manufacturing industry on a mission-critical project for 6 months with potential of extension. This role is critical in supporting enterprise-wide data integration, transformation, and analytics initiatives by designing robust ETL\/ELT solutions and optimizing data workflows across platforms.; ; ; The Role; Design, develop, and optimize complex SQL-based transformations on the Snowflake platform.; Build and maintain reliable ETL\/ELT pipelines using Snowflake and Fivetran to seamlessly ingest data from diverse sources such as SharePoint and Oracle ERP.; Collaborate closely with analytics teams to create and optimize Power BI data models and reports that empower business decision-making.; Partner with the Data Architect Manager to drive best practices in data architecture, modeling, and governance across the organization.; Lead efforts in ensuring data quality and integrity by implementing validation checks and quickly resolving inconsistencies.; Monitor and optimize pipeline performance and Snowflake warehouse usage to maximize efficiency and control costs.; Provide ongoing technical support and knowledge transfer to internal teams and end users across APAC.; Document data flows, transformation logic, and system configurations clearly to maintain transparency and support ongoing development.; Use GitLab to manage code, collaborate with peers through version control, and automate deployment with CI\/CD pipelines.; Participate in Agile ceremonies: sprint planning, stand-ups, retrospectives, and peer reviews.; Your Profile; Degree in Computer Science, Information Technology, Data Science, Software Engineering, or a related field.; 8+ years' experience in data engineering or enterprise data warehousing; Expert in Snowflake, advanced SQL, data modeling (star\/snowflake schemas), performance tuning, and security.; Proven skills building and managing ETL\/ELT pipelines, preferably using Fivetran.; Hands-on experience with Power BI for data modeling, report building, and DAX.; Comfortable with GitLab for version control, CI\/CD, and code collaboration.; Experience with cloud platforms (AWS, Azure, or GCP) and orchestration tools like dbt, Airflow, or NiFi.; Strong focus on data quality, governance, and lineage best practices.; Excellent problem-solving, communication, and teamwork skills.; ; Apply Today; ; Please send your resume, in WORD format only and quote reference number GO13257376, by clicking the apply button. Please note that only short-listed candidates will be contacted.; ; ; Robert Half International Pte Ltd. Co. Registration no.: 200612189E | EA Licence No.: 07C5595 | Gabriela De Brito Lopes Prestes Oxby EA Registration no.: 1989404; By clicking 'apply', you give your express consent that Robert Half may use your personal information to process your job application and to contact you from time to time for future employment opportunities. For further information on how Robert Half processes your personal information and how to access and correct your information, please read the Robert Half privacy notice: https:\/\/www.roberthalf.com.sg\/privacy-statement. Please do not submit any sensitive personal data to us in your resume (such as government ID numbers, ethnicity, gender, religion, marital status or trade union membership) as we do not collect your sensitive personal data at this time.","salary":"Competitive (SGD)","work_type":"Kontrak\/Temporer","country":"singapore"}
{"Job_ID":"83280897","Role":"Data Engineer, Marketing Intelligence","Company":"TikTok Pte. Ltd.","Location":"Singapore","Publish_Time":"2025-06-17 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/83280897","job_desc":"Responsibilities; Team Introduction The Marketing Intelligence team focuses on exploring and developing innovative approaches, methodologies, and technologies to enhance data-driven marketing strategies and practices Ad Data is the cornerstone of every business decision that millions of advertisers make every day on TikTok and its affiliates.; Therefore, we are tasked with innovating on daily basis to create and maintain complex systems at large scale and continue expanding the capacity to better serve advertisers growing at exponential pace. You will share in the ownership of the technical vision and direction for advanced Ad Data products. You will be a part of a team of top notch technical professionals developing scalable systems and with a focus on sustained operational excellence.; Members of this team will be challenged to innovate using cutting edge technologies. We are looking for people who are highly motivated by aiming for the highest, moving fast, and changing the way customers use data to drive profitability. Responsibilities; - Responsible for streaming and batch data development for data marketing products; - Communicate with engineers, product managers and product analysts to understand data needs; - Build up data expertise and own data quality for allocated areas;; - Work with data infrastructure to triage infra issues and drive to resolution.; Qualifications; Minimum Qualifications; BS or MS degree in Computer Science or related technical field or equivalent practical experience;; Experience in; Big Data technologies (Hadoop, M\/R, Hive, Spark, Kafka, ClickHouse, Flink etc.);; Skilled in SQL and at least one programming language of; Go, Java, Scala, Python;; Experience in performing data analysis, data ingestion, and data integration;; Solid communication and collaboration skills.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85580206","Role":"Senior Associate (12 months contract), Investment Data Science (Data Engineer)","Company":"Temasek International Pte. Ltd.","Location":"Singapore","Publish_Time":"2025-07-08 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85580206","job_desc":"Temasek is a global investment company headquartered in Singapore, with a net portfolio value of S$389 billion (US$288b, \u20ac267b, \u00a3228b, RMB2.08t) as at 31 March 2024. Marking our unlisted assets to market would provide S$31 billion of value uplift and bring our mark to market net portfolio value to S$420 billion.; Our Purpose \u201cSo Every Generation Prospers\u201d guides us to make a difference for today\u2019s and future generations.; Operating on commercial principles, we seek to deliver sustainable returns over the long term.; We have 13 offices in 9 countries around the world: Beijing, Hanoi, Mumbai, Shanghai, Shenzhen, and Singapore in Asia; and Brussels, London, Mexico City, New York, Paris, San Francisco, and Washington, DC outside Asia.; Introduction; Temasek is looking to add a Data Engineer to work closely with other members of the Investment Data Science team to build, deploy and manage the data and analytics workflows used in our data-driven investment analysis process.; Responsibilities; Build tools and automation capabilities for data pipelines that improve the efficiency, quality and resiliency of our data analytics platform; Partner with the investment professionals, quantitative researchers, and data scientists to design, develop and deploy solutions that answer fundamental questions about companies, sectors, countries and financial markets; Explore new external data sources to understand availability and quality =; Develop solutions that enable investment professionals and data science teams to efficiently extract insights from data. This includes owning the ingestion and transformation  ; Requirements; Bachelor\u2019s Degree in Computer Science, Information Technology, Computer Engineering, and\/or related fields; Passion for working with data and developing software to solve data processing challenges; Proficiency with building, tuning, and debugging ETL pipelines in Python , including common libraries (Pandas, Numpy), and testing frameworks (e.g. PyTest); Experience working with SQL and relational databases,  Snowflake is strongly preferred.; Experience working with or managing  cloud technologies e.g. AWS, Kubernetes.; Experience with Data Warehousing and Machine Learning workflows; Experience with CI\/CD workflows preferred; Experience with containerized services  ; Strong written and verbal communications skills","salary":"","work_type":"Kontrak\/Temporer","country":"singapore"}
{"Job_ID":"85783260","Role":"Attractive Job Opening for AWS Python Data Engineer in Singapore","Company":"TALENT XPERTS PTE. LTD.","Location":"Paya Lebar Air Base","Publish_Time":"2025-07-15 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85783260","job_desc":"Mandatory skills*; Very Strong Proficiency:; Python: Extensive experience in Python for data manipulation, scripting, and building data applications.; PySpark: Deep expertise in developing and optimizing large-scale data transformations using PySpark.; SQL: Advanced SQL skills, including complex query writing, performance tuning, and database design.; AWS: Hands-on experience designing, deploying, and managing data solutions on various AWS services (S3, EMR, Glue, Lambda etc).; Solid understanding of data warehousing concepts, ETL\/ELT principles, and data pipeline best practices.; Excellent problem-solving, analytical, and communication skills.; Ability to work independently and as part of a collaborative team.; Desired skills*; Airflow: Experience with Airflow for orchestrating and managing data workflows.; Snowflake: Familiarity with Snowflake for cloud data warehousing and analytical processing.; Bitbucket (or Git): Proficient in using version control systems for collaborative development.; Domain*; Data Engineering; Mode of Interview: Telephonic\/Face to Face\/Skype Interview* -Teams or F2F","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"84766430","Role":"Data Engineer III\/IV","Company":"Ollion","Location":"Singapore","Publish_Time":"2025-06-17 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/84766430","job_desc":"OUR STORY; Let\u2019s be honest: there are lots of people out there doing what we do. We\u2019re just not convinced they\u2019re doing it right. Businesses are hungry for innovation and opportunity, but not at the cost of their independence. At Ollion, we\u2019ve connected companies and capabilities around the world to help ambitious organizations make the most of their transformation and leave the status quo in the dust.; WORKING AT OLLION; Innovation is risky. It demands bold steps and big questions, but that\u2019s the price of making change. We\u2019ve got our head in the cloud and two feet on the ground, channeling tech\u2019s endless potential towards a single goal: making a world of difference. And we\u2019re building a global team to do just that\u2014 a team capable of making game-changing breakthroughs without ever losing sight of the people it will impact. This is more than consulting. This is the change you can be.; THE OLLION DIFFERENCE; At Ollion, we\u2019re all in on your independence. Our teams are seasoned. Our solutions are straightforward\u2014sometimes even groundbreaking. And our engagements? Exactly as long as you want them to be. We deliver fresh thinking and hard-earned insight in a way that works for you and your customers, arming your organization with everything you need to make your transformation truly mean something.; WORKING WITH OLLION (our clients\u2019 experiences); Progress matters more than process. Our global team of cloud-native pros is all about creating new and better ways to work\u2014not just by solving your tech challenges, but by using technology to solve your business challenges. We keep the formulas, frameworks, and ten-point plans to a minimum, tackling your most pressing problems with a proprietary mix of good-old-fashioned ingenuity and refreshing humanity.; DIVERSITY AT OLLION ; One of our cultural keystones, \u2018Find the angle\u2019 recognizes that every individual has different aspirations, needs and brings a unique perspective. ; We value diversity, inclusion, and equity (DE&I) as core to our success. We believe that a diverse workforce brings together unique perspectives, experiences, and ideas, leading to innovation, creativity, and better outcomes for our clients and our organization. We are on a journey and are committed to building a workplace that celebrates and respects individuals from all backgrounds, including but not limited to race, ethnicity, gender, sexual orientation, age, disability, and cultural heritage.  ; As our commitment to diversity and inclusion is reflected in our: ; Awareness and sensitisation programs: to create awareness and sensitisation. We encourage open dialogue, active listening, and mutual respect, creating a safe and supportive environment for everyone to contribute their unique perspectives and ideas. ; Dedicated efforts to building diverse teams: that leverage the strength of our differences to tackle complex challenges and drive innovation. By embracing diversity, we broaden our collective knowledge, enhance problem-solving capabilities, and unlock limitless potential for our employees.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85270581","Role":"Data Analytics (Banking, SQL) CBD ~","Company":"PERSOLKELLY Singapore Pte Ltd (Formerly Kelly Services Singapore Pte Ltd)","Location":"Raffles Place","Publish_Time":"2025-06-30 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85270581","job_desc":"Wealth and Retail Banking Industry;  Hybrid Working arrangements; Duration: 6 months subject to extendable\/convertible; Working Location: Central Business District, CBD; Working hours: 09.00am \u2013 6.00pm (Monday to Friday);  Job Duties; \u00b7         Strong Proficiency in SQL for data extraction, transformation, and analysis.; \u00b7         Expertise with visualization tools like Tableau, PowerBi in building dynamic dashboard, visual analytics and story telling with data; \u00b7         Experience with DataIku, Alteryx or other ETL tools for designing and operationalizing end-to-end data pipelines and workflows; \u00b7         Proficiency in Python for data wrangling, statistical analysis, machine learning and automation of data tasks.; \u00b7         Ability to analyze large datasets, identify trends, outliers, and generate business insights; \u00b7         Strong grasp of statistical techniques; \u00b7         Designing and maintaining data pipelines; \u00b7         Strong communication and storytelling skills for both technical and non-technical stakeholders; \u00b7         Experience working cross-functionally with product, engineering, or business teams; \u00b7         Domain knowledge and experience in Finance industry is a plus;  Requirements; Bachelor\u2019s or Master\u2019s degree in Computer Science, Statistics, Data Science, Engineering, or related field; 8+ years of professional experience in a data science or analytics role; Interested candidates, please click on the following link to begin your job search journey and submit your curriculum vitae (CV) directly through the official PERSOLKELLY job application platform - GO. https:\/\/sg.go.persolkelly.com\/job\/apply\/13140;  Contact number: 8189 1194;  We regret to inform that only shortlisted candidates will be notified.;  By sending us your personal data and CV, you are deemed to consent to PERSOLKELLY Singapore Pte Ltd and its affiliates to collect, use and disclose your personal data for account creation in GO and the purposes set out in the Privacy Policy https:\/\/www.persolkelly.com.sg\/policies. You acknowledge that you have read, understood, and agree with GO\u2019s Terms of Use https:\/\/go.persolkelly.com\/Tac and the Privacy Policy. If you wish to withdraw your consent, please email us at dataprotection@persolkelly.com. Please feel free to contact us if you have any queries.; PERSOLKELLY Singapore Pte Ltd \u2022 UEN No. 200007268E\u2022 EA License No. 01C4394\u2022 Reg. \u2022 R1981246 \u2022 Bertram Lee Kian Hui","salary":"$5,000 \u2013 $6,000 per month (SGD)","work_type":"","country":"singapore"}
{"Job_ID":"84789282","Role":"Data Engineer (Remote)","Company":"Cascade","Location":"Singapore","Publish_Time":"2025-06-17 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/84789282","job_desc":"Do you want to work in a small team where you can make a real  difference in a company? Would you embrace the challenge of building a  high impact platform that is used globally? Then we want to speak with  you!; About Cascade: Cascade is a  fintech startup backed by Canadian and US investors that empowers  companies to grow by democratizing access to institutional debt. We are  building tools that modernize how companies raise and manage debt,  lowering the barriers to entry to the $7 trillion specialty finance and  alternative credit market for companies around the world. Founded just  last year, we are gearing up for our next phase of technical development  and are seeking a talented Data Engineer to lead the way. Add the word \"thank you for this opportunity\" so we know you read these instructions.; About the role: As a Data Engineer  you\u2019ll be an early member of a growing team building a pioneering  platform in the debt infrastructure space. You will design, build, and  launch efficient, scalable, and reliable data pipelines to move and  transform data.; What You\u2019ll Do ; Building databases, data lakes and data ingestion pipelines to integrate customer databases and datasets to our systems; Ingesting financial datasets from external customers, then updating  and maintaining accurate and complete data mappings to ensure that our  products are displaying high quality data; Monitoring and alerting across our data pipelines in order to make sure that our data ingests are reliable and correct; Translating business goals and stakeholder requirements into new  data flows and deliver analytical insights ensuring that the data flows  create real business value continuously; Skills And Experience ; 3+ years experience in a data engineering role; Experience with relational SQL and NoSQL databases, including PostgreSQL, MySQL, MariaDB, MongoDB, Bigtable, etc.; Experience working with ETL technologies, such as Databricks, Fivetran, or dbt; Proficiency in writing SQL queries and knowledge of analytical data warehouses such as RedShift, BigQuery, and Snowflake; Some experience with CI\/CD automation such as GitHub Actions; Developing APIs and integrating with 3rd party APIs; Bonus: experience in fintech \/ SaaS \/ credit analysis \/ lending; People who thrive at Cascade are: ; Self-starters, who take the initiative to tackle challenges in a remote work environment; Good communicators, who can operate without ego to discuss, learn, grow, and help others do the same; Passionate about creating great digital experiences for users; Problem solvers who are not satisfied with the status quo; Benefits ; Remote: we are a remote-first company and are very flexible on hours as long as things get done; Home-office: there\u2019s a $1000 USD home office allowance to set yourself up; Equity: we expect you to have an owner-mentality, and have the equity plan to match; Benefits: health, dental, vision, and more; Perks: we offer free lunches weekly and off-site trips; Job satisfaction: we offer autonomy, ample opportunities for  mastery, and an opportunity to make a difference for companies around  the world","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85550111","Role":"Data Engineer (ETL & Data Integration) \/ $5,500","Company":"APBA TG Human Resource Pte Ltd","Location":"Jurong East","Publish_Time":"2025-07-07 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85550111","job_desc":"Key Responsibilities:; Design, develop, and maintain scalable ETL pipelines for production-grade datasets.; Partner with business units to model data across multiple domains.; Ensure the timely and reliable delivery of high-quality datasets for analytics, reporting, and application consumption.; Curate large and complex datasets to support data science and BI initiatives.; Implement and maintain data virtualization views for seamless data access.; Collaborate with infrastructure teams on secure, efficient deployment practices.; Monitor and optimize data workflows for performance and reliability.; Ensure compliance with data governance, access control, and security policies.; Automate and schedule data jobs using enterprise-grade schedulers.; Participate in continuous improvement of data architecture and engineering standards.; Technical Skill Set:; ETL & Data Streaming: Talend, Qlik Replicate (Attunity), Apache Kafka, REST APIs, JSON\/XML; Databases & Querying: Proficient in SQL; experience with MS SQL Server and MongoDB; Data Virtualization: Experience with Denodo (VQL, data services, scheduling); Scheduling Tools: Talend Scheduler, Denodo Scheduler, Unix\/Linux cron jobs; Programming & Scripting: SQL, Python, Shell scripting; Data Modeling: Familiarity with ER modeling and dimensional modeling (e.g., Kimball methodology); Infrastructure & Deployment: Linux\/Unix, Kubernetes (on Nutanix), Jenkins CI\/CD, Nexus Repository; Security & Access Control: Knowledge of LDAP, SAML, Azure Entra ID; To Apply, please kindly email your updated resume to weizhe.teoh@tg-hr.com; Regret to inform that only shortlisted candidates will be notified.; CEI: R25127749; EA License: 14C7275","salary":"$4,500 \u2013 $5,500 per month (SGD)","work_type":"Kontrak\/Temporer","country":"singapore"}
{"Job_ID":"85206114","Role":"Senior Data Engineer","Company":"CYBER RECRUITZ (PTE. LTD.)","Location":"Singapore","Publish_Time":"2025-06-26 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85206114","job_desc":"We\u2019re Hiring: Senior Data Engineer; Location: Onsite \u2013 Singapore; Experience: 8\u201310 years; Employment Type: Full-time; \u2e3b; Role: Senior Data Engineer; We\u2019re looking for a hands-on Senior Data Engineer skilled in building scalable pipelines and transforming data using Databricks on AWS, PySpark, and Python. Experience with Informatica DEI is a bonus.; Key Responsibilities; Build scalable data pipelines using Databricks + PySpark; Perform ingestion, transformation, validation across diverse sources; Support analytics\/data science teams with high-quality datasets; Refactor Informatica workflows to Databricks (if needed); Optimize and automate ETL workflows; Ensure data quality, lineage, and governance; Required Skills; 8+ years overall experience; 3+ in data engineering; Strong in Databricks on AWS, PySpark, Python; Skilled in ETL, data quality, and pipeline automation; Familiarity with Informatica DEI is a plus; Excellent communicator in client-facing roles; \u2e3b; Ready to Join?; Email your CV to [ info@cyberrecruitz.sg ] with the subject:; Senior Data Engineer \u2013 [Your Name]","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85831330","Role":"Cloudera Platform Engineer","Company":"Morgan McKinley","Location":"Marina South","Publish_Time":"2025-07-17 04:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85831330","job_desc":"Roles & Responsibilities; Morgan McKinley is looking for Cloudera Platform Engineer for a 12- month contract assignment with our global professional services client providing a broad range of services and solutions in strategy, consulting, digital, technology, and operations.; Responsibilities; Oversee day-to-day operations of big data platform to ensure high availability, reliability and performance.; Proactively monitor big data platform services, components and clusters to identify potential issues.; Take corrective actions as needed to maintain platform health; Manage configuration, upgrades, and patching of big data platform, ensuring all services are up to date up to date; Work with the Authority's technical teams to ensure smooth deployment and adoption of new solution to support data ingestions, process and workflows; Maintain clear and detailed documentation of platform configuration, troubleshooting steps and incident resolution.; Continuously monitor for and address platform security vulnerabilities. Implement patching strategies to resolve identified vulnerabilities and maintain a secure environment.; Develop automation script to streamline administrative tasks, platform health and ensure operational consistency.; Ensure the smooth operations and service level of IT solutions.; Support production issues; Qualifications; Bachelor\u2019s degree in Information Technology or related field.; Minimum of 5 years\u2019 experience in related field or in Data Warehousing (e.g. Snowflakes, Databricks, etc.); Hands-on experience, knowledge and troubleshooting of Cloudera Data Platform such as HDFS, YARN, HIVE, Spark, Impala, Ranger, operating systems, security and network.; Hands on experience with monitoring tools like Cloudera Manager, Zabbix, Grafana, Splunk, SyslogNG; Familiarity with middleware applications i.e. Informatica, Denodo and scripting languages like bash, python, or shell scripting for automation; Experience with cloud technology i.e. AWS, Azure is a plus; Ability to troubleshoot complex issues ranging from system resource to application stack traces.; Track record in implementing systems with high availability, high performance, high security hosted at various data centres or hybrid cloud environments will be an added advantage.; Cloudera Certified Administrator or similar certification are a plus.; Excellent communication skill to work with cross-functional teams.; Interested candidates may apply through the application system or send it to sg-rscontracting@morganmckinley.com. Shortlisted candidates will be notified.; ; By sending us your personal data and curriculum vitae (CV), you are deemed to consent to Morgan Mckinley Pte Ltd and its affiliates to collect, use and disclose your personal data for the purposes set out in the Privacy Policy available at https:\/\/www.morganmckinley.com\/sg\/privacy-policy. You acknowledge that you have read, understood, and agree with the Privacy Policy.; ; Morgan McKinley Pte Ltd; Koh Boon Sien; EA Licence No: 11C5502; EA Registration No. R1110345","salary":"","work_type":"Kontrak\/Temporer","country":"singapore"}
{"Job_ID":"85503348","Role":"Data Engineer - Data Pipeline \/ Computer Vision \/ Entry Level welcome","Company":"TRUST RECRUIT PTE. LTD.","Location":"West Region","Publish_Time":"2025-07-05 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85503348","job_desc":"Entry level is welcome to apply as training is provided.; Strong interest in AI, machine learning, or computer vision; What You\u2019ll Gain:; Hands-on experience in the AI data pipeline; Exposure to real-world applications of computer vision in security and logistics; Opportunity to work with a multidisciplinary team of engineers and researchers; A stepping stone into the world of AI and data science; Responsibilities:; Assist in the setup and operation of X-ray scanning equipment to capture baggage images; Organize and catalog images according to predefined categories and metadata standards; Perform basic image preprocessing (e.g., cropping, anonymization, format conversion); Annotate or label images using internal tools to identify objects of interest (e.g., electronics, liquids, prohibited items); Ensure data quality and consistency across the dataset; Collaborate with engineers and data scientists to understand dataset requirements; Follow strict data privacy and security protocols; Requirements:; Diploma in Engineering, Information Technology, Computer Science, or a related field; Comfortable working with computers, file systems, and basic image editing tools; Strong attention to detail and ability to follow structured workflows; Basic understanding of data formats (e.g., JPEG, PNG, CSV) and file management; Good communication skills and ability to work in a team environment; Willingness to work in a secure or controlled environment (e.g., airport or lab setting);  Bonus Skills (Nice to Have):; Experience with Python or scripting for automation; Familiarity with annotation tools (e.g., LabelImg, CVAT, VGG Image Annotator); Interest in AI, machine learning, or computer vision; HOW TO APPLY:; Interested applicants, kindly send your resume in MS WORD format to ref10@trustrecruit.com.sg or please click on \u201cApply Now\u201d and provide the below details in your resume.; Last drawn salary; Expected salary; Notice period; Reason for leaving; We regret only shortlisted candidates will be notified.; Important Note: Trust Recruit Pte Ltd is committed to safeguarding your personal data in accordance with the Personal Data Protection Act (PDPA).; Please read our privacy statement on our corporate website www.trustrecruit.com.sg.; Trust Recruit Pte Ltd; EA License No: 19C9950; EA Personnel: Hooi Wai Man (Samantha); EA Personnel Reg No: R21100062","salary":"$2,500 \u2013 $2,800 per month (SGD)","work_type":"Full time","country":"singapore"}
{"Job_ID":"84356922","Role":"Senior Data Engineer","Company":"StarHub Ltd","Location":"Singapore","Publish_Time":"2025-06-18 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/84356922","job_desc":"JOB PURPOSE; The Data Platform Team is responsible for designing, implementing, and managing a modern data platform that embraces the principles of data mesh, empowering teams to create and manage their own data products. Our mission is to deliver high-quality, scalable data solutions that drive business value across the organization.; As a key member of this team, you will be responsible for building scalable, stable, and secure data pipelines that support both batch and streaming workloads. Your work ensures reliable data delivery across domains and supports the development of reusable, self-serve data products.; In this role, you will collaborate with business owners, engineers, and data stewards to implement ingestion frameworks and transformation jobs that align with the data-as-a-product vision. You will apply best practices in data engineering to enable efficient data integration across cloud and on-prem environments.; KEY RESPONSIBILITIES; Design and develop scalable, secure, and efficient data ingestion pipelines for structured and unstructured data from internal and external systems across AWS and on-prem environments.; Work closely with architects and business domain teams to translate data requirements into robust data pipelines and process workflows.; Design, build, and maintain real-time and batch data pipelines using Kafka, Spark Streaming, AWS EMR, Glue, Lambda, and other AWS services to; ingest and process high-frequency data from diverse internal and external sources.; Implement data partitioning, compaction, and optimization techniques to improve data processing performance and reduce cloud storage costs.; Assist in incident investigations, root cause analysis, and resolution of data pipeline failures or performance bottlenecks.; Document data flow designs, ingestion standards, and transformation logic clearly for use by other engineers, data stewards, and auditors.; QUALIFICATIONS; Required:; Minimum 2 years of experience (or 5 years for a senior position) in Data Engineering, Software Engineering or related fields.; Proven experience building and managing real-time and batch data pipelines on AWS using services such as EMR, Glue, Lambda, S3, and EC2.; Strong knowledge of Python and Spark, with hands-on experience designing low latency ETL\/ELT pipelines.; Experience handling large-scale datasets and optimizing cloud storage formats and query performance.; Familiarity with infrastructure components such as IAM roles, Security Groups, and VPC networking to support secure data access and movement.; Hands-on experience with Linux environments, shell scripting, and AWS CLI for managing data and computation resources.; Strong communication and collaboration skills to work across data, engineering, and network teams.; Ability to maintain clean, structured documentation of ingestion logic, transformation steps, and data flow dependencies.; Preferred:; Certifications in cloud technology platforms (such as cloud architecture, container platforms, systems, and\/or network virtualization).; Knowledge of telecom networks, including mobile and fixed networks, will be an added advantage.; Familiarity with data fabric and data mesh concepts, including their implementation and benefits in distributed data environments, is a bonus.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85692981","Role":"Lead Data Engineer - Cloud Migration","Company":"TRIPLE 8 CONSULTING SERVICES PTE. LTD.","Location":"Anson","Publish_Time":"2025-07-11 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85692981","job_desc":"Roles And Responsibilities:; \u2022 Design and architect data storage solutions, including databases, data lakes, and warehouses, using AWS services such as Amazon S3, Amazon RDS, Amazon Redshift, and Amazon DynamoDB, along with Databricks' Delta Lake. Integrate Informatica IDMC for metadata management and data cataloging.; Create, manage, and optimize data pipelines for ingesting, processing, and transforming data using AWS services like AWS Glue, AWS Data Pipeline, and AWS Lambda, Databricks for advanced data processing, and Informatica IDMC for data integration and quality.; Integrate data from various sources, both internal and external, into AWS and Databricks environments, ensuring data consistency and quality, while leveraging Informatica IDMC for data integration, transformation, and governance.; ; \u2022 Develop ETL (Extract, Transform, Load) processes to cleanse, transform, and enrich data, making it suitable for analytical purposes using Databricks' Spark capabilities and Informatica IDMC for data transformation and quality.; \u2022 Monitor and optimize data processing and query performance in both AWS and Databricks environments, making necessary adjustments to meet performance and scalability requirements. Utilize Informatica IDMC for optimizing data workflows.; \u2022 Implement security best practices and data encryption methods to protect sensitive data in both AWS and Databricks, while ensuring compliance with data privacy regulations. Employ Informatica IDMC for data governance and compliance.; \u2022 Implement automation for routine tasks, such as data ingestion, transformation, and monitoring, using AWS services like AWS Step Functions, AWS Lambda, Databricks Jobs, and Informatica IDMC for workflow automation.; \u2022 Maintain clear and comprehensive documentation of data infrastructure, pipelines, and configurations in both AWS and Databricks environments, with metadata management facilitated by Informatica IDMC.; \u2022 Collaborate with cross-functional teams, including data scientists, analysts, and software engineers, to understand data requirements and deliver appropriate solutions across AWS, Databricks, and Informatica IDMC.; \u2022 Identify and resolve data-related issues and provide support to ensure data availability and integrity in both AWS, Databricks, and Informatica IDMC environments.; \u2022 Optimize AWS, Databricks, and Informatica resource usage to control costs while meeting performance and scalability requirements.; \u2022 Stay up-to-date with AWS, Databricks, Informatica IDMC services, and data engineering best practices to recommend and implement new technologies and techniques.; Requirements \/ Qualifications; \u2022 Bachelor\u2019s or master\u2019s degree in computer science, data engineering, or a related field.; Minimum 10 years of experience in data engineering, with expertise in AWS services, Databricks, and\/or Informatica IDMC.; Proficiency in programming languages such as Python, Java, or Scala for building data pipelines.; ; \u2022 Evaluate potential technical solutions and make recommendations to resolve data issues especially on performance assessment for complex data transformations and long running data processes.; \u2022 Strong knowledge of SQL and NoSQL databases.; \u2022 Familiarity with data modeling and schema design.; \u2022 Excellent problem-solving and analytical skills.; \u2022 Strong communication and collaboration skills.; \u2022 AWS certifications (e.g., AWS Certified Data Analytics - Specialty, AWS Certified Data Analytics - Specialty), Databricks certifications, and Informatica certifications are a plus.; Preferred Skills:; \u2022 Experience with big data technologies like Apache Spark and Hadoop on Databricks.; Knowledge of data governance and data cataloguing tools, especially Informatica IDMC.; Familiarity with data visualization tools like Tableau or Power BI.; ; \u2022 Knowledge of containerization and orchestration tools like Docker and Kubernetes.; \u2022 Understanding of DevOps principles for managing and deploying data pipelines.; \u2022 Experience with version control systems (e.g., Git) and CI\/CD pipelines.","salary":"","work_type":"Kontrak\/Temporer","country":"singapore"}
{"Job_ID":"85308332","Role":"Principal Engineer, Computational Engineering (Data), Office of DGCMIO(Research)","Company":"Singapore Health Services Pte Ltd (SingHealth HQ)","Location":"Outram","Publish_Time":"2025-07-01 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85308332","job_desc":"You will provide technical leadership to architect and evolve SingHealth's data infrastructure for research and innovation. You will spearhead data engineering initiatives, including data cleaning, structuring, and standardisation to improve research data quality and usability while ensuring interoperability, standardisation and readiness of data pipelines for real-world evidence generation and Artificial Intelligence\/Machine Learning.; You will be involved in mapping clinical datasets to standardised frameworks such as the OMOP Common Data Model, implementing Natural Language Processing techniques to enhance unstructured data usability for research, and leading the re-identification and linkage of clinical and operational datasets to support multimodal (text, radiology, digital pathology, videos, genomics) and cross-cohort (inter-institutional) studies.; You will develop a cluster-wide de-identification framework using common hashing methodologies to enable seamless data linkage across institutions and research studies.; Job Requirements; Bachelor\u2019s Degree in Science, Technology, Engineering, or Mathematics related disciplines with at least 7 years of relevant working experience in data engineering; Experience in healthcare data engineering and familiarity with clinical terminologies is an advantage; Expertise in big data architecture, ETL pipelines, and IT operations involving complex and\/or large-scale data; A strategic thinker with strong project leadership and stakeholder engagement skills and the ability to translate research needs into data-driven solutions.; Strong customer service skills, team skills, and the ability to collaborate within cross-functional teams; Detail-oriented and results-driven, with the ability to perform under pressure in a fast-paced and complex research environment.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85379862","Role":"Data Engineer (DST)","Company":"MOH Office for Healthcare Transformation Pte Ltd","Location":"Singapore","Publish_Time":"2025-07-02 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85379862","job_desc":"MOHT envisions a transformed health system that is patient-centric, data-driven and digitally-enabled to better empower health, prevent disease and provide excellent value-based care. To realise this vision, MOHT\u2019s mission is to design and implement innovative solutions essential for the desired health system transformation.; Technology plays a significant role in MOHT\u2019s projects and experiments, and in its operating environment. MOHT adopts an agile approach using rapid and continuous build-measure-learn cycles that identify, develop, deliver and adapt technology to improve health. They work closely with IHiS and implementation partners. This allows MOHT\u2019s teams to be responsive to real needs, as demonstrated by actual data, and nimble in evolving prototypes and scalable solutions.; The main tools being used are:; Agile development on cloud based environment; Digital solution architecture design, development and operation; Data analytics and modelling to understand healthcare flows and resource usage; Artificial Intelligence; Current Initiatives:; Digital application for mental wellness; Passive and active sensing tools for patient lifestyle understanding and improvement; Tele-health monitoring solutions for connected care; Mobile patient and clinical applications; Digital phenotyping; Data analytics, including for patient flow analysis; JOB RESPONSIBILITIES; Lead the design, development, and maintenance of scalable and secure data pipelines and architectures to support healthcare analytics and digital health solutions.; Collaborate with cross-functional teams to understand data needs and translate them into robust data engineering solutions.; Manage and optimize data ingestion, transformation, and storage processes across structured and unstructured data sources.; Ensure data quality, integrity, and governance across all data platforms.; Drive the adoption of best practices in data engineering, including CI\/CD, testing, and monitoring.; Mentor and guide junior data engineers and contribute to team capability building.; Work closely with data scientists, analysts, and application developers to enable data-driven decision-making.; Document data workflows, architecture, and operational procedures for knowledge sharing and continuity.; JOB REQUIREMENTS; Required:; Bachelor\u2019s degree (or above) in Computer Science, Data Engineering, Information Systems, or a related field.; At least 8 years of experience in data engineering, with a strong background in building and managing data pipelines and architectures.; Must have:; Hands-on experience with cloud-based data platforms (AWS preferred or Azure, or GCP).; Proficiency in data pipeline tools (e.g. Apache Airflow, Kafka, Spark).; Strong SQL skills and experience with relational and NoSQL databases.; Strong Python programming skills; Experience with data warehousing solutions (e.g., Snowflake, Redshift, BigQuery).; Familiarity with data governance, security, and compliance in healthcare or regulated environments.; Plus: Experience with healthcare data standards (e.g., FHIR, HL7) and DevOps practices.; Leadership: Ability to lead data engineering initiatives, coordinate with stakeholders, and manage project timelines and deliverables.; Good communication skills, both written and verbal.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85640039","Role":"Data Engineer - Sanderson-iKas Singapore Pte Ltd","Company":"Sanderson-Ikas","Location":"Singapore","Publish_Time":"2025-07-10 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85640039","job_desc":"Responsibilities; Design, develop, and maintain backend systems and data pipelines.; Support data integration, transformation, and delivery processes.; Implement solutions using Java, Scala, or Python.; Work within Agile development teams using tools such as Git, Bitbucket, Jenkins, and Jira.; Troubleshoot, optimize, and ensure performance of data systems.; Engage in code reviews and contribute to system documentation.; Skills; 4-10 years of experience in backend development or data engineering.; Proficiency in Java or Scala (ability to work with both is preferred).; Strong understanding of data modeling and data architecture.; Experience with cloud platforms (AWS, Azure, or GCP).; Familiarity with Databricks, Snowflake, or similar platforms is a plus.; Ability to work with both legacy and modern systems.; Strong analytical and problem-solving skills.; Bachelor's or Master's degree in Computer Science, Engineering, or a related field.; Prior experience in regulatory or risk-related systems is advantageous but not mandatory.; For a confidential discussion, please reach out to me at ali.zaidi@sanderson-ikas.sg; Personal data collected will be used for recruitment purposes only.; Only shortlisted candidates will be notified \/ contacted.; EA Registration No: R1988468; \"Sanderson-iKas\" is the brand name for iKas International (Asia) Pte Ltd, a company incorporated in Singapore under Company UEN No.: 200914065E with EA license number 16S8086.; Website: www.sanderson-ikas.sg; ; information_technology","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85690631","Role":"Data Engineer","Company":"PricewaterhouseCoopers","Location":"Singapore","Publish_Time":"2025-07-11 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85690631","job_desc":"Line of Service; Internal Firm Services; Industry\/Sector; Not Applicable; Specialism; IFS - Information Technology (IT); Management Level; Associate; Job Description & Summary; At PwC, we help clients build trust and reinvent so they can turn complexity into competitive advantage. We\u2019re a tech-forward, people-empowered network with more than 370,000 people in 149 countries. Across audit and assurance, tax and legal, deals and consulting we help clients build, accelerate and sustain momentum. Find out more at www.pwc.com. ; How will you value-add?; Design, develop, and maintain robust ETL\/ELT pipelines using tools like Azure Data Factory, Azure Data Bricks and MS Fabric. ; Build and optimize Data Architectures (Data lakes, Data warehouses) on Azure Data Services, Azure SQL PaaS and Power BI ; Collaborate with data analysts, and business stakeholders to understand data requirements. ; Ensure data quality, integrity, and security across all data systems along with authentication and authorizations for Database. ; Monitor and troubleshoot data pipeline performance and reliability. ; Implement best practices for data governance, metadata management, and documentation. ; Memory management for database systems. ; Develop database schemas, tables and dictionaries.  ; Ensure the data quality and integrity in databases and fix any issues related to database performance and provide corrective measures. ; Work with structured and unstructured data from various sources (APIs, databases, flat files, etc.). ; Perform regular Audit checks ; About you; Bachelor's or master's degree in computer science, Engineering, Information Systems, or related field. ; Good to have Azure Data Engineer certificate   ; 2-3 years of experience in Data Engineering or related roles. ; Proficiency in MS SQL T-SQL and programming languages such as Python or C# .Net. ; Experience with data pipeline orchestration tool Azure Data Factory or Azure Data Bricks. ; Familiarity with cloud data platforms MS Fabric. ; Strong understanding of data modeling and warehousing concepts ; Education (if blank, degree and\/or field of study not specified); Degrees\/Field of Study required:Degrees\/Field of Study preferred:; Certifications (if blank, certifications not specified); Required Skills; Optional Skills; Accepting Feedback, Accepting Feedback, Active Listening, Ansible (Open-Source Tool for Software Provisioning, Configuring, and Deployment), AWS CloudFormation, Azure DevOps Server, Cloud Infrastructure, Communication, Continuous Deployment, Continuous Integration (CI), CrowdStrike, Cybersecurity, Deployment Management, Dynatrace APM, Emotional Regulation, Empathy, GitHub (Version Control Platform), GitLab (DevOps Tool), Google Cloud Platform, Incident Remediation, Inclusion, Infrastructure as a Service (IaaS), Intellectual Curiosity, Jenkins (Software), Jira Software {+ 21 more}; Desired Languages (If blank, desired languages not specified); Travel Requirements; Not Specified; Available for Work Visa Sponsorship?; No; Government Clearance Required?; No; Job Posting End Date","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85603062","Role":"AVP, Data Engineer, Group Asset Management - Business Technology","Company":"United Overseas Bank Limited (UOB)","Location":"Singapore","Publish_Time":"2025-07-09 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85603062","job_desc":"United Overseas Bank Limited (UOB) is a leading bank in Asia with a global network of more than 500 branches and offices in 19 countries and territories in Asia Pacific, Europe and North America. In Asia, we operate through our head office in Singapore and banking subsidiaries in China, Indonesia, Malaysia and Thailand, as well as branches and offices.; Our history spans more than 80 years. Over this time, we have been guided by our values \u2013 Honorable, Enterprising, United and Committed. This means we always strive to do what is right, build for the future, work as one team and pursue long-term success. It is how we work, consistently, be it towards the company, our colleagues or our customers.; Established in 1986, UOB Asset Management (UOBAM) is a wholly owned subsidiary of United Overseas Bank. Headquartered in Singapore, UOBAM has grown extensively across Asia with local presence in Brunei, Indonesia, Japan, Malaysia, Taiwan, Thailand and Vietnam. Our network includes UOB Islamic Asset Management in Malaysia and a joint venture with China\u2019s Ping An Trust to form Ping An Fund Management Company. We have also forged a strategic alliance with Wellington Management Singapore.; Our experienced team of more than 90 investment professionals conduct rigorous fundamental research within a proven investment framework to provide our clients with innovative investment solutions. The strength of our team lies in our commitment to investment excellence. Our performance has been recognised by the industry and we have garnered over 340 awards regionally since 1986.; Through our regional network, we offer global investment management expertise to individuals, institutions and corporations. Our comprehensive suite of products ranges from retail unit trusts and exchange-traded funds to customised portfolio management services for institutional clients. A leader in innovation, UOBAM offers a digital option to manage investments with UOBAM Invest robo-adviser, making investing simpler, smarter and safer.;   UOBAM Technology provides software and system development, as well as information technology support services and banking operations.;   We have centralized and standardized the technology components into Singapore, creating a global footprint which can be utilized for supporting our regional subsidiaries and the branches around the world. We operate and support 8 countries with this architecture to provide a secure and flexible Asset Management infrastructure.; Develop and maintain infrastructure for enterprise data platforms and machine learning.; Collaborate with business stakeholders to gather requirements and translate them into effective data visualizations and reporting solutions.; Design, build, and maintain scalable and interactive dashboards using BI tools such as Power BI, Tableau, or Looker.; Implement and manage data governance frameworks, including data cataloging, lineage, quality, and access controls using cloud-native tools (e.g., AWS Glue, Azure Purview, Google Cloud Data Catalog).; Data Platform Development & Management:; Design, develop, and maintain data platforms that support large-scale data ingestion, storage, and processing using cloud-based data infrastructure.; Implement and manage data warehousing and centralized data solutions tailored for asset management.; Evaluate and integrate new data technologies and tools to enhance data platform capabilities.; Data Pipeline Development:; Build and maintain robust and efficient data pipelines for data ingestion, processing, and transformation.; Develop and implement data quality checks and validation processes to ensure data accuracy, timeliness, and consistency.; Utilize ETL\/ELT tools and techniques to transform and load data into target systems.; Employ exceptional problem-solving skills, with the ability to see and solve issues before they snowball into problems.; Learn and share knowledge and experience in a multi-disciplinary team.; Bachelor's or Master's degree in Computer Science, Data Science, Engineering, or a related field.; At least 5 years of experience in data engineering, business intelligence, machine learning engineering, or a related role in a production environment.; Familiarity with data governance frameworks (e.g., DAMA-DMBOK) and regulatory compliance (e.g., GDPR, CCPA).; Hands-on experience in Python and SQL. Experience with other programming languages (e.g., Java, Scala, C++) is a plus.; Experience in best practices such as DataOps and MLOps; Experience with big data technologies and cloud platforms such as BigQuery, Kafka, GCP, AWS and their data engineering and machine learning products and services.; Strong understanding of software development best practices, including version control (Git), testing, and CI\/CD.; Excellent communication and organizational skills, and the ability to stay focused on completing tasks and meeting goals within a busy workspace.; Skilled at working in tandem with a team of engineers, or alone as required.; Strong troubleshooting and analytical skills.; Cloud and data certifications are a plus.; UOB is an equal opportunity employer. UOB does not discriminate on the basis of a candidate's age, race, gender, color, religion, sexual orientation, physical or mental disability, or other non-merit factors. All employment decisions at UOB are based on business needs, job requirements and qualifications. If you require any assistance or accommodations to be made for the recruitment process, please inform us when you submit your online application.; ; Apply now and make a difference.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"84542865","Role":"Senior Data Engineer","Company":"Plaud AI","Location":"Singapore","Publish_Time":"2025-06-17 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/84542865","job_desc":"PLAUD AI is a pioneering AI-native hardware and software company that turns meetings and conversations into actionable insights with AI devices like PLAUD NOTE and PLAUD NotePin. By recording, transcribing, and summarizing real-life conversations, our solutions boost productivity and save time. Designed for precision and flexibility, whether in meetings or on the go, our products empower you to focus on creative, high-value work while AI handles the details.; We are a growing global team of hardware and software experts seeking advanced AI innovations that integrate with real-life user scenarios. Our newly established headquarters in San Francisco will collaborate with our teams in Shenzhen, Beijing, and Tokyo to extend AI benefits to users globally.; Visit https:\/\/www.plaud.ai to learn more.; WHY JOIN US; Join a skyrocketing team where your impact drives success and your career reaches new heights, along with what we have achieved, as shared below.; Global Leadership: Positioned uniquely to lead the future of work by leveraging innovative AI-driven devices and solutions.; Founded in December 2021: Bootstrapped, profitable, and experiencing explosive growth.; 10x Revenue Growth: Achieved 10x revenue growth for two consecutive years, reaching a $100 million run rate, with expectations for even greater expansion in 2025.; Proven Product-Market Fit: Over 700,000 devices shipped globally since November 2023, with users engaging for an average of 30 hours per month to enhance productivity.; New Initiatives: Expanding from consumer-focused products to industry-specific solutions and enterprise-level services.; Loved by Professionals: Our products are trusted by professionals in sectors such as healthcare and sales, where conversations drive success.; WHAT YOU WILL DO; Infrastructure Design and Maintenance: Design, build, and maintain the infrastructure for big data platforms.; ETL Workflow Development: Develop and maintain efficient ETL preprocessing workflows to ensure smooth data extraction, transformation, and loading.; Data Warehouse Design and Optimization: Design, implement, and optimize data warehouses to ensure data is easily accessible and well-organized for analysis.; Data Analysis and Reporting: Conduct detailed data analysis, including data modeling, dashboard development, and data mining, to extract actionable insights for the business.; Optimization and Performance Tuning: Continuously monitor and optimize the performance of data pipelines and systems to ensure data processing is efficient and scalable.; WHAT YOU WILL BRING; Programming Expertise: Proficient in at least one object-oriented programming language, such as Python, Java, or Scala.; Analytical and Problem-Solving Skills: Strong analytical abilities with innovative thinking, capable of independently solving complex technical problems.; Experience: Over 6 years of experience in big data development, including real-time and offline data processing, modeling, ETL development, and data analysis.; Big Data Technologies Knowledge: Deep understanding and practical experience with big data technologies, including Spark, Flink, Hive, and other mainstream big data processing frameworks.; Collaboration and Communication: Strong self-motivation, excellent teamwork spirit, and communication skills. Able to thrive in a fast-paced and dynamic work environment.; Language Proficiency: Proficient in both English and Mandarin for effective work communication in a bilingual environment.; P.S. Mention Tech in Asia Jobs when you apply! Helps keep the good stuff coming \ud83d\ude09","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85579374","Role":"Lead Data Engineer","Company":"AIRR Labs","Location":"Central Region","Publish_Time":"2025-07-07 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85579374","job_desc":"Reporting to the Regional Head of Data & BI, this role requires a hands-on approach to lead a team of data engineers to build and upkeep data pipelines, data warehouse, and analytics platforms to ensure data accuracy, data integrity and security.; Responsibilities; Lead the design, development, and implementation of scalable and efficient data pipelines, databases, and data infrastructure on cloud solutions; Implement ETL processes to integrate data from various sources, ensuring data quality, consistency, and integrity.; Optimize data pipelines, queries, and infrastructure for performance, scalability, and cost-efficiency.; Collaborate with cross-functional teams to gather requirements, assess technological options, and propose architectural solutions.; Mentor, coach, and guide a team of data engineers, providing technical leadership and support.; Requirements; Solid understanding of relational databases.; Extensive experience in Python, SQL, or other scripting languages is required. Experience with AWS services (S3, EC2, EMR, RDS) is a must.; Self-motivated, proactive and excellent communication skills. Experience in ecommerce industry is an advantage.; Bachelor or degrees above in Computer Science, Computer Engineering or other relevant degrees.; 6+ years of experience in data engineering, with at least 3 years in a lead or senior engineering role. Strong technical background in designing and implementing data pipelines, databases, and analytical tools.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85219825","Role":"Core Engineering, Data Software Engineer, Analyst\/ Associate, Singapore","Company":"Goldman Sachs","Location":"Singapore","Publish_Time":"2025-06-27 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85219825","job_desc":"Core Engineering, Data Software Engineer, Analyst\/ Associate, Singapore; Goldman Sachs employs thousands of engineers across many divisions. We write software that powers every aspect of our business. It's no surprise that we collect and analyze vast amounts of data relating to the efficiency of our development processes, the value that we drive through software, and the effectiveness of how we leverage our internal tooling and services to empower development. Our data allows our internal businesses to optimize, focus their attention and manage costs. It allows us to assess the impact and performance of developer productivity initiatives, cost saving initiatives, and manage our huge software inventory.; What We Need; We are seeking a highly skilled Data Software Engineer to design, implement, and maintain robust data systems and pipelines that empower our organization to leverage data effectively. The ideal candidate will come from a software engineering background, with commercial development experience in one or more object-oriented languages (Python, Go, Java, C#). Knowledge of data modeling, pipeline construction, data normalization and sanitization, and data governance would be highly advantageous. They will collaborate with cross-functional teams to ensure the availability, quality, and security of data to support business objectives.; Key Responsibilities; Data Pipeline Development; Design, build, and maintain scalable and efficient data pipelines for processing and transforming large datasets.; Ensure pipelines are optimized for reliability and performance.; Data Modeling and Architecture; Develop and maintain logical and physical data models tailored to business needs.; Optimize data storage solutions for scalability and performance.; Data Quality and Sanitization; Implement processes for data normalization, deduplication, and cleaning to ensure high-quality datasets.; Identify and resolve data inconsistencies, errors, and anomalies.; Data Governance and Security; Establish and enforce data governance standards, including policies for data access, compliance, and privacy.; Implement security measures to protect sensitive data and ensure compliance with regulatory requirements.; Monitoring and Optimization; Develop monitoring solutions to ensure the health and reliability of data pipelines and systems.; Continuously optimize performance, storage, and costs of data infrastructure.; Qualifications; Education:; Bachelor\u2019s or Master\u2019s degree in Computer Science, Engineering, Data Science, or a related field.; Experience:; Min. 1 year (for Analyst)\/ 3 years (for Associate) of experience in software engineering, data engineering or a related role.; Proven experience with data modeling, ETL\/ELT pipelines, and data architecture design.; Proficiency in programming languages such as Python, Java, or Scala.; Strong knowledge of SQL and relational databases (e.g., PostgreSQL, Sybase, SQL Server).; Desirable Technical Skills:; Experience with big data technologies (e.g., Hadoop, Spark) and cloud platforms (e.g., AWS, GCP, Azure).; Familiarity with data integration tools (e.g., Apache Airflow, Talend, Informatica) and streaming technologies (e.g., Kafka, Flink).; Experience with data warehouse technologies (e.g., Snowflake, Redshift, BigQuery).; Soft Skills:; Strong analytical and problem-solving abilities.; Effective communication and collaboration skills to work with diverse teams.; Attention to detail and a proactive approach to ensuring data integrity.; Preferred Qualifications; Experience in conforming to data governance frameworks.; Knowledge of data lake architectures and unstructured data processing.; Familiarity with machine learning workflows and AI-driven data applications.; Certifications in relevant technologies (e.g., AWS Certified Data Analytics, Google Professional Data Engineer).; ABOUT GOLDMAN SACHS; At Goldman Sachs, we commit our people, capital and ideas to help our clients, shareholders and the communities we serve to grow. Founded in 1869, we are a leading global investment banking, securities and investment management firm. Headquartered in New York, we maintain offices around the world. ; We believe who you are makes you better at what you do. We're committed to fostering and advancing diversity and inclusion in our own workplace and beyond by ensuring every individual within our firm has a number of opportunities to grow professionally and personally, from our training and development opportunities and firmwide networks to benefits, wellness and personal finance offerings and mindfulness programs. Learn more about our culture, benefits, and people at GS.com\/careers. ; We\u2019re committed to finding reasonable accommodations for candidates with special needs or disabilities during our recruiting process. Learn more: https:\/\/www.goldmansachs.com\/careers\/footer\/disability-statement.html; \u00a9 The Goldman Sachs Group, Inc., 2023. All rights reserved.; Goldman Sachs is an equal opportunity employer and does not discriminate on the basis of race, color, religion, sex, national origin, age, veterans status, disability, or any other characteristic protected by applicable law.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85652652","Role":"Data Engineer","Company":"Safran Helicopter Engines","Location":"North Region","Publish_Time":"2025-07-10 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85652652","job_desc":"The Data Engineer's mission is to collect data, aggregate it into analytical or BI solutions, and make it available to the systems and\/or organizations that require it. The engineer also carries out developments to collect, cleanse, and structure data within our platform from internal and external information systems.; Roles and Responsibilities:; - Build and develop data models.; - Build APIs to exchange data.; - Propose and implement data retrieval tools, processing, and solutions.; - Implement and\/or industrialize data entry and retrieval algorithms.; - Validate and monitor platform developments.; - Perform maintenance and upkeep of deployed solutions.; - IT Functional Maintenance (incidents and developments),; - POCs, POVs, and MVPs (implemented in QCD),; - Preliminary Projects (solution selection, definition of the provisional schedule, provisional budget, deployment, and risk identification),; - Projects (implemented in QCD),; - Technology monitoring,; - Compliance with the service level of the application solutions in operation; - Manage project life cycle:; Assists project owners in expressing requirements; Manages the design (technical and functional) and specifications; Evaluate software packages with the project owner; Participates in implementation in terms of specific developments or integration; Participates in the definition and implementation of test plans and business acceptance testing; - Project management:; Organizes, coordinates, and leads the entire project management team; Arbitrates any disputes between the team and other stakeholders; Supervises project progress; Coordinates, summarizes, and ensures the quality of approvals issued; Circulates and disseminates information to the project management team; Manages the relationship with the supplier(s) (from the (from contract signing to final project validation); - Technical deployment of the project and implementation of user support actions:; Deploys the new application or service; Organizes maintenance and SLAs; Participates in user training; Organizes user support Guarantees the best quality-cost-deadline match:; Ensures compliance with specifications; Ensures compliance with deadlines and costs; Challenges needs to ensure the convergence of needs\/solutions in compliance with information systems standards; Proposes to the business or project owner, during the project, any changes to objectives (quality, cost, deadline) related to implementation constraints or environmental changes; This role requires collaborations with:; * IT Business Leaders,; * IT Project Owners,; * Safran Group entities within the scope of the Group IT Department.; The ideal candidate would have more than 8 years of relevant working experience in similar role, possess at least a diploma in engineering field and is familiar with SAP.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85530003","Role":"Data Analytics Manager (Data Engineering, Azure, SQL)","Company":"Prestige Professions Pte Ltd","Location":"North-East Region","Publish_Time":"2025-07-07 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85530003","job_desc":"\u2022 Highest remuneration and bonus package in market; \u2022 Promising career prospects; \u2022 Conducive working environment; Job Scopes:; Responsible for developing robust data pipelines, ensuring data quality, and optimizing data storage and retrieval systems to support our organization's data-driven decision-making processes; Leading a team of Data Analysts (local and remote), the Manager, Data Engineer & Data Analyst; Ensures all company-wide reporting and analytical services are delivered to the business teams; Providing leadership in the areas of Data Strategy and Data Analytics and other emerging areas related to the business; Design, develop, and maintain data pipelines, ensuring a smooth flow of data from multiple sources to data warehouses or lakes; Enable, maintain, and support pipelines to provide unified and reliable data sources; Data architecture design and ETL processes; Scalability and optimization of data infrastructure; Job Requirements:; Degree qualification; 5 years of related experience (hands-on technical); Microsoft Office; Proficiency in Azure cloud environment, SQL, PowerBI, SSIS, ETL will be plus; Strong analytical skills; *** Sincere & Interested applicants, kindly forward your *Updated resume (word doc format) to allan(at)prestigeprofessions.com.sg and CC: Allan (R1223894) ***","salary":"$6,000 \u2013 $7,500 per month (SGD)","work_type":"Full time","country":"singapore"}
{"Job_ID":"85608182","Role":"Backend Engineer Intern, Data Infra - Platform (Aug - Dec 2025)","Company":"SHOPEE SINGAPORE PRIVATE LIMITED","Location":"Singapore","Publish_Time":"2025-07-09 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85608182","job_desc":"The Engineering and Technology team is at the core of the Shopee platform development. The team is made up of a group of passionate engineers from all over the world, striving to build the best systems with the most suitable technologies. Our engineers do not merely solve problems at hand; We build foundations for a long-lasting future. We don't limit ourselves on what we can or can't do; we take matters into our own hands even if it means drilling down to the bottom layer of the computing platform. Shopee's hyper-growing business scale has transformed most \"innocent\" problems into huge technical challenges, and there is no better place to experience it first-hand if you love technologies as much as we do.; About the Team:; Shopee will be prioritizing applicants who have a current right to work in Singapore, and do not require Shopee sponsorship of a visa.; Kindly note that you can only be considered in one recruitment process at a time within Sea Group and will be considered for jobs in the order that you have applied.; Our team builds and maintains the company\u2019s big data infrastructure and platforms, ensuring they are stable, efficient, secure, and easy to use. We provide core data capabilities\u2014including storage, batch and real-time computing, querying, and analytics\u2014to support business teams, analysts, and machine learning applications. Our platforms cover the full data lifecycle, from ingestion to processing and monitoring, enabling teams to build reports, dashboards, real-time pipelines, and data-driven insights efficiently.; Job Description:; Responsible for the development of company level big data platform; Communicate effectively with product managers, designing and implementing product features; Writing high-quality, clean and maintainable code using engineering best practices; Requirements:; Bachelor's degree and above in Computer Science or related fields; Relevant experience in Backend Development; Familiar with commonly used languages and framework, such as Java, SpringBoot; In-depth understanding of Data Structures and Algorithms, Networking, OS and other Computer Science fundamentals; Familiar with commonly used Databases, such as MySQL; Familiar with commonly used Middlewares, such as Redis and Kafka; Familiar with HTTP protocols; Familiar with Distributed Systems; Full-time interns preferred; Part-time interns who can commit at least 4 working days a week are also welcome to apply; Familiar with big data technologies is a plus, such as Hadoop, Spark, Presto, etc","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85068650","Role":"Associate, Full Stack Data Engineer (Singapore)","Company":"Nomura Singapore Limited","Location":"Singapore","Publish_Time":"2025-06-20 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85068650","job_desc":"Job title:        Full Stack Data Engineer; Corporate Title:    Associate; Department:        Chief Data Office; Location:        Singapore; Company overview; Nomura is an Asia-based financial services group with an integrated global network spanning over 30 countries. By connecting markets East & West, Nomura services the needs of individuals, institutions, corporates and governments through its three business divisions: Retail, Asset Management, and Wholesale (Global Markets and Investment Banking). Founded in 1925, the firm is built on a tradition of disciplined entrepreneurship, serving clients with creative solutions and considered thought leadership. For further information about Nomura, visit www.nomura.com; Department overview:; The Chief Data Office plays a key role in defining and implementing the firm's data, cloud and AI strategy, driving change through these capabilities, enforcing data, cloud and AI governance for the firm, and elevating Nomura's data culture. Governance remains a critical focus area, and the Chief Data Office, in partnership with Business and Corporate functions, is responsible for ensuring that the firm's data assets are managed in line with the firm's data management framework, policy and standards.; Role Description:; Job Responsibilities:; \u2022    Information delivery & analytics. State-of-the-art expertise across, data\/information preparation, data insight & visualization using BI (or similar tools), and advanced data prediction using AI, ML, DL, etc.; \u2022    AI\/ML Ops. Responsible for integration, deployment and monitoring of AI\/ML products and solutions,; \u2022    Data management. Demonstrate expertise in data management to ensure the analytics products are appropriate\/ethical and well-controlled. Enabling data architecture and delivery of data-analytics platforms and Solutions- on-premises, cloud, and hybrid ensuring adherence and conformance to Nomura standards and policies; \u2022    Be a trusted partner. Shape the information & analytics agenda at Nomura, and work with all of Nomura\u2019s businesses in laying out their information & analytics adoption roadmaps.; \u2022    Risk Mindset: Familiar with risk and controls frameworks and ability to operate with a control mindset; Skills, experience, qualifications and knowledge required:; Core Skills requirement:; \u2022    Designing and developing scalable data pipelines to collect and process large volumes of data from multiple sources.; \u2022    Building physical data models and ETL processes to ensure data quality, integrity, and accessibility.; \u2022    Microservices Development: Building and maintaining highly scalable and fault tolerant microservice, including efficient server-side APIs.; \u2022    Deployment: Hands on with CI\/CD, Jenkins, Ansible, DevOps process, Enterprise integration patterns.; \u2022    Hands-on with programming languages (Python, SQL, Java, Unix scripting etc.) and with orchestration tools like Airflow or Autosys; \u2022    Experience with cloud technologies such as EC2, EMR, Snowflake or similar tools with ability to drive design and data model discussions, hybrid data architecture. ; \u2022    Proficiency in React with hands-on experience in UI development a plus.; \u2022    Ability to understand and integrate cultural differences and work effectively with virtual cross-cultural, cross-border teams.; \u2022    Flexibility to adjust to multiple demands, shifting priorities, ambiguity, and rapid change.; \u2022    Experience with senior stakeholder management will be an added advantage.; \u2022    Excellent communication (verbal, written, listening), presentation, and interpersonal skills.; \u2022    Able to analyze complex situations and derive workable actions.; \u2022    Able to constructively challenge requirements and current state to increase overall value to the firm.; Education and experience; Wide variety of degrees will be considered, however work experience will be of equal, if not greater importance; \u2022    At least 4-year Bachelor\u2019s degree in quantitative fields with minimum of 5 years of relevant data experience in data engineering \/ MLOps, full stack engineering, preferably in financial organizations or Masters in quantitative fields (Computer Science, Statistics or similar) ; \u2022    Experience of working with a multi-cultural, multi-disciplined, globally dispersed teams; \u2022    Certifications in relevant technologies or frameworks are a plus.; Diversity Statement; Nomura is committed to an employment policy of equal opportunities, and is fundamentally opposed to any less favourable treatment accorded to existing or potential members of staff on the grounds of race, creed, colour, nationality, disability, marital status, pregnancy, gender or sexual orientation.; ; DISCLAIMER:  This Job Description is for reference only, and whilst this is intended to be an accurate reflection of the current job, it is not necessarily an exhaustive list of all responsibilities, duties, skills, efforts, requirements or working conditions associated with the job.  The management reserves the right to revise the job and may, at his or her discretion, assign or reassign duties and responsibilities to this job at any time.  ;                                                                                                         Nomura is an Equal Opportunity Employer","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"84067715","Role":"Senior Data Engineer","Company":"Tencent International Service Pte. Ltd.","Location":"Singapore","Publish_Time":"2025-06-17 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/84067715","job_desc":"About the Hiring Team; Tencent Overseas IT has the mission to empower Tencent\u2019s rapid global growth with future ready, global IT platforms, applications and services. We are chartered to lead the Overseas IT strategy, architecture, roadmap and execution. Satisfying our internal\/external customers and becoming a world class global IT team are our top aspirations. What the Role Entails; Tencent Overseas IT aims to empower its rapid global growth with future-ready, global IT platforms, applications, and services. We are chartered to lead the Overseas IT strategy, architecture, roadmap, and execution. Our top aspirations are to satisfy our internal\/external customers and become a world-class global IT team.; Data platform architecture design and pipeline development, ensuring system stability and scalability; Work closely with analysts and product teams to define data standards and key metrics support business decisions; Own the deployment, management, and optimization of big data components such as Airflow, Spark, Flink and data warehouse; Continuously improve data workflows and promote automation and engineering efficiency; Who We Look For; Bachelor\u2019s degree or above in Computer Science or related fields\uff0c3+ years in data engineering; Experience with SQL and Python; and at least one of Java or Scala; Familiar with ETL pipeline design, scheduling, and performance tuning; Familiar with databases and data warehouses such as PostgreSQL, MySQL, StarRocks, ClickHouse; 5Experience with using and managing big data tools like Airflow, Spark, and Flink; Cross-team communication, able to independently engage with business teams; Fluent in Chinese and English is preferred in order to communicate with various stakeholders in headquarters; Equal Employment Opportunity at Tencent; As an equal opportunity employer, we firmly believe that diverse voices fuel our innovation and allow us to better serve our users and the community. We foster an environment where every employee of Tencent feels supported and inspired to achieve individual and common goals.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85044103","Role":"Senior Data Engineer (Frontend)","Company":"Mediacorp Pte Ltd","Location":"Clementi","Publish_Time":"2025-06-19 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85044103","job_desc":"As a Senior Frontend (Full Stack) Engineer, you will be responsible for designing, developing, and maintaining our data product suite. You will work closely with cross-functional teams including data scientists, data engineers, and UX\/UI developers to create scalable, robust, and user-friendly applications. Your expertise in both front-end and back-end development will be crucial in implementing innovative features and ensuring seamless integration between different components of our data products. This role requires strong problemsolving skills, a deep understanding of data processing and visualization techniques, and a passion for delivering high-quality software solutions.; Responsibilities; Collaborate with product owner, data scientists, data engineers and designers to understand product requirements and translate them into technical specifications.; Design and develop full-stack solutions, including user interfaces, APIs, data processing and storage.; Implement data visualization components using modern front-end frameworks and libraries.; Develop scalable and efficient back-end services to handle data from multiple data source with complex business logic.; Optimize application performance and ensure high availability and reliability.; Conduct code reviews, ensure seamless deployment and testing of the data products and provide constructive feedback.; Stay updated with the latest industry trends and best practices in full-stack development, data engineering, and data visualization.; Troubleshoot and resolve issues reported by users, ensuring a smooth user experience.; Continuously improve software development processes and contribute to the overall technical excellence of the team; Qualifications; Bachelor's or master's degree in Computer Science, Software Engineering, or a related field.; 5+ years of professional experience as a Full Stack Engineer, with a focus on datadriven applications.; Strong proficiency in JavaScript, HTML, CSS, and modern front-end frameworks (e.g., React is preferred as we use React in our team).; Expertise in server-side programming languages such as Node.js, Python or Java.; In-depth knowledge of databases and SQL, with experience in designing and optimizing data models.; Familiarity with data processing frameworks (e.g., Apache Spark) and big data technologies (e.g., Kafka) is a plus.; Solid understanding of RESTful APIs and microservices architecture.; Experience with cloud platforms (e.g., Azure, AWS) and containerization technologies (e.g., Docker, Kubernetes).; Proficiency in version control systems (e.g., Git); Excellent problem-solving skills and the ability to debug complex issues in a distributed system.; Strong communication skills and the ability to work effectively in a collaborative team environment.; Prior experience working on data products or analytics platforms is highly desirable.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85222923","Role":"Contract Data Engineer (Healthcare) at Buona Vista, up to $6500","Company":"Success Human Resource Centre Pte Ltd","Location":"One North","Publish_Time":"2025-06-27 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85222923","job_desc":"Job Responsibilities:; Support the design, build, and maintenance of data pipelines to ensure seamless integration and flow of data from multiple systems across a variety of frequencies and fidelities; Prepare, clean, and transform data for analytics and reporting purposes; Create and maintain comprehensive documentation for data processes; Identify and follow through on opportunities for improvements and optimisation; Test data system configurations to increase efficiency; Support the handling and logging of errors; Monitor data system performance; Period:; 6 months contract ; Location:; Buona Vista ; Working Hours:; Monday to Friday, 8.30am to 6pm; Salary:; up to $6500; Job Requirements:; Minimum Degree in Computer Science, Information Technology, Computer Engineering or equivalent.; 3 to 7 years\u2019 experience in developing, implementing and maintaining data pipelines, architecture, and data sets, and deploying pipelines and codes in production.; Proficient in Python with experience in scripting.; Strong SQL skills and SQL server databases.; Experience working with specific AWS services such as S3, Athena, Lambda, IAM and CloudWatch; Ability to relate to business and technical stakeholders on aspects of health informatics and Singapore\u2019s Healthcare Systems; Interested applicants, kindly email your detailed resume (MS Word format is preferred):; tracy@successhrc.com.sg (Reg No: R1107390); Please ensure that applications sent through email are no bigger than 1Mb.; We thank all applicants for your interest but regret to inform that only shortlisted candidates would be notified.; Success Resource Centre Pte Ltd (EA License Number: 04C3201); 160 Robinson Road, #13-07\/08\/09 SBF Center, Singapore 068914; T: 6337 3183 | F: 6337 0329 | W: www.successhrc.com.sg","salary":"$4,500 \u2013 $6,500 per month (SGD)","work_type":"Full time","country":"singapore"}
{"Job_ID":"84249971","Role":"Data Engineer, Associate\/Senior Associate, Data & AI, Technology Consulting","Company":"Ernst & Young Solutions LLP","Location":"Singapore","Publish_Time":"2025-06-17 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/84249971","job_desc":"At EY, you\u2019ll have the chance to build a career as unique as you are, with the global scale, support, inclusive culture and technology to become the best version of you. And we\u2019re counting on your unique voice and perspective to help EY become even better, too. Join us and build an exceptional experience for yourself, and a better working world for all. ; The opportunity; EY DnA is the data and advanced analytics capability within EY Asia-Pacific, with over 500 specialist employees working across multiple industry sectors.; We implement information-driven strategies, data platforms and advanced data analytics solution systems that help grow, optimize and protect client organizations. We go beyond strategy and provide end to end design, build and implementation of real life data environments and have some of the best architects, project managers, business analysts, data scientists, big data engineers, developers and consultants in the region.; We are looking for a Data Engineer (Associate\/Senior Associate) within the DnA team in our Singapore office. This role is offered on a full time basis.; Your key responsibilities; Design, build, and maintain efficient, reusable, and scalable ETL\/ELT pipelines for structured and unstructured data.; Develop and optimize data workflows to support advanced analytics, machine learning models, and reporting tools.; Collaborate with data scientists, analysts, and business stakeholders to gather requirements and ensure data quality and availability; Work with cloud and on-prem data platforms (e.g., AWS, Azure, GCP, Hadoop, or on-prem SQL\/NoSQL systems).; Ensure data integrity, governance, and security best practices across pipelines and data lakes\/warehouses.; Troubleshoot and resolve data issues and performance bottlenecks in real-time and batch pipelines.; Monitor job performance and implement automation and alerting for data operations.; Contribute to documentation, code reviews, and development best practices.; Skills and attributes for success; 2\u20136 years of experience in data engineering or related roles.; Proficient in SQL, Python, or Scala for data processing.; Experience with data orchestration tools like Apache Airflow, DBT, or Luigi.; Familiarity with cloud platforms (AWS Glue, Azure Data Factory, GCP Dataflow, etc.).; Hands-on experience with data warehouses such as Snowflake, BigQuery, or Redshift.; Understanding of data modeling, normalization, and data warehousing concepts.; Exposure to CI\/CD, version control (Git), and agile development practices.; To qualify for the role, you must have; Bachelor\u2019s or Master\u2019s degree in Computer Science, Engineering, or related field.; Knowledge of streaming data frameworks (Kafka, Spark Streaming, Flink) is a plus.; Experience working in consulting or client-facing environments (for Senior Associate roles).; Certifications in AWS\/Azure\/GCP or specific data tools.; What we look for; Highly motivated individuals with excellent problem-solving skills and the ability to prioritize shifting workloads in a rapidly changing industry.; An effective communicator, you\u2019ll be a confident leader equipped with strong people management skills and a genuine passion to make things happen in a dynamic organization.; What we offer; EY offers a competitive remuneration package commensurate with your work experience where you\u2019ll be rewarded for your individual and team performance. We are committed to being an inclusive employer and are happy to consider flexible working arrangements, where this may be needed, guided by our FWA Policy.; Plus, we offer:; Continuous learning: You\u2019ll develop the mindset and skills to navigate whatever comes next.; Success as defined by you: We\u2019ll provide the tools and flexibility, so you can make a meaningful impact, your way.; Transformative leadership: We\u2019ll give you the insights, coaching and confidence to be the leader the world needs.; Diverse and inclusive culture: You\u2019ll be embraced for who you are and empowered to use your voice to help others find theirs.; If you can demonstrate that you meet the criteria above, please contact us as soon as possible.; The exceptional EY experience. It\u2019s yours to build.; Apply now.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85170784","Role":"SAS Data Engineer, Data & Analytics, Technology Consulting","Company":"Ernst & Young Solutions LLP","Location":"Singapore","Publish_Time":"2025-06-17 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85170784","job_desc":"At EY, you\u2019ll have the chance to build a career as unique as you are, with the global scale, support, inclusive culture and technology to become the best version of you. And we\u2019re counting on your unique voice and perspective to help EY become even better, too. Join us and build an exceptional experience for yourself, and a better working world for all. ; The opportunity; EY DnA is the data and advanced analytics capability within EY Asia-Pacific, with over 500 specialist employees working across multiple industry sectors.; We implement information-driven strategies, data platforms and advanced data analytics solution systems that help grow, optimize and protect client organizations. We go beyond strategy and provide end to end design, build and implementation of real life data environments and have some of the best architects, project managers, business analysts, data scientists, big data engineers, developers and consultants in the region.; We are looking for a Data Engineer with SAS Viya skills within the DnA team in our Singapore office. This role is offered on a flexible full time basis.;  Your key responsibilities; Strong analytical and problem-solving skills; Strong drive to excel professionally, and to guide and motivate others; Advanced written and verbal communication skills; Dedicated, innovative, resourceful, analytical and able to work under pressure; Foster an efficient, innovative and team-oriented work environment; Skills and attributes for success; Experience in ETL, Data Engineering, Scripting.; Knowledge and experience in end-to-end project delivery, either traditional SDLC or agile delivery methodologies (or hybrid approaches); Experience in a delivery role on Business Intelligence, Data Warehousing, Big Data or analytics projects; Exceptional communication, documentation and presentation skills and stakeholder management experiences; Experience in business intelligence, data warehousing\/platform, and data strategy projects; To qualify for the role you must have; At least 5 years\u2019 experience as a SAS Engineer with experience in development and  maintenance support; Experience with SAS BASE, Enterprise Guide (EG), Event Stream Processing (ESP), Visual Analytics (VA) , Viya platforms (VI and VA) and Data Integration (DI) are required; Minimum 2yr experience in SAS application maintenance and SAS Management Console (SMC); Minimum 2yrs experience in dealing large data sets in Greenplum \/ Hadoop \/ Oracle \/ DB2; Dashboarding experience with tableau \/ power BI \/ SAS VA is desirable; Experience in shell \/batch scripting; Application packaging and deployment experience across DEV to PROD environments; Experience in maintenance reporting and managing change \/ service requests.; Ideally, you\u2019ll also have; Experience in engaging with both technical and non-technical stakeholders; Consulting experience and background, including engaging directly with clients; Degree in Computer Science or IT or Business Analytics; What we offer; EY offers a competitive remuneration package commensurate with your work experience where you\u2019ll be rewarded for your individual and team performance. We are committed to being an inclusive employer and are happy to consider flexible working arrangements, where this may be needed, guided by our FWA Policy.; Plus, we offer:; Continuous learning: You\u2019ll develop the mindset and skills to navigate whatever comes next.; Success as defined by you: We\u2019ll provide the tools and flexibility, so you can make a meaningful impact, your way.; Transformative leadership: We\u2019ll give you the insights, coaching and confidence to be the leader the world needs.; Diverse and inclusive culture: You\u2019ll be embraced for who you are and empowered to use your voice to help others find theirs.; If you can demonstrate that you meet the criteria above, please contact us as soon as possible.; The exceptional EY experience. It\u2019s yours to build.; Apply now.","salary":"","work_type":"Full time, Paruh waktu","country":"singapore"}
{"Job_ID":"82255060","Role":"Data Architect (Snowflake\/Microsoft Fabric), AI & Data, Technology Consulting","Company":"Ernst & Young Solutions LLP","Location":"Singapore","Publish_Time":"2025-06-17 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/82255060","job_desc":"At EY, we develop you with future-focused skills and equip you with world-class experiences. We empower you in a flexible environment, and fuel you and your extraordinary talents in a diverse and inclusive culture of globally connected teams.; We work together across our full spectrum of services and skills powered by technology and AI, so that business, people and the planet can thrive together. ; We\u2019re all in, are you?; Join EY and shape your future with confidence.; About the opportunity; EY AI & Data is the data and advanced analytics capability within EY Asia-Pacific, with over 500 specialist employees working across multiple industry sectors. We implement information-driven strategies, data platforms and advanced data analytics solution systems that help grow, optimize and protect client organizations. We go beyond strategy and provide end to end design, build and implementation of real-life data environments and have some of the best architects, project managers, business analysts, data scientists, big data engineers, developers and consultants in the region. ; We are seeking a skilled Data Architect with extensive experience in Snowflake and\/or Microsoft Fabric to join our dynamic team. The ideal candidate will be responsible for designing, implementing, and managing data architecture solutions that meet the needs of our organization. You will work closely with data engineers, analysts, and other stakeholders to ensure that our data systems are robust, scalable, and aligned with business objectives.; Key Responsibilities:; Design and implement data architecture solutions using Snowflake and\/or Microsoft Fabric.; Collaborate with cross-functional teams to gather requirements and translate them into technical specifications.; Develop and maintain data models, data flow diagrams, and other architectural documentation.; Ensure data integrity, security, and compliance with industry standards and regulations.; Optimize data storage and retrieval processes for performance and cost efficiency.; Monitor and troubleshoot data architecture issues, providing timely resolutions.; Stay updated with the latest trends and technologies in data architecture and cloud computing.; Mentor and guide junior team members in best practices for data architecture and management.;   Qualifications:; Bachelor\u2019s degree in Computer Science, Information Technology, or a related field; Master\u2019s degree preferred.; Proven experience as a Data Architect or similar role, with a strong portfolio of successful projects.; Extensive experience with Snowflake and\/or Microsoft Fabric, including data modeling, ETL processes, and data warehousing.; Proficiency in SQL and experience with data integration tools.; Strong understanding of data governance, data quality, and data security principles.; Excellent analytical and problem-solving skills.; Strong communication and collaboration skills, with the ability to work effectively in a team environment.; Preferred Skills:; Experience with other cloud platforms (e.g., AWS, Azure, Google Cloud).; Familiarity with big data technologies (e.g., Hadoop, Spark).; Knowledge of machine learning and data analytics concepts.; What working at EY offers; EY offers a competitive remuneration package where you\u2019ll be rewarded for your individual and team performance. We are committed to being an inclusive employer and are happy to consider flexible working arrangements. Plus, we offer:; Continuous learning: You\u2019ll develop the mindset and skills to navigate whatever comes next.; Success as defined by you: We\u2019ll provide the tools and flexibility, so you can make a meaningful impact, your way.; Transformative leadership: We\u2019ll give you the insights, coaching and confidence to be the leader the world needs.; Diverse and inclusive culture: You\u2019ll be embraced for who you are and empowered to use your voice to help others find theirs.; Company description; EY is building a better working world by creating new value for clients, people, society and the planet, while building trust in capital markets.; Enabled by data, AI and advanced technology, EY teams help clients shape the future with confidence and develop answers for the most pressing issues of today and tomorrow.; EY teams work across a full spectrum of services in assurance, consulting, tax, strategy and transactions. Fueled by sector insights, a globally connected, multi-disciplinary network and diverse ecosystem partners, EY teams can provide services in more than 150 countries and territories.; All in to shape the future with confidence.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85173408","Role":"Data Engineer Intern","Company":"Millipede Pte Ltd","Location":"Raffles Place","Publish_Time":"2025-06-25 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85173408","job_desc":"Company; Millipede Pte Ltd; millipede.sg; Designation; Data Engineer Intern; Date Listed; 19 Jun 2025; Job Type; Entry Level \/ Junior Executive; Intern\/TS; Job Period; Immediate Start, For At Least 6 Months; Profession; IT \/ Information Technology; Industry; Computer and IT; Location Name; Raffles Place, Singapore; Address; Raffles Pl, Singapore; Map; Allowance \/ Remuneration; $800 - 1,200 monthly; Company Profile; Millipede Pte Ltd is a Singapore-based technology company specialising in Integrated Digital Delivery (IDD) solutions for the construction and real estate development sectors. Established in 2022, Millipede aims to enhance efficiency, safety, and quality across construction projects by providing a connected platform that facilitates real-time data collection and stakeholder collaboration.; What Millipede Offers; Millipede\u2019s platform is designed to integrate seamlessly into existing workflows, offering tools that cater to various stakeholders in the built environment:; Mobile App: Allows on-site workers & supervisors to report progress, complete checklists in real-time and manage approvals, ensuring accurate and timely data collection.; Web Portal: Provides project managers and administrators with a centralised dashboard to monitor activities, manage stakeholders, and track project timelines across multiple sites.; Milli-Tools*: A suite of customisable tools that aid in accurate progress tracking, safety compliance and quality assurance.; By leveraging these tools, Millipede enables firms to optimise processes, reduce turnaround times, and improve overall project outcomes.; Job Description; We are looking for a skilled and detail-oriented Data Engineer \/ BI Engineer Intern to join our team. This role requires a strong foundation in SQL, data engineering, and dashboard development, along with a passion for data-driven solutions, automation, and exploring data science applications.; At Millipede, we encourage innovation and technical exploration. The construction industry presents a unique mix of challenges and opportunities, where problems are rarely straightforward but always exciting to solve. If you're someone who enjoys tackling complex problems, you'll have the freedom to experiment with cutting-edge tools, frameworks, and technologies to streamline workflows, drive automation, and extract powerful data insights. Millipede is a place where curiosity meets impact - come build something meaningful with us.; Responsibilities; Explore, clean, and manage datasets efficiently with minimal supervision.; Develop and enhance custom customer-facing reports and dashboards tailored to specific client needs.; Set up and manage pipelines for handling large-scale data processing, model execution, and performance monitoring. Experience with workflow automation and optimizing model-driven systems is beneficial; Experiment with new tools and frameworks to streamline data workflows and improve automation.; Work closely with engineering and product teams to ensure data accuracy and consistency.; Required Skills; SQL proficiency \u2013 experience with BigQuery or equivalent.; Strong experience in data transformation and pipeline setup.; Resourceful self-starter with the ability to understand datasets proactively.; Highly meticulous and sensitive to data accuracy.; Strong communication skills \u2013 ability to explain technical concepts to a range of audiences.; Basic UI\/UX familiarity for designing intuitive dashboards.; Problem-solving mindset with a structured approach to open-ended challenges.; Familiarity with Agile methodologies and iterative development processes.; Technologies you\u2019ll work with:; SQL\/ BigQuery; Google Cloud Platform \u2013 Cloud Functions, BigQuery, Firebase etc; Excel \/ Google Sheets; Visualization Tools \u2013 Looker Studio, Tableau, Power BI etc.; Scripting Languages \u2013 JavaScript, Bash, TypeScript; Python \u2013 Pandas, SciPy, Scikit-learn, TensorFlow\/PyTorch etc.; This role requires on-site presence. We value in-person collaboration, as it allows us to better understand and design for the people we serve.; This position is already closed and no longer available.  You may like to view the other latest internships here.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"84359312","Role":"Vice President, Data Engineer, Group Technology","Company":"United Overseas Bank Limited (UOB)","Location":"Singapore","Publish_Time":"2025-06-17 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/84359312","job_desc":"United Overseas Bank Limited (UOB) is a leading bank in Asia with a global network of more than 500 branches and offices in 19 countries and territories in Asia Pacific, Europe and North America. In Asia, we operate through our head office in Singapore and banking subsidiaries in China, Indonesia, Malaysia and Thailand, as well as branches and offices. Our history spans more than 80 years. Over this time, we have been guided by our values \u2013 Honorable, Enterprising, United and Committed. This means we always strive to do what is right, build for the future, work as one team and pursue long-term success. It is how we work, consistently, be it towards the company, our colleagues or our customers.; ; Group Technology and Operations (GTO) provides software and system development, information technology support services and banking operations.; We have centralized and standardized the technology components into Singapore, creating a global footprint which can be utilized for supporting our regional subsidiaries and the branches around the world. We operate and support 19 countries with this architecture to provide a secure and flexible banking infrastructure.; Our Operations divisions provide transactional customer services for our businesses while also focusing on cost efficiency through process improvements, automation and straight through processing.; You will be responsible for the end-to-end software development and support for all work related to Enterprise Data Warehouse (EDW) projects, quarterly change requests, L3 production fixes. This includes software product implementation and administration, application design, development, implementation, testing and support. You will be expected to work on Data team.; ; You will also be responsible for quality assurance of the team\u2019s delivery in conformance with the Bank-defined software delivery methodology and tools. You will partner with other technology functions to help deliver required technology solutions.; Create frameworks, technical features which helps in faster operationalisation of Data models, Analytical models(including AI\/ML) and user generated contents (dashboards, reports etc); Effectively partner with citizen data scientists in enabling faster adoption of AL\/ML model based systems; Independently install, customise and integrate software packages and programs; Carry out POCs involving new data technologies; Design and develop application frameworks for data integration; Create technical documents such as solution design, program specifications for target solutions; Perform design and development of applications which may not be limited to: Software Applications, Data Integration, User Interfaces, Automation Maintain and recommend software improvements to ensure a platform centric management of software applications; Performance tuning; Work with production support team members to conduct root cause analysis of issues, review new and existing code and\/or perform unit testing; Perform tasks as part of a cross functional development team using agile or other methodologies and utilising project management software; Hands-on experience in implementing large scale data warehouse, Data mart & analytics platforms in financial services industry with good functional knowledge of products & services offered in Retail bank \/ Wholesale \/ Global Markets covering some of the following analytics domains:; Experience in Data Modeling, Data mapping for Data Warehouse and Data Marts solutions; Expertise in FSLM or similar industry models; Expertise in Reference data management \u2013 Tools experience such as MDM (Master Data Management); Experience in functional domain - Retail , Wholesale, Compliance , Digital; Experience in analytics - Retail Analytics; Expertise in design of role based fine grained access control; Designing cloud ready data solutions, Virtualization  ; Technical skillsets -ML Model operationalization, Building Data Pipelines and Hadoop based Data marts; Expertise in implementing Big Data frameworks using multiple SQL engine such as Spark, Impala, Hive, etc; Expertise in implementing Hadoop based Data mart using Spark based framework (Java, Scala, Pyspark); Leverage LLM model to build intelligent data applications (eg: Natural Language based SQL generation); Experience in end to end AI \/ ML life cycle (Data pipeline, model training, Model development, deployment, fine tuning, etc); Expertise in building Data federation solution(Trino, Presto, Dremio, Query Grid) along with Data caching \/ Indexing; Experience in working with any of Data Catalog tools and Automating Data Quality checks using frameworks; Good Working experience in core technical area using Python, Java, PySpark and Scala; Expertise in Cloudera CDP components; Good knowledge in developing Spark based ingestion framework (Java, Scala, Pyspark); Experience in building and operationalising feature pipeline to support AI\/ML model execution, data pipelines for supporting large scale data warehouse\/data marts; Additional requirements -   2 to 3 technical certifications from enclosed list:; Cloudera Hadoop distribution     \u2013 Hive, Impala, Spark, Kudo, Kafka, Flume; Teradata             \u2013 Bteq, Query Grid, GCFR, MDM, TAS, Data Mover, BAR; Informatica Data Integration     \u2013 PC, IDR, BDM, MM, IDQ, EDC; Data modelling tools (Erwin); QlikSense; Microsoft \u2013 R; Data science workbenches \u2013 Cloudera Machine Language, Jupyter, DataRobot, H2O.AI, IBM DSX; Data Virtualization tool (Denodo, Dremio); AS400; Language \u2013 SQL, Java, Python, Scala, Pyspark; Automation \/ scripting \u2013 CtrlM, Shell Scripting, Groovy ; UOB is an equal opportunity employer. UOB does not discriminate on the basis of a candidate's age, race, gender, color, religion, sexual orientation, physical or mental disability, or other non-merit factors. All employment decisions at UOB are based on business needs, job requirements and qualifications. If you require any assistance or accommodations to be made for the recruitment process, please inform us when you submit your online application.; Apply now and make a difference.; Competencies; 1. Strategise; 2. Engage; 3. Execute; 4. Develop; 5. Skills; 6. Experience","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"84725676","Role":"Backend Software Engineer - (Data Integration)","Company":"TikTok Pte. Ltd.","Location":"Singapore","Publish_Time":"2025-06-17 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/84725676","job_desc":"Responsibilities; About the Team The Data Management Suite team is building products that cover the whole lifecycle of data pipeline, including data ingestion and Integration, data development, data catalog, data security and data governance. These products support various businesses, so data engineers and data scientists could greatly boost their productivity.; As a software engineer in the data management suite team, you will have the opportunity to build, optimize and grow one of the largest data platforms in the world. You'll have the opportunity to gain hands-on experience on core systems in the data platform ecosystem. Your work will have a direct and huge impact on the company's core products as well as hundreds of millions of users.; Responsibilities: - Responsible for the design and development of the computing architecture of data integration , and support data integration requirements delivery across different business lines in whole ByteDance;; - Work closely with partner teams across the company and all over the world; - Optimize the performance and stability of real-time data integration services; - Participate in the customization and improvement of the Flink kernel, and maintain cooperation with the open source community; Qualifications; Minimum Qualifications:; Bachelor's degree in Computer Science or equivalent practical experience;; 3+ years of experience in at least one backend language, like Java, Scala, Go etc;; 3+ years of experience in software development, and with data structures\/algorithms;; 3+ years of experience with design and architecture, and testing and launching software products;; Preferred Qualifications:; Be familiar with web frameworks will be a plus, like Spring boot etc.; 2 year experience in Storm\/SparkStreaming\/Flink real-time computation and development experience, to the community contribution patch is preferred ;; 2 year experience in data platform related product development or big data technologies (such as Flink, Clickhouse, kafka etc.) will be a plus;","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85817271","Role":"Senior Software Developer (Java\/Big Data)","Company":"Synechron","Location":"Raffles Place","Publish_Time":"2025-07-16 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85817271","job_desc":"Job Responsibilities:; Lead technical study into a propose solution, while involving expertise from infrastructure big data expert, business analyst requirement.; Document proposed design and develop the solution.; Implicitly ensure all CI-CD artefacts are part of the solution.; Perform code review while fostering knowledge and coaching best practices to team members.; Interact and provide reporting to project managers.; Monitor technical risk and escalate appropriately to management.; Research, design, and develop software.; Analyse user needs and develop software solutions.; Update software, enhances existing software capabilities, and develops and direct software testing and validation procedures.; Work with other engineers to integrate hardware and\/or software systems.; The position requires autonomy and reliability in performing duties with initiatives and leadership when it comes to all non-functional deliverables such as testing tools, mocking objects, production monitoring concerns, quality control including performance and load testing.; Job Requirements:; At least a Bachelor\u2019s degree in any of these faculties: Computer Science\/Information Technology\/Programming & Systems Analysis.; At least 8 years in Software development.; At least 5 years in Java\/J2EE development.; Hands on Data ingest and data processing technology like Spark streaming and Spark.; Hands on Messaging systems like Kafka, Flume or ActiveMQ, MQSeries or RabitMQ.; Hands on knowledge on Hadoop (preferably Hortonworks distribution) - HDFS, HBase, Hive, ORC\/Parquet.; Build tool - Maven\/sbt\/ant, UML, Restful web services, Jenkins\/Team City, Source management \u2013 SVN\/GIT, TDD using Junit, Jira\/QC.; Interested candidates are invited to submit application via https:\/\/www.linkedin.com\/jobs\/view\/4264439774","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"78534391","Role":"Data Engineer (High Speed Packet Processing)","Company":"Csit","Location":"Central Region","Publish_Time":"2025-06-17 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/78534391","job_desc":"CSIT develops products to advance the national security interests of Singapore. Our products are used in a wide range of operations, including but not limited to, Counter-terrorism and Computer Network Defence. We are looking for talented engineers to build mission critical products that identify, analyse and disrupt threats.; Our product teams build high performance systems that need to crunch through enormous amount of data in real-time. As a High Speed Packet Processing Data Engineer, you will work on developing high performance DPI engine. You will add new functionalities, identify bottlenecks, optimize and re-design components where necessary. Your contributions will evolve our DPI engine to meet complex and ever-evolving user requirements.; Responsibilities; Identify and prototype new products that enhance existing capabilities or provide new opportunities; Design, develop, test, deploy, maintain and improve software based on best practices; Work with the product manager, software engineers and stakeholders to build solutions and gain novel insights to complex problems; As CSIT is an agency under the Ministry of Defence (Singapore), only Singapore Citizens will be considered.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85742158","Role":"Site Reliability Engineer - Data Management Suite","Company":"TikTok Pte. Ltd.","Location":"Central Region","Publish_Time":"2025-07-14 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85742158","job_desc":"About TikTok; TikTok is the leading destination for short-form mobile video. At TikTok, our mission is to inspire creativity and bring joy. TikTok's global headquarters are in Los Angeles and Singapore, and we also have offices in New York City, London, Dublin, Paris, Berlin, Dubai, Jakarta, Seoul, and Tokyo.; Why Join Us; Inspiring creativity is at the core of TikTok's mission. Our innovative product is built to help people authentically express themselves, discover and connect \u2013 and our global, diverse teams make that possible. Together, we create value for our communities, inspire creativity and bring joy - a mission we work towards every day.; We strive to do great things with great people. We lead with curiosity, humility, and a desire to make impact in a rapidly growing tech company. Every challenge is an opportunity to learn and innovate as one team. We're resilient and embrace challenges as they come. By constantly iterating and fostering an \"Always Day 1\" mindset, we achieve meaningful breakthroughs for ourselves, our company, and our users. When we create and grow together, the possibilities are limitless. Join us.; Diversity & Inclusion; TikTok is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives. Our platform connects people from across the globe and so does our workplace. At TikTok, our mission is to inspire creativity and bring joy. To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach. We are passionate about this and hope you are too.; Job highlights; Industry experts, Meals provided, Competitive compensation, Flexible hours; Responsibilities; About the Team; The Data Management Suite team is building products that cover the whole lifecycle of data pipeline, including data ingestion and Integration, data development, data catalog, data security and data governance. These products support various businesses, so data engineers and data scientists could greatly boost their productivity.; As a software engineer in the data management suite team, you will have the opportunity to build, optimize and grow one of the largest data platforms in the world. You'll have the opportunity to gain hands-on experience on core systems in the data platform ecosystem. Your work will have a direct and huge impact on the company's core products as well as hundreds of millions of users.; Responsibilities:; Be responsible for the production stability for big data development and governance systems;; Engage in and improve the whole lifecycle of service, from inception and design, through to deployment, operation and refinement;; Maintain services once they are live by measuring and monitoring availability, latency and overall system health. Practice sustainable incident response and blameless postmortems;; Establish best engineering practice for engineers as well as non-technical people;; Design and implement reliable, scalable, robust and extensible big data systems that support core products and business;; Qualifications; Minimum Qualifications; Bachelor's degree in Computer Science, a related technical field involving software or systems engineering, or equivalent practical experience; Experience with site reliability engineering, monitoring, alerting for big data related systems; Experience writing code in Java, Go, Python or a similar language; Preferred Qualifications; Knowledge about a variety of strategies for ingesting, modeling, processing, and persisting data, ETL design, job scheduling and dimensional modeling;; Familiarity with running production grade services at scale and understanding cloud native technologies and networking;; Experience developing tools and APIs to reduce human interaction with systems and applications using a variety of coding and scripting standards;; Expertise in designing, analyzing, and troubleshooting large-scale distributed systems is a plus (Hadoop, M\/R, Hive, Spark, Presto, Flume, Kafka, ClickHouse, Flink or comparable solutions);; Systematic problem-solving approach, coupled with effective communication skills and a sense of drive;","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85167769","Role":"Data Engineer (High Speed Packet Processing)","Company":"Csit","Location":"Singapore","Publish_Time":"2025-06-17 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85167769","job_desc":"CSIT develops products to advance the national security interests of Singapore. Our products are used in a wide range of operations, including but not limited to, Counter-terrorism and Computer Network Defence. We are looking for talented engineers to build mission critical products that identify, analyse and disrupt threats.; Our product teams build high performance systems that need to crunch through enormous amount of data in real-time. As a High Speed Packet Processing Data Engineer, you will work on developing high performance DPI engine. You will add new functionalities, identify bottlenecks, optimize and re-design components where necessary. Your contributions will evolve our DPI engine to meet complex and ever-evolving user requirements.; Responsibilities; Identify and prototype new products that enhance existing capabilities or provide new opportunities; Design, develop, test, deploy, maintain and improve software based on best practices; Work with the product manager, software engineers and stakeholders to build solutions and gain novel insights to complex problems; As CSIT is an agency under the Ministry of Defence (Singapore), only Singapore Citizens will be considered.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85688942","Role":"Data Engineer - Risk Applications","Company":"Sanderson-Ikas","Location":"Singapore","Publish_Time":"2025-07-10 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85688942","job_desc":"Sanderson iKas, because we\u2019re better and stronger together.; In 2021, we saw the partnership of the Sanderson - iKas International Singapore and Hong Kong business' which welcomed a whole host of people and a whole lot of opportunity. This partnership was a testament of the hard work and dedication from all. The merging of both businesses will create a synergy between us, with one common goal, to create more opportunities for our clients and candidates. The history, legacy, and strong client relationships that iKas holds in the APAC region, paired with the growth and success of Sanderson, offers a solid and dynamic solution to current and future clients and candidates. Who are we? Sanderson are a UK based recruitment business with over 40-years\u2019 experience, heritage, and success, with partnership playing a big role in everything they do. They also offer four specialist services under one name, Recruitment, Solutions, Executive Search, and Projects, all built with collaboration, understanding and flexibility.; iKas are an international recruitment solutions business, who\u2019s consultants hold both local and regional knowledge to support you with your business\u2019s technology, digital and financial services recruitment initiatives. iKas have earnt a reputation for providing innovative resourcing solutions to fill critical organisational skills-gaps. So, what\u2019s changing? Well, at the back of the business not much but we are having a cool rebranding of our new joint offices in SG and HK. There will be no change in how both Sanderson and iKas work with you (same recruitment licenses, terms of business and consultant points of contact). But by coming together, we will be able to do more for you, without you having to do anything. Sanderson iKas Singapore and Hong Kong are not only a permanent and contract recruitment business but a powerful force of innovative and flexible solutions. With Sanderson also offering services that cover a wide range of total support with divisions in Recruitment, Solutions, Executive Search, and Projects, which they now offer to clients internationally. The integrity of the iKas brand perfectly compliments Sanderson\u2019s offering, making for a strong future that maximises on the experience, abilities, and expertise of each business.; If you want to know more about this partnership then do not hesitate to get in touch. We will be happy to introduce you, our clients, to both our SG and HK businesses so giving you an insight on how Sanderson-iKas can help you and your company.","salary":"","work_type":"Kontrak\/Temporer, Full time","country":"singapore"}
{"Job_ID":"85068729","Role":"Sales - Software Engineer (Data and AI Enablement), Singapore Hub","Company":"Apple Inc.","Location":"Singapore","Publish_Time":"2025-06-20 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85068729","job_desc":"Summary; Posted: Jun 20, 2025; Weekly Hours: 40; Role Number:200609985; The people here at Apple don't just create products - they create the kind of wonder that's revolutionized entire industries. It's the diversity of those people and their ideas that inspires the innovation that runs through everything we do, from amazing technology to industry-leading environmental efforts. Join Apple, and help us leave the world better than we found it.; We work in a startup atmosphere where individuals take ownership and have significant impact on the final product. We are a dynamic team within Apple\u2019s Worldwide Sales organization, Data Solutions & Initiatives\u2014focused on driving innovation through product design, engineering, and portfolio management. In our startup-like environment, we move quickly, experiment boldly, and expect our team to take full ownership of what they deliver.; We\u2019re looking for a hands-on Software Engineer to build and operate the data infrastructure that powers analytics, automation, and AI across our business. You\u2019ll work on distributed data systems, cloud-native services, and internal tooling that makes data discoverable, trustworthy, and ready for intelligent applications. This role is part of our Singapore engineering hub and works closely with our US-based team to deliver reliable, scalable data solutions that fuel decision-making, modeling, and business operations.; Description; You\u2019ll design and build core components of our internal data platform, spanning ingestion pipelines, semantic layers, and metadata systems. You\u2019ll help bridge open source technologies (e.g., Kafka, Spark, Iceberg) with our internal ecosystem\u2014shaping how teams discover, use, and trust data for analytics and AI workloads. You\u2019ll collaborate with other engineers, product managers, program managers, and end users to understand data needs and evolve platform capabilities that improve scale, quality, and usability.; This is a hands-on engineering role focused on systems thinking, technical craftsmanship, and delivering tools that unlock real business value. Key Responsibilities; Build scalable, cloud-native data systems that support data exploration, reporting, and production ML\/AI use cases; Integrate open-source components with internal tools and APIs to streamline platform usability; Develop and maintain data ingestion pipelines, metadata services, and performance-optimized storage layers; Ensure the platform supports AI-readiness, including high-quality, discoverable, and semantically rich data; Collaborate with internal customers to understand workflows and shape new platform features; Partner with engineers, EPMs, and US-based teams to ensure alignment, reusability, and shared standards; Support production systems through monitoring, debugging, and operational improvements; Minimum Qualifications; 4+ years of experience building distributed data applications or cloud-native platforms; Proficiency in Python, Scala, or Java, with experience developing scalable and maintainable systems; Strong SQL skills and experience with cloud data warehouses (e.g., Snowflake, BigQuery); Experience with modern data infrastructure tools (e.g., Spark, Kafka, Airflow, Iceberg); Understanding of BI and analytics needs, and experience building for internal business use cases; Preferred Qualifications; Experience designing and building cloud-based applications, APIs, and data services; Familiarity with AI\/ML pipeline enablement (e.g., feature engineering, real-time pipelines, metadata); Experience with Kubernetes, distributed compute frameworks, or containerized environments; Hands-on experience integrating with business intelligence or visualization tools (e.g., Streamlit, Tableau, Looker); Exposure to anomaly detection, forecasting, or GenAI use cases and the data requirements they demand; Experience working in global teams or serving as a technical contributor in a regional hub","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"84522772","Role":"Site Reliability Engineer - Data Management Suite","Company":"TikTok Pte. Ltd.","Location":"Singapore","Publish_Time":"2025-06-17 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/84522772","job_desc":"Responsibilities; The Data Management Suite team is building products that cover the whole lifecycle of data pipeline, including data ingestion and Integration, data development, data catalog, data security and data governance. These products support various businesses, so data engineers and data scientists could greatly boost their productivity. As a software engineer in the data management suite team, you will have the opportunity to build, optimize and grow one of the largest data platforms in the world.; You'll have the opportunity to gain hands-on experience on core systems in the data platform ecosystem. Your work will have a direct and huge impact on the company's core products as well as hundreds of millions of users. Responsibilities:; Be responsible for the production stability for big data development and governance systems; Engage in and improve the whole lifecycle of service, from inception and design, through to deployment, operation and refinement; Maintain services once they are live by measuring and monitoring availability, latency and overall system health.; Practice sustainable incident response and blameless postmortems; Establish best engineering practice for engineers as well as non-technical people; Design and implement reliable, scalable, robust and extensible big data systems that support core products and business;; Qualifications; Minimum Qualifications; Bachelor's degree in Computer Science, a related technical field involving software or systems engineering, or equivalent practical experience; Experience with site reliability engineering, monitoring, alerting for big data related systems; Experience writing code in Java, Go, Python or a similar language; Preferred Qualifications; Knowledge about a variety of strategies for ingesting, modeling, processing, and persisting data, ETL design, job scheduling and dimensional modeling;; Familiarity with running production grade services at scale and understanding cloud native technologies and networking;; Experience developing tools and APIs to reduce human interaction with systems and applications using a variety of coding and scripting standards;; Expertise in designing, analyzing, and troubleshooting large-scale distributed systems is a plus (Hadoop, M\/R, Hive, Spark, Presto, Flume, Kafka, ClickHouse, Flink or comparable solutions);; Systematic problem-solving approach, coupled with effective communication skills and a sense of drive;","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"77441074","Role":"Senior Data Engineer","Company":"Ministry of Defence","Location":"Singapore","Publish_Time":"2025-06-17 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/77441074","job_desc":"About the role; You have a high level of autonomy to design, build and maintain data solutions to marshal with Joint Intelligence Command (JIC)'s operational data across its suite of sensors.; What you will be working on; Design, build, and maintain data solutions for JIC's operational data; Develop software and application tools for users; Create solutions for efficient data consumption from data warehouses; Enable self-service analytics for intelligence analysts; Build and maintain data pipelines; Work with containerisation technology across Docker and Kubernetes environments; Challenge(s); Understanding and keeping pace with evolving digital and operational landscapes; Developing effective software and application tools for diverse user needs; What we are looking for; Education in Engineering, Information Technology or a related field; At least 6 years of experience in software engineering; Expertise in building and maintaining data pipelines with Kafka and Spark; Proficiency in at least one programming language; Willingness to work with Python and Java; Solid grasp of containerisation technology; Comfortable working across Docker and Kubernetes environments; (Only shortlisted candidates will be notified.); Apply; Only shortlisted candidates will be notified.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"84124393","Role":"Data Engineer - Global E-Commerce (Governance Service - Security Platform)","Company":"TikTok Pte. Ltd.","Location":"Singapore","Publish_Time":"2025-06-17 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/84124393","job_desc":"Responsibilities; Global E-Commerce is a content e-commerce business with international short video product as the carrier. It is committed to becoming the first choice for users to discover and purchase good products with affordable prices. Global E-Commerce business team hopes to provide users with more tailored and efficient consumption experience, enabling merchants to receive reliable platform services in different scenarios such as live e-commerce, short video content e-commerce, thereby making more affordable and high-quality products easily accessible and improving lives.; The Global E-Commerce's Governance and Experience is a global team responsible for ensuring a safe and trustworthy marketplace not only for our buyers but also for our sellers and creators. We continually work on areas such as risk detection abilities, fairness, and sustainability of the E-Commerce ecosystem, content and commodity quality, and friction-free experiences to drive improvement. Responsibilities:; Responsible for the offline data warehouse construction of global e-commerce security, including data layering, model design, ETL process, etc.; Establish data warehouse standards for global e-commerce security to ensure data accuracy and consistency.; Collaborate with the security team in optimising their offline tasks to ensure system stability and improving resource efficiency.; Assist in identifying risk indicators through data analysis to support the development of key security projects.; Support real-time data processing pipelines, including tagging systems and real-time ETL infrastructure.; Visualise, interpret, and report data findings and may create dynamic data reports as well.; Qualifications; Minimum Qualifications:; Bachelor's or higher degree in Computer Science, Information Technology, Programming & System Analysis, Science (Computer Studies) or related discipline.; Candidates should have at least 5 years of experience in big data ecosystem development, familiar with technologies such as Spark, Flink, Clickhouse, Hadoop, and practical experience with Lambda\/Kappa architectures.; Proficient in data warehouse implementation methodologies, with a deep understanding of data warehouse systems, and experience supporting real-world business scenarios.; Experience in SQL performance tuning, with an understanding of Hive SQL development.; Preferred Qualifications:; Candidates with deep experience in real-time data warehouse construction.; Data-sensitive with strong business understanding, excellent logical reasoning skills, and some data analysis capabilities.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85068770","Role":"Lead Full Stack Data Engineer (2 year contract)","Company":"StarHub Ltd","Location":"Singapore","Publish_Time":"2025-06-17 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85068770","job_desc":"We seek a seasoned Full Stack developer with strong interest and hands-on data engineering skills to design, develop, and deploy AI-powered, cloud-based products. You will own frontend\/backend development, database\/schema design, data pipelines, and integrate user-facing features with data services, collaborating closely with data science and infrastructure teams to deliver production-grade solutions.; As a Lead Full Stack Data Engineer, you will:; \u00b7 Architect & Build Full Stack Applications; \u2022 Design responsive UIs using Next.js, React, Vue, or Angular; \u2022 Implement server-side logic, REST\/GraphQL APIs, and microservices with Nest.js, Node.js, or Express; \u2022 Ensure seamless data flow, authentication (JWT\/OAuth2), and caching (Redis); \u00b7 Design & Maintain Data Pipelines & Databases \u2022 Define data models for relational (PostgreSQL, Redshift) or document stores (MongoDB); \u2022 Develop ETL\/ELT pipelines (PySpark, Airflow) to load data into warehouses; \u2022 Implement scalable storage (tables, indexes, partitions, materialized views) and tune queries; \u00b7 Integrate with Data Services & APIs; \u2022 Consume analytics\/ML endpoints and real-time streams (Kafka, Kinesis); \u2022 Implement efficient data-fetching on the frontend (pagination, caching, debouncing); \u2022 Design endpoints serving aggregated or pre-processed data; manage API versioning\/documentation (Swagger); \u00b7 Implement CI\/CD & DevOps Collaboration; \u2022 Define CI\/CD pipelines (GitLab CI\/CD) for both applications and data workflows; \u2022 Containerize components with Docker; orchestrate with Kubernetes, Docker Compose, or ECS Fargate; \u2022 Collaborate on cloud provisioning (Terraform, CloudFormation) and manage secrets (AWS Secrets Manager); \u00b7 Develop Dashboards & Visualizations; \u2022 Build dynamic charts with D3.js, ECharts, or Recharts to surface key metrics; \u2022 Create real-time data displays using WebSockets or polling; \u2022 Implement frontend data validation (date pickers, filters, drill-downs); \u00b7 Mentorship & Collaboration; \u2022 Mentor junior engineers; conduct code reviews and share best practices; \u2022 Work with product, infra, delivery\/sales specialist teams to refine requirements, automate tests, and enforce security standards; \u00b7 Bachelor\u2019s or Master\u2019s in CS, Software Engineering, Data Science, or equivalent experience; \u00b7 6+ years as a Full Stack developer with demonstrable data engineering involvement; \u00b7 Proficient in JS\/TypeScript frameworks: Next.js, React, Vue, or Angular; \u00b7 Strong Server-side Rendering (Next.js). Node.js experience with Nest.js or Express; RESTful\/GraphQL API design;; \u00b7 ETL\/ELT pipeline development (PySpark, Airflow) and data modeling for PostgreSQL, Redshift, or MongoDB; \u00b7 Experience integrating real-time streams (Kafka, Kinesis) and consuming ML\/analytics endpoints; \u00b7 Define scalable storage (tables, indexes, partitions, materialized views) and perform query tuning (sort keys, distribution keys, vacuum); \u00b7 CI\/CD pipeline creation (GitLab CI\/CD) and containerization (Docker, Kubernetes); \u00b7 Infrastructure as Code: Terraform or CloudFormation; secret management (AWS Secrets Manager); \u00b7 Cloud experience (AWS) deploying full stack apps and data pipelines (S3, EMR, Redshift); \u00b7 Unit\/integration testing (Jest, Mocha, pytest) and E2E testing (Cypress, Playwright); \u00b7 Strong problem-solving, attention to detail, and ability to mentor and collaborate cross-functionally; Nice to Have; \u00b7 Familiarity with serverless architectures (AWS Lambda) and PWA principles; \u00b7 Exposure to vector databases, data mesh or lakehouse architectures; \u00b7 Participation in GenAI POCs (RAG pipelines, Agentic AI demos); \u00b7 Passion for UI\/UX patterns, accessibility, and developer productivity; \u00b7 Client-facing experience in data-driven or AI\/ML projects; \u00b7 Ability to travel 10\u201330%; This is a Malaysia-based role collaborating closely with Singapore team","salary":"","work_type":"Kontrak\/Temporer","country":"singapore"}
{"Job_ID":"85155664","Role":"Site Reliability Engineer (OLAP Database) - Data Platform","Company":"TikTok Pte. Ltd.","Location":"Singapore","Publish_Time":"2025-06-17 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85155664","job_desc":"Responsibilities; About TikTok TikTok is the leading destination for short-form mobile video. Our mission is to inspire creativity and bring joy. TikTok has global offices including Los Angeles, New York, London, Paris, Berlin, Dubai, Singapore, Jakarta, Seoul, and Tokyo. Why Join Us Creation is the core of TikTok's purpose. Our platform is built to help imaginations thrive. This is doubly true of the teams that make TikTok possible. Together, we inspire creativity and bring joy; a mission we all believe in and aim towards achieving every day.; To us, every challenge, no matter how difficult, is an opportunity; to learn, to innovate, and to grow as one team. Status quo? Never. Courage? Always. At TikTok, we create together and grow together. That's how we drive impact; for ourselves, our company, and the communities we serve.; Join us. About the team TikTok and affiliate are developing the next-generation high-performance analytical database, with a mission to enable efficient and real-time data-driven decision-making on PB-level data sets. The initial product was forked from Clickhouse, after which large re-architecture had been taken place.; The product now not only improves the efficiency of Clickhouse but also fits into the elastic cloud-native infrastructure with better scalability and resource utilization. With years of polishment in the internal EB-level scenarios, we are now ready to serve our business partners via various cloud vendors. What you will be doing:; Responsible for the SLA system, disaster recovery system, and fault self-healing of ByteDance's OLAP products to ensure continuous availability of business operations.; Responsible for all BU (Business Unit) teams using ByteDance's data warehouse products, continuously improving service quality and user experience, and working directly with product and research teams to promote the resolution of customer issues and ongoing product optimization.; Responsible for the development, automation, and continuous iteration of the SRE platform for ByteDance's big data products, guiding the operation and maintenance mode of the product towards digital and intelligent development.; Research, design, and develop computer and network software or specialised utility programs.; Analyse user needs and develop software solutions, applying principles and techniques of computer science, engineering, and mathematical analysis.; Update software, enhances existing software capabilities, and develops and direct software testing and validation procedures.; Work with computer hardware engineers to integrate hardware and software systems and develop specifications and performance requirements.; Qualifications; Minimum bachelor's degree in Computer Science or a related technical background involving software\/system engineering or equivalent working experience; At least 3 year of production-level experience in either Python, Shell, Java or; Go; Familiar with Linux, network, and other system operation and maintenance skills; Familiar with open-source or commercial technologies such as ClickHouse, Hadoop, Doris, and Kubernetes, with experience in practical big data commercial application development.; Proficient in maintaining common SRE\/DevOps open-source components, troubleshooting, and problem-solving skills.; Good communication, presentation, and logical thinking skills, as well as good service and collaboration awareness.; Strong sense of responsibility, strong resistance to pressure, and willingness to challenge technological limits.; TikTok is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives. Our platform connects people from across the globe and so does our workplace. At TikTok, our mission is to inspire creativity and bring joy. To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach. We are passionate about this and hope you are too.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85219557","Role":"Senior Principal Consultant (Data Platform Engineering)","Company":"National University of Singapore","Location":"National University Of Singapore","Publish_Time":"2025-06-27 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85219557","job_desc":"Company description:; ; The National University of Singapore is the national research university of Singapore. Founded in 1905 as the Straits Settlements and Federated Malay States Government Medical School, NUS is the oldest higher education institution in Singapore; ; ; Job description:; ; Job Purpose; We are seeking an experienced Data Platform Engineer to manage data platform operations, and to design and deliver ELT\/ETL frameworks at NUS.; The role will be responsible for addressing stakeholders' needs regarding data platform operations and ensuring alignment with business requirements. The role will also co-lead the design, development and maintenance of metadata driven ELT\/ETL frameworks in NUS.; This is a hands-on role, and the candidate should have at least 5 years of active, hands-on design and development experience.; Role and Responsibilities; Technical Expertise; \u2022 Participate in the technical design of data platforms and their technologies, including their framework, architecture, standards, and guidelines.; \u2022 Stay up-to-date with emerging data platform technologies and industry trends to drive innovation.; \u2022 Ensure adherence to best practices in ELT\/ETL frameworks design, development, testing, and maintenance.; \u2022 Provide technical guidance and expertise to the different stakeholders on data platform operations, helping to solve complex technical challenges.; Leadership Management; \u2022 Provide guidance to the technical data team on using the data platform and ELT\/ETL frameworks.; \u2022 Foster collaboration and high-performance culture within the team.; Delivery Management; \u2022 Ensure timely and high-quality ELT\/ETL frameworks design and delivery, aligned with project objectives, scope, and timelines.; Quality Assurance; \u2022 Implement quality control and testing procedures to guarantee the high-quality delivery.; \u2022 Ensure alignment with the university's regulatory and compliance requirements.; Qualifications and Requirements; \u2022 Bachelor's degree in Computer Science, Information Technology, or a related field.; \u2022 Minimum of 5 years of experience in technical leadership roles, with a strong focus on data platforms, ELT\/ETL frameworks, and data engineering solutions.; \u2022 Strong problem-solving and critical-thinking skills, with a proven track record of effectively addressing technical challenges and risk mitigation.; \u2022 Strong knowledge of quality assurance processes, encompassing testing methodologies, and quality control procedures.; \u2022 Relevant experience in designing, developing, and supporting ELT\/ETL frameworks and data engineering solutions.; \u2022 Strong verbal, written and interpersonal communication skills with the ability to interact and communicate effectively with all levels of management, users, and vendors.; \u2022 Must be a good team player, proactive in nature, fast learner, highly organized and go-getter attitude with can-do spirit.; Technical Expertise; \u2022 Strong working experience in managing enterprise data platform operations.; \u2022 Strong working experience in end-to-end development of ETL frameworks and data engineering solutions for data lakes and data warehouses.; \u2022 Strong working experience in Azure Data services, preferably Microsoft Fabric.; \u2022 Strong database working experience for transactional and analytics systems, including querying and tuning large, complex data sets and performance analysis.; \u2022 Working experience in big data and DataSecOps is an added advantage.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85155657","Role":"Backend Software Engineer, TikTok Data Ecosystem (Data Lake)","Company":"TikTok Pte. Ltd.","Location":"Singapore","Publish_Time":"2025-06-17 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85155657","job_desc":"Responsibilities; TikTok will be prioritizing applicants who have a current right to work in Singapore and do not require TikTok's sponsorship of a visa. About TikTok TikTok is the leading destination for short-form mobile video.; Our mission is to inspire creativity and bring joy. TikTok has global offices including Los Angeles, New York, London, Paris, Berlin, Dubai, Singapore, Jakarta, Seoul and Tokyo. Why Join; Us Creation is the core of TikTok's purpose. Our platform is built to help imaginations thrive.; This is doubly true of the teams that make TikTok possible. Together, we inspire creativity and bring joy - a mission we all believe in and aim towards achieving every day. To us, every challenge, no matter how difficult, is an opportunity; to learn, to innovate, and to grow as one team.; Status quo? Never. Courage?; Always. At TikTok, we create together and grow together. That's how we drive impact - for ourselves, our company, and the communities we serve.; Join us. About The Team; The TikTok Data Ecosystem Team has the vital role of crafting and implementing a storage solution for offline data in TikTok's recommendation system, which caters to more than a billion users. Their primary objectives are to guarantee system reliability, uninterrupted service, and seamless performance. They aim to create a storage and computing infrastructure that can adapt to various data sources within the recommendation system, accommodating diverse storage needs.; Their ultimate goal is to deliver efficient, affordable data storage with easy-to-use data management tools for the recommendation, search, and advertising functions. What you will be doing: 1. Design and implement an offline\/real-time data architecture for large-scale recommendation systems.; 2. Design and implement a flexible, scalable, stable, and high-performance storage system and computation model. 3. Troubleshoot production systems, and design and implement necessary mechanisms and tools to ensure the overall stability of production systems. 4. Build industry-leading distributed systems such as offline and online storage, batch, and stream processing frameworks, providing reliable infrastructure for massive data and large-scale business systems.; Qualifications; What you should have:; Bachelor's Degree or above, majoring in Computer Science, or related fields, with 1+ years of experience building scalable systems;; Proficiency in common big data processing systems like Spark\/Flink at the source code level is required, with a preference for experience in customizing or extending these systems;; A deep understanding of the source code of at least one data lake technology, such as Hudi, Iceberg, or DeltaLake, is highly valuable and should be prominently showcased in your resume, especially if you have practical implementation or customisation experience;; Knowledge of HDFS principles is expected, and familiarity with columnar storage formats like Parquet\/ORC is an additional advantage;; Prior experience in data warehousing modeling;; Proficiency in programming languages such as Java, C++, and Scala is essential, along with strong coding skills and the ability to troubleshoot effectively;; Experience with other big data systems\/frameworks like Hive, HBase, or Kudu is a plus;; A willingness to tackle challenging problems without clear solutions, a strong enthusiasm for learning new technologies, and prior experience in managing large-scale data (in the petabyte range) are all advantageous qualities.; TikTok is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives. Our platform connects people from across the globe and so does our workplace. At TikTok, our mission is to inspire creativity and bring joy. To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach. We are passionate about this and hope you are too.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"83591341","Role":"Lead Data Engineer","Company":"Aon Life","Location":"Singapore","Publish_Time":"2025-06-17 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/83591341","job_desc":"Analytics Product Manager ; Aon is in the business of better decisions; At Aon, we shape decisions for the better to protect and enrich the lives of people around the world.; As an organization, we are united through trust as one inclusive, diverse team, and we are passionate about helping our colleagues and clients succeed.; What the day will look like; Working closely with business SMEs to understand concept initiatives and developing business requirements specifications for H&B analytical initiatives; Performing ad-hoc business analytics; Understand the data needs from the business and challenge stakeholders in requirements; Assisting technical product owner with strategy assessment and development for health solutions initiatives; Managing development backlog through project management tools; Defining sprint requirements and project management through to delivery; Managing communications with project resources, IT and disparate local business units; Co-ordinating with IT, systems design and development ensuring trace ability and quality against business requirements specifications; Conducting training of internal teams to ensure effective use of new processes and systems; Understand risk, privacy and compliance requirements to ensure delivery minimizes risk exposure; How this opportunity is different; Aon\u2019s breadth of product offerings and global reach give us access to information providing a unique opportunity to build lasting and differentiating innovations for clients. But for Aon to capitalize on these opportunities requires a globally coordinated effort, an effort that the Aon Centre for Innovation and Analytics will provide. ; The product owner will work with global stakeholders to understand their need and articulate a delivery method that adds value and supports the delivery of the Human Capital strategy.; It is expected that the colleague has a background in an analytics-related discipline and will contribute to analytics needs beyond individual product delivery.; Skills and experience that will lead to success; University degree, preferably in a finance or business related field; Minimum five years\u2019 experience in people risk \/ health and benefits industry, preferably with solid health insurance knowledge; Knowledge of captive insurance arrangements would be beneficial; Proven individual contributor success with data analysis and visualization; Proven track record in product development with recognized analysis, planning and activity execution; Insightful and analytical to foresee new business needs arising from projects; Collaborative, team player, organized, goal-oriented, committed and ability to solve business problems; Naturally inquisitive with effective reasoning and good negotiation\/influencing skills; Good organizational awareness; Strong written and interpersonal communication skills; Understanding of project management processes and controls and agile projects; Results driven and accountable for results; How we support our colleagues; In addition to our comprehensive benefits package, we encourage a diverse workforce. Plus, our agile, inclusive environment allows you to manage your wellbeing and work\/life balance, ensuring you can be your best self at Aon. Furthermore, all colleagues enjoy two \u201cGlobal Wellbeing Days\u201d each year, encouraging you to take time to focus on yourself.  We offer a variety of working style solutions, but we also recognise that flexibility goes beyond just the place of work... and we are all for it. We call this Smart Working!; Our continuous learning culture inspires and equips you to learn, share and grow, helping you achieve your fullest potential. As a result, at Aon, you are more connected, more relevant, and more valued.; Aon values an innovative, diverse workplace where all colleagues feel empowered to be their authentic selves. Aon is proud to be an equal opportunity workplace.  ; Aon provides equal employment opportunities to all employees and applicants for employment without regard to race, color, religion, creed, sex, sexual orientation, gender identity, national origin, age, disability, veteran, marital, domestic partner status, or other legally protected status. ; We welcome applications from all and provide individuals with disabilities with reasonable adjustments to participate in the job application, interview process and to perform essential job functions once onboard. If you would like to learn more about the reasonable accommodations we provide, email ReasonableAccommodations@Aon.com; #LI-HT1; 2559015","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85838966","Role":"Software Engineer (Tech MNC \/Partner Developer\/ Game\/UP14K+)","Company":"Adecco Personnel Pte Ltd.","Location":"Central Region","Publish_Time":"2025-07-17 08:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85838966","job_desc":"Software Engineer; The Opportunity; Adecco is partnering our client, one of the world's largest tech organizations; We are looking for a Software Engineer; The role will start out as a contract (Renewable); Candidates who are immediately available\/ able to start work within short notice will be preferred; The Talent; Experience as a software engineer building and shipping production quality code; Experience in shipping reliable, scalable, and efficient code, with an emphasis on long-term maintainability, in partnership with all appropriate reviewers, with clear milestones, and with relevant documentation and test plans; Experience with open source languages such as PHP, Python, Java, or JavaScript Frameworks; Experience with software design and architecture; Experience communicating technical concepts to non-technical audiences; Degree in Computer Science, Computer Engineering, relevant technical field, or equivalent practical experience.; Experience with capturing data at scale and\/or using data analytics to drive decision-making and impact measurement; Experience or strong interest in Gaming industry is a big plus; Job Description; Build new products and improve existing ones in collaboration with product engineering teams, Partnerships, and other cross-functional partners to meet business needs; Understand and apply knowledge of products, technologies, and business to build solutions to solve for problems at scale; Design and build end-to-end systems and go-to-market strategies; Use a broad range of technical and soft skills to build productive relationships with our partners, and resolve complex technical and business needs; Use and product insight to deliver high-quality project\/integration\/partner engagements, while influencing product roadmap to meet the business needs; Guide workflow changes, and gain consensus from stakeholders while driving toward solutions; Respond and maintain effective communication with industry partners and internal stakeholders; Work with partners to develop a long-term plan, grounded on business objectives, and manage partners during integrations with platform products and ensure value creation; Build and manage working relationships with technical counterparts. Mentor and share knowledge with peers, creating atmosphere amongst team; Conduct code reviews and provide constructive feedback while quickly actioning feedback from code reviews conducted of your code; Influence decision-making through presentation of data-centric topics; Serve as a subject matter within Partner Engineering and provide consultation on domain-level projects; Participate in interviewing and onboarding of new team members; Willingness to travel, may vary per team\/function; Next Step; Prepare your updated resume (please include your current salary package with full breakdown such as base, incentives, annual wage supplement, etc.) and expected package; Send your resume to Xinyang.liu@Adecco.com; All shortlisted candidates will be contacted; Liu XinYang; ; EA Licence Number: 91C2918; ; Personnel Registration Number: R1988872","salary":"","work_type":"Kontrak\/Temporer","country":"singapore"}
{"Job_ID":"85580696","Role":"Core Engineering, SecDb Services, Cloud Database Software Engineer,...","Company":"Goldman Sachs","Location":"Singapore","Publish_Time":"2025-07-08 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85580696","job_desc":"Role Overview; The SecDb services team is responsible for running SecDb - the firm's flagship trading and risk platform. We are seeking a skilled Software Engineer to work on a new project focused on big data solution using cloud database. The ideal candidate will have strong expertise in database technologies, programming skills, and proficiency in SQL query. You will be responsible for designing, implementing, and maintaining cloud-based database solutions that support our business workflows.; Key Responsibilities; Design, develop, and maintain cloud database architecture and infrastructure; Optimize SQL queries and database performance for large-scale data operations; Implement data security measures and ensure compliance with regulatory requirements; Collaborate with cross-functional teams to understand data requirements and provide solutions; Develop and automate database processes; Troubleshoot database issues and provide timely resolutions; Create and maintain documentation for systems and processes; Support data migration initiatives from on-premises to cloud environments; Implement and manage big data solutions for analytics purposes; Monitor system health, performance, and resource utilization; Experience; Required:; Bachelor's Degree in Computer Science, Information Technology, or related field; Proficiency in at least one modern programming language (e.g., Python, Java, Go, C++); Strong SQL knowledge with proven ability to optimize complex queries; Experience with performance tuning; Understanding of cloud networking concepts and infrastructure; Problem-solving skills with the ability to debug legacy code and diagnose complex technical issues; Excellent communication and collaboration skills; Preferred:; Knowledge of big data technologies and distributed database systems.; Experience with data analytics tools and techniques; Understanding of trading systems and trade lifecycle (desirable but not mandatory); Background in high-performance, scalable database systems; Experience with NoSQL databases and data warehousing solutions; ABOUT GOLDMAN SACHS; At Goldman Sachs, we commit our people, capital and ideas to help our clients, shareholders and the communities we serve to grow. Founded in 1869, we are a leading global investment banking, securities and investment management firm. Headquartered in New York, we maintain offices around the world. ;  We believe who you are makes you better at what you do. We're committed to fostering and advancing diversity and inclusion in our own workplace and beyond by ensuring every individual within our firm has a number of opportunities to grow professionally and personally, from our training and development opportunities and firmwide networks to benefits, wellness and personal finance offerings and mindfulness programs. Learn more about our culture, benefits, and people at GS.com\/careers. ; We\u2019re committed to finding reasonable accommodations for candidates with special needs or disabilities during our recruiting process. Learn more: https:\/\/www.goldmansachs.com\/careers\/footer\/disability-statement.html","salary":"","work_type":"","country":"singapore"}
{"Job_ID":"85608183","Role":"Backend Engineer Intern, Data Infra - OLAP (Aug - Dec 2025)","Company":"SHOPEE SINGAPORE PRIVATE LIMITED","Location":"Singapore","Publish_Time":"2025-07-09 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85608183","job_desc":"The Engineering and Technology team is at the core of the Shopee platform development. The team is made up of a group of passionate engineers from all over the world, striving to build the best systems with the most suitable technologies. Our engineers do not merely solve problems at hand; We build foundations for a long-lasting future. We don't limit ourselves on what we can or can't do; we take matters into our own hands even if it means drilling down to the bottom layer of the computing platform. Shopee's hyper-growing business scale has transformed most \"innocent\" problems into huge technical challenges, and there is no better place to experience it first-hand if you love technologies as much as we do.; About the Team:; Shopee will be prioritizing applicants who have a current right to work in Singapore, and do not require Shopee sponsorship of a visa.; Kindly note that you can only be considered in one recruitment process at a time within Sea Group and will be considered for jobs in the order that you have applied.; Our team builds and maintains the company\u2019s big data infrastructure and platforms, ensuring they are stable, efficient, secure, and easy to use. We provide core data capabilities\u2014including storage, batch and real-time computing, querying, and analytics\u2014to support business teams, analysts, and machine learning applications. Our platforms cover the full data lifecycle, from ingestion to processing and monitoring, enabling teams to build reports, dashboards, real-time pipelines, and data-driven insights efficiently.; Job Description:; Participate in the system optimization, and maintenance of Big Data engines and platform; Engage in system performance tuning and troubleshooting, ensuring service stability and high availability; Collaborate with the product team to develop and optimize features based on business requirements; Write and maintain related technical documentation in English, ensuring knowledge consolidation; Analyze and solve user problems combining user scenarios; Requirements:; Currently enrolled in or a recent graduate of a Bachelor\u2019s program in computer science or a related field; Basic understanding of Big Data Related Components and it\u2019s internal mechanisms; Proficient in Java, HDFS, Presto, ClickHouse, Druid, and other relevant technologies; Excellent team collaboration and communication skills, able to respond and resolve issues promptly; Good English reading and writing abilities, capable of reading technical documentation in English|; Good to have:; Previous exposure to or coursework related to the Big Data domain; An interest in cloud-native technologies, including Kubernetes; A solid foundation in algorithms and a keen interest in problem-solving; Interest in open-source contributions and eagerness to learn from the open-source community; Interest in use AI to solve complex issues","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"84557117","Role":"Data Engineering (ETL & Modeling)","Company":"Airwallex","Location":"Singapore","Publish_Time":"2025-06-17 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/84557117","job_desc":"About Airwallex; Airwallex is the only unified payments and financial platform for global businesses. Powered by our unique combination of proprietary infrastructure and software, we empower over 150,000 businesses worldwide \u2013 including Brex, Rippling, Navan, Qantas, SHEIN and many more \u2013 with fully integrated solutions to manage everything from business accounts, payments, spend management and treasury, to embedded finance at a global scale.; Proudly founded in Melbourne, we have a team of over 1,700 of the brightest and most innovative people in tech across 26 offices around the globe. Valued at US$6.2 billion and backed by world-leading investors including Visa, Airtree, Blackbird, Sequoia, DST Global, Greenoaks, Salesforce Ventures, Lone Pine, and Square Peg, Airwallex is leading the charge in building the global payments and financial platform of the future. If you\u2019re ready to do the most ambitious work of your career, join us.; ; Hiring Location: Singapore and Shanghai; Level: Mid level, Senior \u2013 Staff level; Who We Are? About the Team:; As a global team, we span across Australia, China, USA, and Singapore, revolutionizing applied data science, data engineering and platform solutions to support Airwallex\u2019s rapid growth. You will collaborate with a diverse range of cross-functional partners, including Product, Engineering, Marketing, Sales, Finance, and more, to tackle complex data problems and shape the future of fintech.; What you\u2019ll do; Part 1. Data Modeling ; Design and implement robust and scalable data models that support business intelligence, machine learning, and operational needs.; Possess a deep understanding of data schemas and be able to select appropriate schema designs (e.g., star schema, snowflake, normalized vs denormalized) based on use cases.; Collaborate closely with business teams to translate their data needs into clean, structured, and well-documented models.; Understand and promote the concept of SSOT (Single Source of Truth) throughout the data layers and pipelines.; Maintain data consistency, traceability, and quality across multiple data sources and domains.; Part 2. ETL and Data Pipeline Management; Experience building and maintaining both batch and streaming ETL pipelines, with a strong understanding of end-to-end data workflow \u2014 from data ingestion to transformation and delivery.; Able to work closely with Data Platform Engineers (DPEs) and Product Managers (PMs) to quickly identify root causes of data issues and provide efficient, scalable solutions.; Bonus if you\u2019ve worked with data across distributed or multi-datacenter systems, including solving challenges related to data migration, duplication, and consistency.; Part 3. Data Governance; Participate in and contribute to data governance strategies, policies, and standards.; Be familiar with any of the six key pillars of traditional data governance (e.g., data quality, data stewardship, metadata management, master data management, data privacy\/security, data lifecycle).; Part 4. Data + AI ; We hope you have a basic understanding of AI and enjoy thinking about how data engineering and AI can work together in practical and creative ways.; These four areas are the main focuses of this role. Ideally, you should be strong in at least one of them \u2014 especially data modeling or data ETL. If you also have experience or skills in the other areas, that would be a big plus.; Who you are; We\u2019re looking for people who meet the minimum qualifications for this role. The preferred qualifications are great to have, but are not mandatory.; Minimum qualifications:; Bachelor\u2019s degree in Computer Science, Information Systems, Finance, Maths or a related field. A master\u2019s degree is a plus.; Proven experience (5+ years) in designing and implementing ETL processes using tools such as Informatica, Talend, Apache NiFi, or similar. If you have less than five years of work experience but have done very well in similar roles, we\u2019re happy to consider you for a Mid-Level position (Data Engineer II). Just keep in mind that this role requires at least two years of work experience.; Strong expertise in data modeling.; Proficiency in SQL, database management systems (e.g., MySQL, PostgreSQL, Oracle), and data warehousing solutions.; Familiarity with Google Cloud Platform (GCP), specifically BigQuery and Airflow.; Excellent problem-solving skills, with a keen attention to detail and a commitment to producing high-quality work.; Strong communication and collaboration skills, with the ability to work effectively in a fast-paced, team-oriented environment.; Outstanding verbal communication skills for effective interaction with overseas teams.; Preferred qualifications:; Experience with financial industries( or financial applications, with a focus on Treasury and Ledger systems), payment systems, or fintech platforms.; Knowledge of data governance practices and regulatory requirements in the financial industry.; Experience with scripting languages (e.g., Python, R) for data analysis and automation.; Certification in data management or related technologies is a plus.; At Airwallex, you can make an impact in a rapidly growing, global fintech. We want you to share in our success, which is why you\u2019ll be offered a competitive salary plus valuable equity within Airwallex. We also like to ensure we create the best environment for our people by providing collaborative open office space with a fully stocked kitchen. We organise regular team-building events and we give our people the freedom to be creative.; Equal opportunity; Airwallex is proud to be an equal opportunity employer. We value diversity and anyone seeking employment at Airwallex is considered based on merit, qualifications, competence and talent. We don\u2019t regard color, religion, race, national origin, sexual orientation, ancestry, citizenship, sex, marital or family status, disability, gender, or any other legally protected status when making our hiring decisions. If you have a disability or special need that requires accommodation, please let us know.; Airwallex does not accept unsolicited resumes from search firms\/recruiters.  Airwallex will not pay any fees to search firms\/recruiters if a candidate is submitted by a search firm\/recruiter unless an agreement has been entered into with respect to specific open position(s).  Search firms\/recruiters submitting resumes to Airwallex on an unsolicited basis shall be deemed to accept this condition, regardless of any other provision to the contrary.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85170464","Role":"Data Governance Specialist (SA\/M), Data & Analytics, Technology Consulting","Company":"Ernst & Young Solutions LLP","Location":"Singapore","Publish_Time":"2025-06-17 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85170464","job_desc":"At EY, you\u2019ll have the chance to build a career as unique as you are, with the global scale, support, inclusive culture, and technology to become the best version of you. And we\u2019re counting on your unique voice and perspective to help EY become even better, too. Join us and build an exceptional experience for yourself, and a better working world for all.  ; The opportunity: ; EY DnA is the data and advanced analytics capability within EY Asia-Pacific, with over 500 specialist employees working across multiple industry sectors. ; We implement information-driven strategies, data platforms and advanced data analytics solution systems that help grow, optimize, and protect client organizations. We go beyond strategy and provide end to end design, build and implementation of real-life data environments and have some of the best architects, project managers, business analysts, data scientists, big data engineers, developers and consultants in the region. ; We are seeking an experienced Data Governance Expert to join our team and lead data governance initiatives for our customers. The ideal candidate will have a strong background in data governance implementation, including expertise in data quality, data security, data management, and data sharing. Experience with data governance tools such as Alation and Azure Purview is highly desired. Prior experience in Data Engineering is also required.; Your key responsibilities: ; Define data governance strategies, policies, and procedures.; Define and enforce data governance standards, ensuring data quality, consistency, and accuracy throughout the data lifecycle.; Develop data governance frameworks and guidelines, including data classification, data ownership, and data stewardship.; Provide implementation guidance on data quality initiatives, including establishing data quality standards, conducting data profiling, and defining data quality rules and metrics.; Ensure data security and privacy compliance by providing implementation guidance on appropriate access controls, data classification protocols, and data protection measures.; Design data management processes, including data identification, metadata management, data lineage, and data cataloging.; Define data sharing and collaboration across teams and departments, ensuring data access, usability, and proper data sharing agreements.; Evaluate, select, and implement data governance tools and technologies, such as Alation and Azure Purview, to support data governance initiatives and enhance data management capabilities.; Provide guidance and training to business users and stakeholders on data governance best practices, policies, and procedures.; Stay updated on industry trends, standards, and regulatory requirements related to data governance and recommend appropriate modifications to existing policies and practices.; To qualify for the role, ideally, you should:  ; 3+ years of experience in data analytics or data governance ; Bachelor's degree in Computer Science, Information Management or related field.; Prior experience in a data governance or data management role, with a focus on data quality, metadata management, and data stewardship.; Strong conceptual knowledge of Business Intelligence, Data Warehousing, and Master Data Management.; Excellent communication, presentation, and interpersonal skills.; Strong organizational skills and attention to detail.; Proficient in relevant data governance tools and applications.; What we offer ; EY offers a competitive remuneration package commensurate with your work experience where you\u2019ll be rewarded for your individual and team performance. We are committed to being an inclusive employer and are happy to consider flexible working arrangements, where this may be needed, guided by our FWA Policy.; Plus, we offer: ; Continuous learning: You\u2019ll develop the mindset and skills to navigate whatever comes next. ; Success as defined by you: We\u2019ll provide the tools and flexibility, so you can make a meaningful impact, your way. ; Transformative leadership: We\u2019ll give you the insights, coaching and confidence to be the leader the world needs. ; Diverse and inclusive culture: You\u2019ll be embraced for who you are and empowered to use your voice to help others find theirs. ; If you can demonstrate that you meet the criteria above, please contact us as soon as possible. ; The exceptional EY experience. It\u2019s yours to build. ; Apply now.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"83788669","Role":"Backend Engineer - Application Computing (Data + AI)","Company":"TikTok Pte. Ltd.","Location":"Singapore","Publish_Time":"2025-06-17 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/83788669","job_desc":"Responsibilities; Team Introduction The Recommendation Architecture Data Platform (Offline Computing) Team is Responsible for the design and development of the offline computing systems for the recommendation architectures that power products with over 1 billion users, including Douyin, Toutiao, and Xigua Video. The role focuses on ensuring system stability and high availability, abstracting general-purpose real-time computing systems, and building unified recommendation feature and sample platforms. It also involves constructing flexible and scalable high-performance storage systems and computing models, continuously addressing paradigm shifts in recommendation systems, especially in the era of large models. Reponsibilities 1. Design and implement efficient offline computing systems for large-scale recommendation systems. 2. Design and develop flexible, scalable, stable, and high-performance storage systems and computing models. 3. Conduct troubleshooting on production systems, and design and implement necessary mechanisms and tools to ensure the overall stability of production environments. 4. Build industry-leading streaming computing frameworks and other distributed systems to provide reliable infrastructure for massive data and large-scale business systems. Qualifications; Minimum Qualifications 1. Bachelor's degree or above, majoring in Computer Science, or related fields, with at least 5 years of relevant experience 2. Deep understanding of big data computing systems, with hands-on experience in Flink, Spark, Paimon, Velox, and other components of the big data computing stack. 3. Familiar with machine learning technology stacks, including core technologies such as PyTorch, LLMs, and multimodal systems, with practical experience. 4. Practical experience with features and samples in search, advertising, and recommendation systems. 5. Strong coding and troubleshooting skills. Proficient in programming languages like Java, C++, Scala, Python. Preferred Qualification 1. Passion for tackling challenging, undefined problems and a strong enthusiasm for learning new technologies. Habitual in keeping up with new tech trends and regularly following the latest academic papers.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85155306","Role":"Backend Engineering Lead - Global E-Commerce (Commercial Platform - Data...","Company":"TikTok Pte. Ltd.","Location":"Singapore","Publish_Time":"2025-06-17 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85155306","job_desc":"Responsibilities; About TikTok TikTok will be prioritizing applicants who have a current right to work in Singapore, and do not require TikTok sponsorship of a visa. --- TikTok is the leading destination for short-form mobile video. At TikTok, our mission is to inspire creativity and bring joy. TikTok's global headquarters are in Los Angeles and Singapore, and its offices include New York, London, Dublin, Paris, Berlin, Dubai, Jakarta, Seoul, and Tokyo. --- Why Join Us Creation is the core of TikTok's purpose. Our platform is built to help imaginations thrive. This is doubly true of the teams that make TikTok possible. Together, we inspire creativity and bring joy - a mission we all believe in and aim towards achieving every day. To us, every challenge, no matter how difficult, is an opportunity; to learn, to innovate, and to grow as one team. Status quo? Never. Courage? Always. At TikTok, we create together and grow together. That's how we drive impact - for ourselves, our company, and the communities we serve. Join us. --- About The e-commerce industry has seen tremendous growth in recent years and has become a hotly contested space amongst leading Internet companies, and its future growth cannot be underestimated. With millions of loyal users globally, we believe TikTok is an ideal platform to deliver a brand new and better e-commerce experience to our users. We aim to bring discovery, inspiration, and joy back to shopping by making TikTok the commerce channel of choice for merchants, creators, and affiliates.With millions of loyal users globally, we believe TikTok is an ideal platform to deliver a brand new and better e-commerce experience to our users. We are looking for passionate and talented people to join our product and operations team, to build an e-commerce ecosystem that is innovative, secure and intuitive for our users and brands. --- About the team The Data Intelligence team is responsible for development of data analytics & data-empowered platform capabilities across Global E-Commerce. Our mission is to empower our users to leverage and extract actionable insights from data to maximise their potential and efficiency on the global e-commerce platform. In essence, we want to extract facts, attribute causes and predict the future from oceans of data; and our fundamental goals are to reflect business impact, leverage data to support key decisions by lowering decision making complexity and optimising decision making efficacy and efficiency. --- Responsibilities 1. Develop a top-tier data product system that offers dependable insights and analytical diagnostics for both internal and external business users, fostering ongoing growth. 2. Steadily advance the development and enhancement of our data system architecture, focusing on reliability, reusability, scalability among others. 3. Work in tandem with upstream and downstream departments to jointly design and construct data production pipelines as well as data management platforms for efficient and adaptable metrics management. 4. Research, design, and develop computer and network software or specialised utility programs. 5. Analyse user needs and develop software solutions, applying principles and techniques of computer science, engineering, and mathematical analysis. 6. Update software, enhances existing software capabilities, and develops and direct software testing and validation procedures. Qualifications; 1. Bachelor's or higher degree in Computer Science, Information Technology, Programming & System Analysis, Science (Computer Studies) or related discipline. 2. At least 2+ years of experience managing or tech-leading a software engineering team and at least 5 years of experience in constructing large-scale, high-availability distributed backend systems. 3. Proficient in software programming with a deep understanding of data structures and algorithms; demonstrates excellent code design and coding style. 4. Well-understanding with mainstream distributed system platforms and tools: programming languages (Java\/Golang\/Python\/C++), databases and caches (Mysql\/PostgreSQL\/Redis), message queue platforms (Kafka\/RocketMQ) etc. 5. [Optional] Familiarity with big data technology stack including Flink, Spark, ClickHouse, Hive. Ideal Candidate 1. Agile, quick learner with a strong sense of product ownership and a knack for creative problem-solving. 2. Expertise in both product and data science, capable of merging technical and data perspectives to provide fresh insights and directions for the team 3. Skilled at leading teams through technical breakthroughs and resolving complex issues. 4. An empathetic leader focused on results, adept at mentoring others. 5. A great collaborator who thrives in fast-paced, culturally diverse global team environments. --- TikTok is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives. Our platform connects people from across the globe and so does our workplace. At TikTok, our mission is to inspire creativity and bring joy. To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach. We are passionate about this and hope you are too. ---","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85155082","Role":"Backend Software Engineer - (Data Management Suite)","Company":"TikTok Pte. Ltd.","Location":"Singapore","Publish_Time":"2025-06-17 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85155082","job_desc":"Responsibilities; About Tiktok TikTok is the leading destination for short-form mobile video. Our mission is to inspire creativity and bring joy. TikTok has global offices including Los Angeles, New York, London, Paris, Berlin, Dubai, Singapore, Jakarta, Seoul, and Tokyo. Why Join Us Creation is the core of TikTok's purpose. Our platform is built to help imaginations thrive. This is doubly true of the teams that make TikTok possible. Together, we inspire creativity and bring joy; a mission we all believe in and aim towards achieving every day.; To us, every challenge, no matter how difficult, is an opportunity; to learn, to innovate, and to grow as one team. Status quo? Never. Courage? Always. At TikTok, we create together and grow together. That's how we drive impact; for ourselves, our company, and the communities we serve.; Join us. About the Team The Data Management Suite team is building products that cover the whole lifecycle of data pipeline, including data ingestion and Integration, data development, data catalog, data security and data governance.; These products support various businesses, so data engineers and data scientists could greatly boost their productivity. As a software engineer in the data management suite team, you will have the opportunity to build, optimize and grow one of the largest data platforms in the world. You'll have the opportunity to gain hands-on experience on core systems in the data platform ecosystem.; Your work will have a direct and huge impact on the company's core products as well as hundreds of millions of users. Responsibilities:; Design and implement a unified data development platform that improves the efficiency of globalization data producers, which includes intelligence IDE, workflow management, multiple engine integration etc.; Figure out efficient ways to manage distributed states, and sync metadata between centralized place and regions globally;; Design and develop a high-performance and distributed scheduling system that manages large-scale tasks across different business lines in whole ByteDance;; Work closely with partner teams across the company and all over the world;; Responsible for the design and development of integration with other systems in the big data area; Qualifications; Minimum Qualifications:; Bachelor's degree in Computer Science or equivalent practical experience; 5+ years of experience in software development, and with data structures\/algorithms; 3+ years of experience with design and architecture, and testing and launching software products; 2+ years of experience in data management systems, have experience of building big data development platforms, data catalog platforms, or data transfer platforms;; Preferred Qualifications:; 5+ years of experience building and developing large-scale infrastructure, distributed systems or networks, and\/or experience with compute technologies, storage, and\/or hardware architecture; 3+ years of experience working in a complex, matrixed organization involving cross-functional, and\/or cross-business projects; Experiences in data platform related product development or big data technologies (such as Hadoop, Clickhouse, Flink etc.); TikTok is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives. Our platform connects people from across the globe and so does our workplace. At TikTok, our mission is to inspire creativity and bring joy. To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach. We are passionate about this and hope you are too.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85790972","Role":"Digital, Cloud, Data (Data) Consulting Year-End Internship (Dec 2025 - Feb 2026)","Company":"PricewaterhouseCoopers","Location":"Singapore","Publish_Time":"2025-07-15 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85790972","job_desc":"Line of Service; Advisory; Industry\/Sector; Not Applicable; Specialism; Technology Strategy; Management Level; Intern\/Trainee; Job Description & Summary; At PwC, we help clients build trust and reinvent so they can turn complexity into competitive advantage. We\u2019re a tech-forward, people-empowered network with more than 370,000 people in 149 countries. Across audit and assurance, tax and legal, deals and consulting we help clients build, accelerate and sustain momentum. Find out more at www.pwc.com.; The Cloud, Digital, Data (Data) team helps clients optimise data and analytical assets to make better decisions, work more efficiently, and find new sources of revenue.; The team seeks to understand our clients\u2019 challenges and identify ways of using data through data mining, modelling and advanced predictive and simulation analytics to assist them in navigating the impact of various financial or operational changes to their business.; We have a supportive and diverse team who strongly believes in learning and growing as a team. By joining the team, you will have the opportunity to be exposed to a range of industry and clients and solve interesting problems with data.; We are looking for candidates to join us for our year-end internship from December 2025 to January\/February 2026 (min. 8 weeks); Responsibilities: ; You will understand and analyse complex client commercial, operational and financial data ; You will design, build and review analytical models to find solutions for our client\u2019s business challenges ; You will translate complex findings into simple language and communicate them to internal and external stakeholders, such as your clients and team members ; Requirements: ; Penultimate year Computer Science, Computer Engineering, Engineering, Information Systems or other related disciplines that have an analytical component and quantitative focus students from local or overseas universities ; Pro-active, adaptable, and flexible. Able to apply new tools and techniques such as proprietary analytical software, data models and programming languages ; Strong analytical and critical thinking skills. Able to analyse and break down complex concepts and technical findings into clear and simple language ; It will be required to have basic competency in one or more of the following: ;                 (a) Analytical software and languages (e.g. PowerBI, Python, R)             ;                 (b) Relational or graph databases and tools (eg. SQL, SSMS, MySQL) ;                 (c) Knowledge of simulation or optimisation software and theory (e.g. discrete choice, agent based) ; Note:  ; Please note we accept only one application per candidate. You may indicate your second preference in the same application. We recommend that you apply to your preferred position that closely aligns with your skills, passions and interests ; You can indicate another role on the same application form. Duplicate entries will slow down your application with us ; Kindly upload both your resume and degree audit or transcript in PDF format.  ; Kindly note that only shortlisted candidates will be contacted. ; Got a question? Email to sg_graduate_recruitment@pwc.com; Education (if blank, degree and\/or field of study not specified); Degrees\/Field of Study required:Degrees\/Field of Study preferred:; Certifications (if blank, certifications not specified); Required Skills; Optional Skills; Accepting Feedback, Accepting Feedback, Active Listening, Algorithm Development, Alteryx (Automation Platform), Analytic Research, Big Data, Business Data Analytics, Communication, Complex Data Analysis, Conducting Research, Customer Analysis, Customer Needs Analysis, Dashboard Creation, Data Analysis, Data Analysis Software, Data Collection, Data-Driven Insights, Data Integration, Data Integrity, Data Mining, Data Modeling, Data Pipeline, Data Preprocessing, Data Quality {+ 33 more}; Desired Languages (If blank, desired languages not specified); Travel Requirements; Not Specified; Available for Work Visa Sponsorship?; Yes; Government Clearance Required?; No; Job Posting End Date","salary":"","work_type":"","country":"singapore"}
{"Job_ID":"85632903","Role":"APAC Cloud Solution Architect (AI)","Company":"Huawei International Pte. Ltd.","Location":"Changi","Publish_Time":"2025-07-10 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85632903","job_desc":"Tasks & responsibilities; Design and plan AI-based cloud solutions for customers, with a focus on industry-specific challenges.; Collaborate with customers to understand their digital transformation goals and develop tailored AI product and application architectures.; Lead the implementation of AI solutions, ensuring successful deployment and management of projects.; Optimize cloud and AI technology solutions to meet diverse customer needs, ensuring high feasibility and continuous product improvement.; Provide technical POC support and assist in product roadmap development.;  Qualifications & experience; Education Requirements: Bachelor degree or above in Computer Science, Artificial Intelligence, Data Science, Software Engineering or related major.; Work experience: 5+ years in IT, with at least 2 years in AI\/machine learning ; Technical capabilities: ; o Proficiency in AI\/machine learning frameworks (e.g., TensorFlow, PyTorch, Keras).; o Familiarity with big data tools (e.g., Hadoop, Spark). \u3010Good to have\u3011; o Experience designing cloud platform architectures (AWS, Azure, or Google Cloud). \u3010Good to have\u3011; o Familiar with API integration and microservice architecture.; Project Management: Experience leading AI projects to on-time, high-quality delivery.; Communication: Strong ability to explain technical solutions to non-technical stakeholders.; Problem-Solving: Ability to analyze complex challenges and deliver fast, effective solutions.; Travel: Willingness to travel for project needs.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85155197","Role":"Backend Software Engineer (Data Management Suite) - 2025 Start","Company":"TikTok Pte. Ltd.","Location":"Singapore","Publish_Time":"2025-06-17 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85155197","job_desc":"Responsibilities; About the Team The Data Management Suite team is building products that cover the whole lifecycle of data pipeline, including data ingestion and Integration, data development, data catalog, data security and data governance. These products support various businesses, so data engineers and data scientists could greatly boost their productivity.; As a software engineer in the data management suite team, you will have the opportunity to build, optimize and grow one of the largest data platforms in the world. You'll have the opportunity to gain hands-on experience on core systems in the data platform ecosystem. Your work will have a direct and huge impact on the company's core products as well as hundreds of millions of users.; Responsibilities:; Design and implement a unified data development platform that improves the efficiency of globalization data producers, which includes intelligence IDE, workflow management, multiple engine integration etc.; Fingure out efficient ways to manage distributed states, and sync metadata between centralized place and regions globally;; Design and develop a high-performance and distributed scheduling system that manages large-scale tasks across different business lines in whole ByteDance;; Work closely with partner teams across the company and all over the world;; Responsible for the design and development of integration with other systems in the big data area; Qualifications; Minimum Qualifications:; Bachelor's degree in Computer Science or equivalent practical experience;; Be familiar with at least one backend language, like Java, GO, Python etc;; Experience in software development, and with data structures\/algorithms;; Experience with design and architecture, and testing and launching software products;; Preferred Qualifications:; Be familiar with web frameworks will be a plus, like Spring boot etc.; Experiences in data platform related product development or big data technologies (such as Hadoop, Clickhouse, Flink etc.) will be a plus;; By submitting an application for this role, you accept and agree to our global applicant privacy policy, which may be accessed here: https:\/\/careers.tiktok.com\/legal\/privacy. If you have any questions, please reach out to us at apac-earlycareers@tiktok.com","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"84983757","Role":"Sales - Software Engineer (Data), PART","Company":"Apple Inc.","Location":"Singapore","Publish_Time":"2025-06-17 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/84983757","job_desc":"Summary; Posted: Jun 16, 2025; Weekly Hours: 38; Role Number:200608513; Imagine what you could do here. The people at Apple don\u2019t just create products \u2014 they create the kind of wonder that\u2019s revolutionized entire industries. It\u2019s the diversity of those people and their ideas that inspires the innovation that runs through everything we do. Bring passion and dedication to your job and there's no telling what you could accomplish! Apple's Finance Process, Analytics, Reporting, and Technology (PART) team is looking for a passionate and highly motivated Software Engineer. As part of this team, you will support Apple\u2019s growth, both top and bottom line, by applying the same level of innovation toward financial matters as we do toward our products and services. The PART team is seeking an experienced Software Engineer to build high quality, scalable and resilient distributed systems that power Apple's cloud analytics platforms and data pipelines. Apple's Enterprise Data Warehouse landscape caters to a wide variety of real-time, near real-time and batch analytical solutions. These solutions are an integral part of business functions like Sales, Operations, Finance, AppleCare, Marketing and Internet Services, enabling business drivers to make critical decisions. We use proprietary and open source technologies, Kafka, Spark, Iceberg, Airflow, Presto, etc. If you are looking to tackle infrastructure problems at scale, both on-prem or in cloud, focusing on ease of use, ease of maintenance and most importantly implement solutions that are scalable, you will enjoy working in PART!; Description; We engineer high-quality, scalable and resilient distributed systems on cloud that power data exploration, analytics, reporting and production models. Our core systems are diverse and come with an unusual intersection of high data volumes with systems distributed across cloud and on-premise infrastructure. This role will build solutions that integrate open source software with Apple\u2019s internal ecosystem. You will drive development of new components and features from concept to release: design, build, test, and ship at a regular cadence. You will work closely with internal customers to understand their requirements and workflows, and propose new features and ecosystem changes to streamline their experience of using the solutions on our platform. This is a challenging software engineering role, where a large part of an engineer's time is spent in writing code and designing\/developing applications on cloud, with the remainder being spent on tuning and debugging codebase, supporting production applications and supporting our application end users. This role requires in-depth knowledge of innovative technologies and cloud data platform with the ability to independently learn new technologies and contribute to the success of various initiatives.; Minimum Qualifications; 4 or more years of experience building enterprise-level data applications on distributed systems; Knowledge of BI concepts and Implementation experience on Cloud with databases like SnowFlake or Big Query; Programming experience with Python, Scala or Java.; Experience in developing highly optimized SQLs, procedures & semantic process for distributed data applications; Bachelor\u2019s degree in Computer Science or equivalent experience; Preferred Qualifications; Hands-on experience in designing and development of cloud-based applications that include compute services, database services, APIs to design RESTful services, ETL, queues and notification services.; Experience in cloud data warehousing platforms like Snowflake is highly valued; Hands-on knowledge of Spark cluster-computing framework & Kubernetes or similar containerization technologies.; Experience developing Big Data applications using Java, Spark, Kafka is a huge plus; Understanding of fundamentals of object-oriented design, data structures, algorithm design, and problem solving; Cloud technology experience on platforms like AWS, Microsoft Azure, Google Cloud; Data Visualization Tools: experience in software such as Streamlit, Superset, Tableau, Business Objects, and Looker; Data Insights and KPIs: Working experience on generating and visualizing data insights, metrics, and KPIs. Usage of basic ML models in the space of anomaly detection, forecasting, GenAI.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85155280","Role":"Backend Software Engineer, TikTok Location Based Service (LBS) Platform","Company":"TikTok Pte. Ltd.","Location":"Singapore","Publish_Time":"2025-06-17 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85155280","job_desc":"Responsibilities; TikTok is the leading destination for short-form mobile video. Our mission is to inspire creativity and bring joy. TikTok has global offices including Los Angeles, New York, London, Paris, Berlin, Dubai, Singapore, Jakarta, Seoul and Tokyo. Why Join Us Creation is the core of TikTok's purpose. Our platform is built to help imaginations thrive. This is doubly true of the teams that make TikTok possible. Together, we inspire creativity and bring joy; a mission we all believe in and aim towards achieving every day.; To us, every challenge, no matter how difficult, is an opportunity; to learn, to innovate, and to grow as one team. Status quo? Never. Courage? Always. At TikTok, we create together and grow together. That's how we drive impact; for ourselves, our company, and the communities we serve.; Join us. TikTok LBS Platform's goal is to build a global high-quality LBS database and high-performance LBS services that connects TikTok users with nearby merchants and local services.; Build a global POI (Point of; Interest) database, multi source POI integration and POI nearby search service to support TikTok local services.; Build geocoding database, geocoding service, and IP Location service to support TikTok local services and monetization product; Participate in the analysis\/mining of location-based data based on compliance requirements, and build a location-related data warehouse; Develop global general location based service (LBS) with high concurrency and low latency.; Improve system design and architecture to ensure high stability, performance and reliability of the product.; Collaborate with multiple cross-functional teams to deliver high quality work in rapid product development.; Qualifications; Minimum Requirements; Bachelor\/Masters in Computer Science or related major; Proficiency in building backend services for large-scale consumer-facing applications; Proficient in at least one of the following languages: Go, Python, Java, C++.; Deep understanding of computer architectures, data structures and algorithms.; Familiar with common components such as MySQL, Hive, Redis, Spark, ElasticSearch, Message Queue and RPC.; Preferred Qualification: -Experience in R&D of geographic location-based data is preferred, such as POI(Point of Interest) data, Geocoding related data. -Experience in big data\u3001data warehouse\u3001streaming data flow and real time data processing system is preferred.; TikTok is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives. Our platform connects people from across the globe and so does our workplace. At TikTok, our mission is to inspire creativity and bring joy.; To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach. We are passionate about this and hope you are too.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85786142","Role":"Tencent Cloud Architect (Manager\/Senior Manager), Technology Consulting","Company":"Ernst & Young Solutions LLP","Location":"Singapore","Publish_Time":"2025-07-16 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85786142","job_desc":"Job Description: Tencent Cloud Architect,  AI Data, Technology Consulting; At EY, we develop you with future-focused skills and equip you with world-class experiences. We empower you in a flexible environment, and fuel you and your extraordinary talents in a diverse and inclusive culture of globally connected teams.; We work together across our full spectrum of services and skills powered by technology and AI, so that business, people and the planet can thrive together. ; We\u2019re all in, are you?; Join EY and shape your future with confidence.; About the opportunity; EY AI & Data is the data and advanced analytics capability within EY Asia-Pacific, with over 500 specialist employees working across multiple industry sectors. We implement information-driven strategies, data platforms and advanced data analytics solution systems that help grow, optimize and protect client organizations. We go beyond strategy and provide end to end design, build and implementation of real-life data environments and have some of the best architects, project managers, business analysts, data scientists, big data engineers, developers and consultants in the region. ; We are looking for an experienced Tencent Cloud Architect at the Manager\/Senior Manager level to join our Technology Consulting practice. The ideal candidate will bring deep technical expertise in Tencent Cloud architecture, with proven experience in delivering cloud transformation and digital innovation projects for enterprise clients. As a leader in the team, you will help shape strategy, manage delivery, and guide clients through their cloud adoption journeys.; Your key responsibilities:; Lead end-to-end architecture design and implementation of solutions on Tencent Cloud for large-scale enterprise and public sector clients.; Translate client business requirements into scalable, secure, and cost-effective cloud solutions.; Drive client workshops, technical assessments, and solution presentations.; Provide architectural guidance and governance across all project phases (discovery, design, delivery, and operations).; Design cloud-native architectures including microservices, containers (Kubernetes), serverless, and hybrid cloud models.; Develop cloud migration strategies (lift-and-shift, re-platforming, re-architecting).; Ensure alignment with best practices for cloud security, performance, and compliance.; Stay abreast of Tencent Cloud\u2019s evolving services and capabilities; identify how these can benefit client use cases; Manage cross-functional project teams across multiple geographies and time zones.; Oversee project timelines, deliverables, and resource planning; Engage senior stakeholders and act as the technical point of contact for cloud-related engagements.; Provide mentorship and technical leadership to junior architects and engineers.; Support business development by contributing to proposals, RFPs, and solution demonstrations.; Build and expand strategic partnerships with Tencent Cloud and related ecosystem vendors.; Drive internal capability development, knowledge sharing, and thought leadership in cloud computing and digital transformation.; Skills & attributes for success:; Bachelor\u2019s degree in Computer Science, Engineering, or related field; Master\u2019s preferred.; 8\u201312 years of experience in IT consulting or enterprise architecture roles.; At least 3\u20135 years of hands-on architecture experience with Tencent Cloud, including services like CVM, COS, TKE, SCF, and VPC.; Tencent Cloud certification (e.g., Tencent Cloud Solutions Architect) or equivalent is a strong advantage.; Experience with other cloud platforms (e.g., AWS, Azure, GCP) for multi-cloud or migration scenarios.; Familiarity with DevSecOps, CI\/CD pipelines, and Infrastructure as Code (Terraform, Ansible).; Deep understanding of cloud governance, cost optimization, and security frameworks.; Consulting or client-facing experience in government, finance, or telecom industries in Asia, especially China\/Southeast Asia.; To qualify for the role, you must have; Experience working with Singapore public sector clients.; Strong communication and stakeholder engagement skills.; What we look for; Highly motivated individuals with excellent problem-solving skills and the ability to prioritize shifting workloads in a rapidly changing industry. An effective communicator, you\u2019ll be a confident team player that collaborates with people from various teams while looking to develop your career in a dynamic organization.; What working at EY offers; EY offers a competitive remuneration package where you\u2019ll be rewarded for your individual and team performance. We are committed to being an inclusive employer and are happy to consider flexible working arrangements. Plus, we offer:; Continuous learning: You\u2019ll develop the mindset and skills to navigate whatever comes next.; Success as defined by you: We\u2019ll provide the tools and flexibility, so you can make a meaningful impact, your way.; Transformative leadership: We\u2019ll give you the insights, coaching and confidence to be the leader the world needs.; Diverse and inclusive culture: You\u2019ll be embraced for who you are and empowered to use your voice to help others find theirs.; Company description; EY is building a better working world by creating new value for clients, people, society and the planet, while building trust in capital markets.; Enabled by data, AI and advanced technology, EY teams help clients shape the future with confidence and develop answers for the most pressing issues of today and tomorrow.; EY teams work across a full spectrum of services in assurance, consulting, tax, strategy and transactions. Fueled by sector insights, a globally connected, multi-disciplinary network and diverse ecosystem partners, EY teams can provide services in more than 150 countries and territories.; All in to shape the future with confidence.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85685987","Role":"Site Reliability Engineer Intern, Data Infra (Aug - Dec 2025)","Company":"SHOPEE SINGAPORE PRIVATE LIMITED","Location":"Singapore","Publish_Time":"2025-07-11 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85685987","job_desc":"The Engineering and Technology team is at the core of the Shopee platform development. The team is made up of a group of passionate engineers from all over the world, striving to build the best systems with the most suitable technologies. Our engineers do not merely solve problems at hand; We build foundations for a long-lasting future. We don't limit ourselves on what we can or can't do; we take matters into our own hands even if it means drilling down to the bottom layer of the computing platform. Shopee's hyper-growing business scale has transformed most \"innocent\" problems into huge technical challenges, and there is no better place to experience it first-hand if you love technologies as much as we do.; About the Team:; We are looking for a proactive and detail-oriented Site Reliability Engineering (SRE) Intern to join our Big Data Infrastructure team. This internship is ideal for students who are passionate about Linux systems, scripting, and large-scale data platforms. You will gain hands-on experience in operating and improving the reliability of data infrastructure services.; Job Description:; Support daily operations of big data platforms, including monitoring, troubleshooting, and routine maintenance.; Write and optimize Shell scripts to automate operational workflows and system tasks.; Assist in system health checks, log analysis, and reliability improvements.; Participate in building tools to enhance the observability and automation of data services.; Document standard operating procedures and support knowledge sharing across the team.; Requirements:; Basic Qualifications ; Currently pursuing a Bachelor\u2019s or Master\u2019s degree in Computer Science, Software Engineering, or a related field.; Strong understanding of Linux operating systems and command-line tools.; Proficiency in Shell scripting (bash, sh, etc.).; Clear interest in large-scale systems and reliability engineering.; Willingness to learn, take initiative, and work collaboratively.; Bonus Qualifications (Nice to Have); Familiarity with Python for automation or internal tooling.; Experience with web platform development (e.g., using Flask, FastAPI, or similar frameworks).; Exposure to big data and storage engines such as: HDFS, Apache Ozone, Alluxio; Understanding of monitoring or alerting tools (e.g., Prometheus, Grafana, ELK).; Knowledge of Git and basic CI\/CD workflows.; What You'll Gain; Real-world experience in operating and improving a production-grade big data platform.; Exposure to SRE practices including automation, fault-tolerance, and observability.; Mentorship from experienced infrastructure engineers.; Potential opportunity for full-time conversion based on performance and graduation timeline.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85155120","Role":"Backend Engineer - Global E-Commerce (Commercial Platform)","Company":"TikTok Pte. Ltd.","Location":"Singapore","Publish_Time":"2025-06-17 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85155120","job_desc":"Responsibilities; About the team The e-commerce industry has seen tremendous growth in recent years and has become a hotly contested space amongst leading Internet companies, and its future growth cannot be underestimated. With millions of loyal users globally, we believe TikTok is an ideal platform to deliver a brand new and better e-commerce experience to our users.; Our product engineering team is responsible for building an e-commerce ecosystem that is innovative, secure and intuitive for our users. We are looking for passionate and talented people to join us as we drive the future of e-commerce here at TikTok. The Commercial Platform team is responsible for development of data analytics & data-empowered platform capabilities across Global E-Commerce.; Our mission is to empower our users to leverage and extract actionable insights from data to maximise their potential and efficiency on the global e-commerce platform. In essence, we want to extract facts, attribute causes and predict the future from oceans of data; and our fundamental goals are to reflect business impact, leverage data to support key decisions by lowering decision making complexity and optimising decision making efficacy and efficiency. Responsibilities; - Undertake the core system research and development work of the global e-commerce operation platform, deeply participate in the construction of multiple core systems, undertake system design, development and maintenance work, and continuously optimize and improve. - Deeply understand the business, possess good business modeling skills, be able to combine business scenario abstraction, reasonably complete technical selection, and promote implementation. - Undertake stability construction work, including domestic and foreign multi-data center scheduling, stability governance, loss prevention and control, disaster recovery downgrade, etc.; - Provide professional and technical skills to support the professional growth of individual team members.; Qualifications; Minimum Qualifications; Bachelor's or higher degree in Computer Science, Information Technology, Programming & System Analysis, Science (Computer Studies) or related discipline.; Candidate should have at least 5 years of experience in constructing distributed backend systems.; Proficient in software programming with a deep understanding of data structures and algorithms; demonstrates excellent code design and coding style.; Well-understanding with mainstream distributed system platforms and tools: programming languages (Java\/Golang\/Python\/C++), databases and caches (Mysql\/PostgreSQL\/Redis), message queue platforms (Kafka\/RocketMQ) etc.; Strong software programming capabilities, exhibits good code design and coding style.; Preferred Qualifications; Sensitive to data, with experience in developing e-commerce related business products.; Familiarity with big data technology stack including Flink, Spark, ClickHouse, Hive.; Sensitive to business, able to quickly understand business background, with excellent technical and business integration ability, good demand abstraction ability and architecture ability.; Proactive and self-driven, with a deep understanding of business and communication with various roles in the business, able to quickly advance projects and solve problems.; A great collaborator who thrives in fast-paced, culturally diverse global team environments.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85504760","Role":"Data Product Manager Project Intern (Data - Data Platform) - 2025 Start (BS\/MS)","Company":"TikTok Pte. Ltd.","Location":"Singapore","Publish_Time":"2025-07-05 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85504760","job_desc":"Responsibilities; Team Introduction The mission of the Data Platform Business Partnering Product Manager(BP PM) team is to build the TikTok business data product system that empower businesses. Based on business data warehouse and engineering technology, we have built several business data portals.; In the future, we still have great challenges to go deep into business scenarios and build data product services to improve business efficiency. If you\u2019re looking for such an opportunity, welcome to join us! As a project intern, you will have the opportunity to engage in impactful short-term projects that provide you with a glimpse of professional real-world experience.; You will gain practical skills through on-the-job learning in a fast-paced work environment and develop a deeper understanding of your career interests. Responsibilities - Have passion and participate in 0-to-1 development of data dashboards or data products to support key business analysis.; - Collaborate with cross-functional teams effectively. eg: Business Teams, Data Engineering teams. - Support data product roadmaps, design data visualization solutions and dashboard development.; Qualifications; Minimum Qualifications; Currently pursuing a Bachelor\u2019s\/Master\u2019s in Computer Science, Engineering, Information Systems, Statistics, or related fields.; Basic understanding of big data concepts (datasets, data metrics, data analytics).; Familiarity with SQL (able to write SQL data queries with troubleshoot single data issues).; Experience with BI tools for Data Dashboard design, use visualization ability to design data products.; Good communication skills with cross-functional teams and passion for data-driven business logic.; Preferred Qualifications; Data analysis skills, able to produce data insights through BI tools.; 0-1 data product or data analysis experience.; Knowledge of TikTok\u2019s business model and ecosystem.; Applications will be reviewed on a rolling basis; we encourage you to apply early.; Successful candidates must be able to commit to at least 3 months long internship period. If you have any questions, please reach out to us at apac-earlycareers@tiktok.com.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85155192","Role":"Backend Software Engineer (Mirror), Data Platform","Company":"TikTok Pte. Ltd.","Location":"Singapore","Publish_Time":"2025-06-17 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85155192","job_desc":"Responsibilities; About TikTok TikTok is the leading destination for short-form mobile video. Our mission is to inspire creativity and bring joy. TikTok has global offices including Los Angeles, New York, London, Paris, Berlin, Dubai, Singapore, Jakarta, Seoul, and Tokyo. Why Join Us Creation is the core of TikTok's purpose. Our platform is built to help imaginations thrive. This is doubly true of the teams that make TikTok possible. Together, we inspire creativity and bring joy; a mission we all believe in and aim towards achieving every day.; To us, every challenge, no matter how difficult, is an opportunity; to learn, to innovate, and to grow as one team. Status quo? Never. Courage? Always. At TikTok, we create together and grow together. That's how we drive impact; for ourselves, our company, and the communities we serve.; Join us. About the team: Mirror is a one-stop marketing data platform that is widely used by Bytedance product lines and also serves for other enterprises. Help our customers break data silos and build a customer-wide data center for business growth.; The system supports data integration, preprocessing, ID Mapping, label production, label management, crowd management, data services, user analysis and other functions, covering the entire life cycle of marketing data. We are a Mirror engineering team. We are passionate about building the best CDP in the world and are looking for top-notch software engineers to join the talented team.; What you'll be doing:; Design, develop, and maintain high-performance data applications.; Segment and design system layers to support componentized and layered application development, including business functionality and database access; Work with other engineers, managers, Product Management, QA, and Operations teams to develop innovative solutions that meet market needs with respect to functionality, performance, scalability, reliability, realistic implementation schedules, and adherence to development goals and principles; Estimate engineering effort, plan implementation, and rollout system changes; Must be able to independently design code and test major features, as well as work jointly with other team members to deliver complex changes; Compliance Requirement: Familiarity with and adherence to international data protection regulations; ability to formulate and execute compliance-oriented strategies.; Global Multi-Environment Deployment and Operations: Experience in deploying and maintaining big data platforms across multiple global locations; familiarity with cross-geographical operational challenges.; Update software, enhances existing software capabilities and develops and direct software testing and validation procedures.; Qualifications; What you should have:; Bachelor degree in Computer Science or related discipline with experience in software engineering; Experience in object-oriented design methodology and strong software development skills and expertise in Java.; Experience in requirements analysis, design, coding and unit testing of scalable, distributed, fault-tolerant applications.; Experience with large scale data-driven systems is highly desired.; Good working knowledge of distributed systems and OLAP databases is preferred.; TikTok is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives. Our platform connects people from across the globe and so does our workplace. At TikTok, our mission is to inspire creativity and bring joy. To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach. We are passionate about this and hope you are too.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85155101","Role":"Backend Software Engineer (Libra), Data Platform","Company":"TikTok Pte. Ltd.","Location":"Singapore","Publish_Time":"2025-06-17 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85155101","job_desc":"Responsibilities; About TikTok TikTok is the leading destination for short-form mobile video. Our mission is to inspire creativity and bring joy. TikTok has global offices including Los Angeles, New York, London, Paris, Berlin, Dubai, Singapore, Jakarta, Seoul, and Tokyo. Why Join Us Creation is the core of TikTok's purpose. Our platform is built to help imaginations thrive. This is doubly true of the teams that make TikTok possible. Together, we inspire creativity and bring joy; a mission we all believe in and aim towards achieving every day.; To us, every challenge, no matter how difficult, is an opportunity; to learn, to innovate, and to grow as one team. Status quo? Never. Courage? Always. At TikTok, we create together and grow together. That's how we drive impact; for ourselves, our company, and the communities we serve.; Join us. About the team: Libra is a large-scale online one-stop A\/B testing platform developed by TikTok Data Platform . Provide experimental evaluation services for all product lines within the company, covering solutions for complex scenarios such as recommendation, algorithm, function, UI, marketing, advertising, operation, social isolation, causal inference, etc.; Provide services throughout the entire experimental lifecycle from experimental design, experimental creation, indicator calculation, statistical analysis to final evaluation launch. Support the entire company's business on the road of rapid iterative trial and error, boldly assuming and carefully verifying. What you'll be doing:; Design and implement a powerful and easy-to-use data integration system; Segment and design system layers to support componentized and layered application development, including business functionality and database access; Work with other engineers, managers, Product Management, QA, and Operations teams to develop innovative solutions that meet market needs with respect to functionality, performance, scalability, reliability, realistic implementation schedules, and adherence to development goals and principles; Estimate engineering effort, plan implementation, and rollout system changes; Must be able to independently design code and test major features, as well as work jointly with other team members to deliver complex changes; Identify technical areas for improvement and make detailed business cases for improvements or new areas of opportunities; Update software, enhances existing software capabilities and develops and direct software testing and validation procedures.; Qualifications; What you should have:; Bachelor degree in Computer Science or related discipline with experience in software engineering; Preferred experience in big data like Spark, Flink, Hadoop and Kafka; Experience in object-oriented design methodology and strong software development skills and expertise in; Go\/Python.; Experience in requirements analysis, design, coding and unit testing of scalable, distributed, fault-tolerant applications.; Experience with large scale data-driven systems is highly desired.; Good working knowledge of distributed systems and OLAP databases is preferred.; TikTok is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives. Our platform connects people from across the globe and so does our workplace. At TikTok, our mission is to inspire creativity and bring joy. To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach. We are passionate about this and hope you are too.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85653129","Role":"Senior Software Development Engineer-Data Insights-Trust and Safety,...","Company":"TikTok Pte. Ltd.","Location":"Central Region","Publish_Time":"2025-06-17 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85653129","job_desc":"About Us; The Trust and Safety(TnS) engineering team is responsible for protecting our users from harmful content and abusive behaviors. With the continuous efforts of our trust and safety engineering team, TikTok can provide the best user experience and bring joy to everyone in the world. Our team is responsible for achieving goals by building content moderation process systems, rule engine, strategy systems, feature engine, moderation platforms, data platforms, risk insight systems and all kinds of supportive platforms across TnS organization.; Responsibilities; - Participate in the overall architecture design and implementation of the data application platforms of the TnS, formulating data application architecture specifications and leading the team to implement them;; - Participate in optimizing the data application architecture, continuously improving the stability and efficiency of the system, and continuously reducing the complexity and cost of storage, calculation, and service architecture;; - Design and implement systematic support for data integration, offline development, real-time development, data governance, and BI analysis;; - Establish good technical influence inside and outside the department, cultivate talents, effectively coach the team, and improve R&D capabilities;; Minimum Qualifications; - Bachelor or above degree in computer science or a related technical discipline; 5 years of industrial experience; - Experience with software development in at least one of the following programming languages: C++, Python, Go, Java;; - Be proficient in the technology stack of the server(rpc, message queue, link tracking and service governance, etc.) in distributed \/ high concurrency scenarios;; - Experience with big data systems and related technologies (Spark, Flink, ClickHouse, HBase etc.);; - Great communication and teamwork skills;; Preferred Qualifications; - Experience with data insight and related technologies (machine learning, statistical analysis etc.) is preferred.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85028967","Role":"DevOps (ShipHats) Engineer \/ Senior Associate, Data & AI, Technology Consulting","Company":"Ernst & Young Solutions LLP","Location":"Singapore","Publish_Time":"2025-06-18 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85028967","job_desc":"Job Description: DevOps (ShipHats) Engineer, Senior Associate,  AI Data, Technology Consulting; At EY, we develop you with future-focused skills and equip you with world-class experiences. We empower you in a flexible environment, and fuel you and your extraordinary talents in a diverse and inclusive culture of globally connected teams.; We work together across our full spectrum of services and skills powered by technology and AI, so that business, people and the planet can thrive together. ; We\u2019re all in, are you?; Join EY and shape your future with confidence.; About the opportunity; EY AI & Data is the data and advanced analytics capability within EY Asia-Pacific, with over 500 specialist employees working across multiple industry sectors. We implement information-driven strategies, data platforms and advanced data analytics solution systems that help grow, optimize and protect client organizations. We go beyond strategy and provide end to end design, build and implementation of real-life data environments and have some of the best architects, project managers, business analysts, data scientists, big data engineers, developers and consultants in the region. ; We are seeking a DevOps Engineer (Senior Associate) with proven experience delivering Government Commercial Cloud (GCC) projects for Singapore government agencies and ministries. The ideal candidate will have hands-on expertise in implementing SHIP-HATS frameworks and designing\/maintaining robust CI\/CD pipelines across multiple projects.; Your key responsibilities:; Design, implement, and maintain CI\/CD pipelines to support efficient and secure application deployment in GCC environments.; Lead or contribute to end-to-end DevOps projects for Singapore government agencies or ministries.; Implement and enhance solutions aligned with SHIP-HATS compliance standards.; Collaborate with application teams, architects, and security specialists to ensure smooth delivery and operation of cloud-native and container-based applications.; Perform troubleshooting, monitoring, and performance tuning of DevOps toolchains and infrastructure.; Document solutions, create deployment guides, and support knowledge transfer to client or internal teams.; Skills & attributes for success:; 3\u20135 years of experience in DevOps, with senior-level contributions in at least 1\u20132 GCC projects involving Singapore government agencies.; Strong experience in implementing and optimizing CI\/CD pipelines (e.g., Jenkins, GitLab CI, Azure DevOps).; Practical experience with SHIP-HATS framework implementation.; Proficient with tools such as Docker, Kubernetes, Terraform, Ansible (or similar).; Familiarity with security best practices in government cloud environments.; Strong communication skills for stakeholder engagement and technical documentation.; Familiarity with agile delivery frameworks, including Scrum or equivalent.; Certified in AWS and\/or Talend.; What we look for; Highly motivated individuals with excellent problem-solving skills and the ability to prioritize shifting workloads in a rapidly changing industry. An effective communicator, you\u2019ll be a confident team player that collaborates with people from various teams while looking to develop your career in a dynamic organization.; What working at EY offers; EY offers a competitive remuneration package where you\u2019ll be rewarded for your individual and team performance. We are committed to being an inclusive employer and are happy to consider flexible working arrangements. Plus, we offer:; Continuous learning: You\u2019ll develop the mindset and skills to navigate whatever comes next.; Success as defined by you: We\u2019ll provide the tools and flexibility, so you can make a meaningful impact, your way.; Transformative leadership: We\u2019ll give you the insights, coaching and confidence to be the leader the world needs.; Diverse and inclusive culture: You\u2019ll be embraced for who you are and empowered to use your voice to help others find theirs.; Company description; EY is building a better working world by creating new value for clients, people, society and the planet, while building trust in capital markets.; Enabled by data, AI and advanced technology, EY teams help clients shape the future with confidence and develop answers for the most pressing issues of today and tomorrow.; EY teams work across a full spectrum of services in assurance, consulting, tax, strategy and transactions. Fueled by sector insights, a globally connected, multi-disciplinary network and diverse ecosystem partners, EY teams can provide services in more than 150 countries and territories.; All in to shape the future with confidence.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85155155","Role":"Backend Engineer, TikTok Ecommerce Recommendation Infrastructure","Company":"TikTok Pte. Ltd.","Location":"Singapore","Publish_Time":"2025-06-17 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85155155","job_desc":"Responsibilities; TikTok will be prioritizing applicants who have a current right to work in Singapore, and do not require TikTok's sponsorship of a visa. TikTok is the leading destination for short-form mobile video. Our mission is to inspire creativity and bring joy.; TikTok has global offices including Los Angeles, New York, London, Paris, Berlin, Dubai, Singapore, Jakarta, Seoul and Tokyo. Why Join Us Creation is the core of TikTok's purpose.; Our platform is built to help imaginations thrive. This is doubly true of the teams that make TikTok possible. Together, we inspire creativity and bring joy; a mission we all believe in and aim towards achieving every day.; To us, every challenge, no matter how difficult, is an opportunity; to learn, to innovate, and to grow as one team. Status quo? Never. Courage? Always. At TikTok, we create together and grow together. That's how we drive impact; for ourselves, our company, and the communities we serve.; Join us. About the Team E-commerce is a new and fast growing business that aims at connecting all customers to excellent sellers and quality products on TikTok Shop, through E-commerce live-streaming, E-commerce short videos, and commodity recommendation. Our E-ecommerce Recommendation Infra team is responsible for building up and optimizing the infrastructure for such recommendation systems, so as to provide the most stable and best experience for our users.; We work closely with applied machine learning engineers and build scalable systems to support all kinds of innovative algorithms and techniques. Responsibilities; Build and maintain high performance online services for TikTok recommendation system; Build extremely efficient and reliable data pipelines for candidates generation, profile generation, training examples generation, realtime online training, etc; Build globalized large-scale recommendation system; Design and develop high performance computing frameworks and storage systems; Research, design, and develop computer and network software or specialised utility programs; Analyse user needs and develop software solutions, applying principles and techniques of computer science, engineering, and mathematical analysis; Update software, enhances existing software capabilities, and develops and direct software testing and validation procedures; Work with computer hardware engineers to integrate hardware and software systems and develop specifications and performance requirements; Qualifications; Bachelor's degree or above, majoring in Computer Science, or related fields; Experience programming in at least one of the following programming languages: C, C++, Java or Golang;; Effective communication skills and a sense of ownership and drive;; Experienced in at least one area of the following areas: personalized recommendations, search engine, machine learning, distributed storage system, big data frameworks is a plus.; Preferred Qualifications; Experience in e-commerce related areas; TikTok is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives. Our platform connects people from across the globe and so does our workplace. At TikTok, our mission is to inspire creativity and bring joy. To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach. We are passionate about this and hope you are too.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"82880629","Role":"Agentic AI Developer, AI & Data, Technology Consulting","Company":"Ernst & Young Solutions LLP","Location":"Singapore","Publish_Time":"2025-06-17 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/82880629","job_desc":"At EY, we develop you with future-focused skills and equip you with world-class experiences. We empower you in a flexible environment, and fuel you and your extraordinary talents in a diverse and inclusive culture of globally connected teams. We work together across our full spectrum of services and skills powered by technology and AI, so that business, people and the planet can thrive together. We\u2019re all in, are you? Join EY and shape your future with confidence.; The opportunity; EY DnA is the data and advanced analytics capability within EY Asia-Pacific, with over 500 specialist employees working across multiple industry sectors. We implement information-driven strategies, data platforms and advanced data analytics solution systems that help grow, optimize and protect client organizations. We go beyond strategy and provide end to end design, build and implementation of real life data environments and have some of the best architects, project managers, business analysts, data scientists, big data engineers, developers and consultants in the region. We are looking for a AI Engineers within the AI & Data team in our Singapore office. This role is offered on a flexible full-time basis.; Your key responsibilities; Build end-to-end Gen AI solutions - develop, refine, and implement advanced Gen AI models and ensure the success delivery of projects; Lead the integration of LLMs and LangChain into business processes.; Utilize Python and other data manipulation languages proficiently to prepare and manipulate data.; Understand the business requirements and translate into Gen AI solution design that successfully meets the business objectives.; Collaborate with stakeholders, presenting findings to a non-technical audience and providing strategic recommendations.; Stay current with technical and industry developments and standards to ensure effective and advanced applications of data analysis techniques and methodologies.; Skills and attributes for success; Knowledge and experience in end-to-end project delivery, especially agile delivery methodologies or hybrid approaches; Agentic AI \/ Generative AI solution design and implementation; Exceptional communication, documentation and presentation skills and stakeholder management experiences; To qualify for the role, you must have; Min Bachelor\u2019s degree in Computer science, Mathematics, Engineering, Statistics, or a related field.; At least 3 years of experience with Generative AI, specifically with Large Language Models (LLM) and Langchain.; Proficiency in Python and other applicable programming languages.; Strong knowledge of machine learning, data mining, and predictive modeling.; Excellent understanding of machine learning algorithms, processes, tools, and platforms.; Possess strong problem-solving and strategic thinking abilities.; What we look for; Highly motivated individuals with excellent problem-solving skills and the ability to prioritize shifting workloads in a rapidly changing industry.; An effective communicator, you\u2019ll be a confident leader equipped with strong people management skills and a genuine passion to make things happen in a dynamic organization.; What we offer; EY offers a competitive remuneration package commensurate with your work experience where you\u2019ll be rewarded for your individual and team performance. We are committed to being an inclusive employer and are happy to consider flexible working arrangements, where this may be needed, guided by our FWA Policy.; Plus, we offer:; Continuous learning: You\u2019ll develop the mindset and skills to navigate whatever comes next.; Success as defined by you: We\u2019ll provide the tools and flexibility, so you can make a meaningful impact, your way.; Transformative leadership: We\u2019ll give you the insights, coaching and confidence to be the leader the world needs.; Diverse and inclusive culture: You\u2019ll be embraced for who you are and empowered to use your voice to help others find theirs.; If you can demonstrate that you meet the criteria above, apply today!; EY is building a better working world by creating new value for clients, people, society and the planet, while building trust in capital markets. Enabled by data, AI and advanced technology, EY teams help clients shape the future with confidence and develop answers for the most pressing issues of today and tomorrow. EY teams work across a full spectrum of services in assurance, consulting, tax, strategy and transactions. Fueled by sector insights, a globally connected, multi-disciplinary network and diverse ecosystem partners, EY teams can provide services in more than 150 countries and territories. All in to shape the future with confidence.; Apply now.","salary":"","work_type":"","country":"singapore"}
{"Job_ID":"85170176","Role":"NPI Engineer (New Product Introduction Engineer)","Company":"Leadtop Technology Pte Ltd","Location":"One North","Publish_Time":"2025-06-25 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85170176","job_desc":"We are seeking a highly skilled NPI Engineer to lead the testing and introduction of new semiconductor products. In this role, you will be responsible for driving the test plan development, data analysis, yield management, and test facility coordination. You will play a crucial part in ensuring the successful transition of products from engineering phase to mass production.; Position Description\uff1a; 1. Develop and implement new product test introduction and delivery plans. Ensure timely and effective execution of test plans for new products .; 2. Implement advanced analysis methods for test data, and manage data tools and software used for test data analysis. Continuously improve testing data quality and analysis capabilities.; 3. Organize and analyze ATE test data during the engineering phase. Perform characterization data analysis and analyze small-batch data. Complete comprehensive test reports based on analysis.; 4. Lead yield management efforts, including identifying and analyzing yield-limiting issues and implementing yield improvement plans. Establish and continuously improve yield analysis processes, workflows, and end-to-end management systems.; Position Requirements:; 1. Bachelor\u2019s degree or above in Microelectronics, Semiconductor Engineering, or a related field.; 2. Minimum of 8 years of experience in test process introduction. Strong experience with ATE test systems and data analysis tools.; 3. Profound understanding of test principles, process principles, WAT testing principles, and big data analysis.; 4. Proficiency with data analysis software and tools, such as JMP, Minitab, Excel VBA, or similar.; 5. Familiarity with OSAT operations and management principles, including knowledge of contractor test facility operations, internal process flows, and the pros and cons of different test facilities.; 6. Strong problem-solving and cross-functional communication skills.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85155571","Role":"Site Reliability Engineer, Compute Platform","Company":"TikTok Pte. Ltd.","Location":"Singapore","Publish_Time":"2025-06-17 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85155571","job_desc":"Responsibilities; TikTok will be prioritizing applicants who have a current right to work in Singapore, and do not require TikTok's sponsorship of a visa. About Us; TikTok is the leading destination for short-form mobile video. At TikTok, our mission is to inspire creativity and bring joy. TikTok's global headquarters are in Los Angeles and Singapore, and its offices include; New York, London, Dublin, Paris, Berlin, Dubai, Jakarta, Seoul, and Tokyo. Why Join Us; Creation is the core of TikTok's purpose. Our products are built to help imaginations thrive. This is doubly true of the teams that make our innovations possible.; Together, we inspire creativity and enrich life - a mission we aim towards achieving every day. To us, every challenge, no matter how ambiguous, is an opportunity; to learn, to innovate, and to grow as one team. Status quo?; Never. Courage? Always.; At TikTok, we create together and grow together. That's how we drive impact-for ourselves, our company, and the users we serve. Join us.; About the team TikTok and affiliate are developing the next-generation high-performance analytical database, with a mission to enable efficient and real-time data-driven decision-making on PB-level data sets. The initial product was forked from Clickhouse, after which large re-architecture had been taken place.; The product now not only improves the efficiency of Clickhouse but also fits into the elastic cloud-native infrastructure with better scalability and resource utilization. With years of polishment in the internal EB-level scenarios, we are now ready to serve our business partners via various cloud vendors. What you will be doing:; - Responsible for the real-time business stability system construction of ByteDance data platform , and promote the stability and service quality improvement of real-time Big data products; - Responsible for ensuring the stability of Flink and data streams, and implementing them from the perspective of problem improvement and governance.; At the same time, work closely with the Product Research & Development team to improve the efficiency of fault hemostasis. - Responsible for the automation tool capacity building of the real-time platform , from standardization precipitation to tooling, to improve the ability of problem discovery and rapid hemostasis.; Qualifications; Minimum Qualifications:; Computer-related major, full-time bachelor's degree or above, 2 years or more experience in SRE operation and maintenance in the real-time field of; Big data;; Familiar with the architecture and principles, operation and maintenance, and stability construction of real-time computing product components.; Product components include: Flink , Kafka, Hadoop, Spark, Kafka, etc.; Familiar with at least one programming language, including but not limited to: Shell, Python, Java, Golang, etc.; Have good communication skills, teamwork and self-motivation to promote cross-team cooperation.; Preferred Qualifications:; 3 years or more experience in SRE operation and maintenance in the real-time field of; Big data;; Have practical experience in troubleshooting and handling; Big data product issues, and have the ability to quickly troubleshoot and locate problems when facing online Big data product issues. TikTok is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives.; Our platform connects people from across the globe and so does our workplace. At TikTok, our mission is to inspire creativity and bring joy. To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach.; We are passionate about this and hope you are too.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"82808534","Role":"Cloud Security Architect - DS\/EZ","Company":"ST Engineering Mission Software & Services Pte Ltd","Location":"North Region","Publish_Time":"2025-06-18 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/82808534","job_desc":"Key Responsibilities:; Design and implement secure cloud architectures for various cloud platforms (e.g., AWS, Azure, Google Cloud, On-Premise (Nutanix, VMWare\u2026); Develop and enforce security policies and procedures for cloud environments.; Conduct security assessments and audits of cloud infrastructure and applications.; Collaborate with IT and development teams to integrate security into cloud-based projects and deployments.; Monitor and respond to security incidents and threats in the cloud environment.; Provide guidance and training on cloud security best practices to internal teams.; Conduct detailed risk assessments of cloud environments, including the evaluation of security controls for third-party services, and implement risk mitigation strategies based on threat modeling and vulnerability analysis.; Stay updated with emerging security threats, technologies, and trends to continuously improve cloud security strategies and architecture.;  Qualifications; Familiar with the principles of common risks, attack and defense strategies; Understand how to systematically lead security architecture and risk governance.; Familiar with cloud-native and infrastructure technologies such as network access, LLM, big data, storage, cloud computing, microservices, zero trust, etc.; Familiarity with container security technologies (e.g., Docker, Kubernetes) and serverless security best practices.; Proficient in at least one language in Python, Java, or NodeJS, capable of doing code auditing\/reviews and development.; Knowledge of security frameworks and standards (e.g., CIS Benchmarks, GDPR, HIPAA).; Strong communication and excellent teamwork skills. With professional working ethics, and have a keen risk awareness. Results orientated, aiming for long-term growth in this field. ; Proven leadership in managing cross-functional security teams.; Certifications such as CISM, CEH, or CRISC.;  Job Skills; Experience with security tools, such as firewalls, IDS\/IPS, SIEM, and vulnerability scanners; Ability to conduct risk assessments and develop security plans; Experience with regulatory compliance and audit processes; A problem solver and leader.; Prior experience as a consultant with WOG experienced","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85578939","Role":"Cloud Solution Architect (Automotive Industry) - Singapore","Company":"Byteplus","Location":"Central Region","Publish_Time":"2025-07-07 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85578939","job_desc":"ByteDance; Founded in 2012, ByteDance's mission is to inspire creativity and enrich life. With a suite of more than a dozen products, including TikTok as well as platforms specific to the China market, including Toutiao, Douyin, and Xigua, ByteDance has made it easier and more fun for people to connect with, consume, and create content.; Why Join Us; Creation is the core of ByteDance's purpose. Our products are built to help imaginations thrive. This is doubly true of the teams that make our innovations possible.; Together, we inspire creativity and enrich life - a mission we aim towards achieving every day.; To us, every challenge, no matter how ambiguous, is an opportunity; to learn, to innovate, and to grow as one team. Status quo? Never. Courage?; Always.; At ByteDance, we create together and grow together. That's how we drive impact - for ourselves, our company, and the users we serve.; Join us.; Join the Fast-Growing Volcano Engine Automotive Team; Volcano Engine Automotive team is one of the leading business units formed to serve global automotive and smart mobility enterprise customers with our leading technology solutions. As part of the Volcano Engine Automotive team, you help enterprise customers build what's next for their business. Leveraging Bytedance's cutting-edge intelligent technologies, such as AI, big data, cloud computing, we are devoted to developing innovative products and solutions to shape the future, e.g. smart cockpit, autonomous driving, digital marketing. We empower our clients to focus on what truly matters. Centering on inspiring innovativeness and excellence, we create life-changing solutions that impact lives around the world. You can help us to achieve our mission.; Responsibilities; 1. Cloud Architecture Design: Design and implement cloud-based solutions for scalability, security and performance. Utilize the Volcano Engine Cloud Computing Platform to support automotive industry applications.; 2. Pre-sales and after-sales support: Collaborate with the sales team to expand business in areas such as autonomous driving, intelligent networking, and automotive simulation. Provide technical consulting and architecture guidance throughout the pre-sales, implementation, and after-sales stages.; 3. Cloud migration: Assist customers in deploying or migrating systems to the Volcano Engine Cloud Computing Platform and maximizing the benefits of Cloud Services.; 4. Customer engagement: Act as a trusted advisor to your customers, understand their business, technical and operational needs, and develop tailored solutions that meet their goals.; Minimum qualifications:; - Engineering Bachelor degree and above.; - At least 5 years of Cloud Service experience, and at least 3 years of experience in designing and implementing solutions on major Cloud Computing platforms.; Preferred qualifications:; - Possess extensive knowledge of Cloud Service products and services, including computing, storage, networking, security, databases, etc., and have experience in designing cloud solutions.; - Familiar with the automotive industry, with experience in designing cloud solutions for automotive industry customers (including but not limited to autonomous driving, intelligent networking, simulation, and digital marketing).; - Experience working with automotive OEMs and suppliers in consulting or solution architecture.; - Able to work under pressure and manage multiple client projects simultaneously.; ByteDance is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives. Our platform connects people from across the globe and so does our workplace. At ByteDance, our mission is to inspire creativity and enrich life. To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach. We are passionate about this and hope you are too.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85155363","Role":"Backend Engineer Intern (Marketing Data Product) - 2025 Start","Company":"TikTok Pte. Ltd.","Location":"Singapore","Publish_Time":"2025-06-17 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85155363","job_desc":"Responsibilities; Team Introduction The Marketing Intelligence team focuses on exploring and developing innovative approaches, methodologies, and technologies to enhance data-driven marketing strategies and practices Ad Data is the cornerstone of every business decision that millions of advertisers make every day on TikTok and its affiliates. Therefore, we are tasked with innovating on daily basis to create and maintain complex systems at large scale and continue expanding the capacity to better serve advertisers growing at exponential pace. As a project intern, you will have the opportunity to engage in impactful short-term projects that provide you with a glimpse of professional real-world experience. You will gain practical skills through on-the-job learning in a fast-paced work environment and develop a deeper understanding of your career interests. Applications will be reviewed on a rolling basis - we encourage you to apply early. Successful candidates must be able to commit to the following Internship: We will prioritize candidates who can commit to a minimum of 6 months, with full-time preferred. We are seeking a Backend Engineer Intern to join our team responsible for the development of ByteDance\u2019s international marketing data product. This role offers hands-on experience in backend development, system optimization, and data-driven product innovation. Responsibilities: - Participate in backend development for our international marketing data product, contributing to technical solution design and high-quality implementation. - Ensure system stability and performance, assisting in troubleshooting, monitoring, and optimizing backend services. - Collaborate closely with business and data teams to understand and abstract business requirements, leveraging data to drive product value. Qualifications; Minimum Qualifications - Strong communication skills, self-learning ability, and a proactive attitude with a strong sense of ownership and responsibility. - Proficiency in at least one backend programming language (Java, Go, Python, etc.), with good coding practices and documentation habits. - Strong business acumen, with the ability to quickly understand business contexts and integrate technical solutions effectively. Preferred Qualification - Familiarity with big data engineering technologies (such as data segmentation, OLAP, data services, user profiling, or marketing analytics) is a plus. By submitting an application for this role, you accept and agree to our global applicant privacy policy, which may be accessed here: https:\/\/careers.tiktok.com\/legal\/privacy. If you have any questions, please reach out to us at apac-earlycareers@tiktok.com","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85155726","Role":"Tech Lead, TikTok Recommendation Developer Infrastructure - Singapore","Company":"TikTok Pte. Ltd.","Location":"Singapore","Publish_Time":"2025-06-17 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85155726","job_desc":"Responsibilities; About TikTok TikTok will be prioritizing applicants who have a current right to work in Singapore, and do not require TikTok sponsorship of a visa. TikTok is the leading destination for short-form mobile video. At TikTok, our mission is to inspire creativity and bring joy.; TikTok's global headquarters are in Los Angeles and Singapore, and its offices include New York, London, Dublin, Paris, Berlin, Dubai, Jakarta, Seoul, and Tokyo. Why Join; Us Creation is the core of TikTok's purpose. Our platform is built to help imaginations thrive. This is doubly true of the teams that make TikTok possible.; Together, we inspire creativity and bring joy; a mission we all believe in and aim towards achieving every day.; To us, every challenge, no matter how difficult, is an opportunity; to learn, to innovate, and to grow as one team. Status quo? Never. Courage? Always. At TikTok, we create together and grow together. That's how we drive impact; for ourselves, our company, and the communities we serve.; Join us. About the team Our Recommendation Architecture Team is responsible for building up and optimizing the architecture for our recommendation system to provide the most stable and best experience for our TikTok users.; Due to the big system scale and large engineering community that covers multiple Recommendation products, we invest heavily in the developer infra area, from developer environments and continuous integration\/continuous deployment (CI\/CD) to frameworks, libraries, and various productivity tools. Responsibilities; Provide high-performance, highly available, and flexible architecture design, high-quality code implementation, and continuously improve engineering quality based on product and strategy requirements for complex business scenarios;; Build a recommendation system analysis platform to provide product-level tool support for problem analysis and debugging;; Thoroughly comb through existing business systems, identify and optimize weak links in the system, improve the overall architecture quality, service performance, and stability of the system;; Abstract and precipitate common business architecture based on actual business conditions, improve the degree of basic capability reuse, and better support business rapid iteration.; Qualifications; Minimum Qualifications - Bachelor's degree or above, majoring in Computer Science, or related fields, with at least 5 years of experience building scalable systems; - Experience in programming, included but not limited to, the following programming languages: C, C++, Java or Golang;; - Effective communication skills and a sense of ownership and drive; Preferred Qualifications - Experienced in at least one area of the following areas: personalized recommendations, search engine, machine learning, distributed storage system, big data frameworks is a plus.; TikTok is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives. Our platform connects people from across the globe and so does our workplace. At TikTok, our mission is to inspire creativity and bring joy.; To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach. We are passionate about this and hope you are too.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85155160","Role":"Backend Engineer Intern (TikTok Recommendation Developer Infrastructure) -...","Company":"TikTok Pte. Ltd.","Location":"Singapore","Publish_Time":"2025-06-17 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85155160","job_desc":"Responsibilities; About the Team E-commerce is a new and fast growing business that aims at connecting all customers to excellent sellers and quality products on TikTok Shop, through E-commerce live-streaming, E-commerce short videos, and commodity recommendation. Our E-ecommerce Recommendation Infra team is responsible for building up and optimizing the infrastructure for such recommendation systems, so as to provide the best experience for our users. We work closely with applied machine learning engineers and build scalable systems to support all kinds of innovative algorithms and techniques.; We are looking for talented individuals to join us for an internship in 2025. Internships at TikTok aim to offer students industry exposure and hands-on experience. Watch your ambitions become reality as your inspiration brings infinite opportunities at TikTok.; Candidates can apply to a maximum of two positions and will be considered for jobs in the order you apply. The application limit is applicable to TikTok and its affiliates' jobs globally. Applications will be reviewed on a rolling basis; we encourage you to apply early.; Successful candidates must be able to commit to at least 3 months long internship period. Responsibilities; Build and maintain high performance online services for TikTok recommendation system\uff1b; Build extremely efficient and reliable data pipelines for candidates generation, profile generation, training examples generation, realtime online training, etc;; Build globalized large-scale recommendation system;; Design and develop high performance computing frameworks and storage systems.; Qualifications; Qualifications: Minimum Qualifications: - Undergraduate, or Postgraduate who is currently pursuing a degree\/master in Computer Science, Computer Engineering, Information Systems or a related technical major;; - Experience programming in at least one of the following programming languages: C, C++, Java or Golang; - Effective communication skills and a sense of ownership and drive; Preferred Qualifications:; - Experienced in at least one area of the following areas: personalized recommendations, search engine, machine learning, distributed storage system, big data frameworks is a plus. By submitting an application for this role, you accept and agree to our global applicant privacy policy, which may be accessed here: https:\/\/careers.tiktok.com\/legal\/privacy. If you have any questions, please reach out to us at apac-earlycareers@tiktok.com","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"82916068","Role":"Cloud Architect","Company":"PricewaterhouseCoopers","Location":"Singapore","Publish_Time":"2025-06-17 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/82916068","job_desc":"Line of Service; Advisory; Industry\/Sector; Not Applicable; Specialism; Technology Strategy; Management Level; Senior Manager; Job Description & Summary; We believe that challenges are best solved together. That\u2019s why, when you join us, you become part of a diverse and global community of problem-solvers. You'll find an unexpected mix of people who bring their unique expertise to build trust in society and tackle important issues. Here, we welcome and encourage you to lead with value and inspiration, question and challenge assumptions, as well as embrace new opportunities to deliver quality outcomes in exciting and unexpected ways, all with the support of technology.; At PwC Southeast Asia Consulting, we help businesses to work smarter and grow faster. We partner with our clients to build effective organizations, innovate and grow, reduce costs, manage risk and regulation and, leverage talent. Our aim is to support businesses in designing, managing and executing lasting beneficial change.; PwC\u2019s Southeast Asia Consulting practice provides a comprehensive range of professional services and experience to deliver large scale, cross territory transformation projects, wherever our clients need us to be \u2013 from strategy through to execution.; About the Team;   Our practice spans the whole range of clients\u2019 digital capabilities, ranging from strategy to execution. We help solve complex boardroom problems, and work with the team to not only solve problems of today but get them ready for new opportunities for tomorrow, creating the pathway of how technology can enable business visions. We help our clients with advisory services such as IT Ops Transformation, Cloud Transformation, Data Transformation, AI Transformation and leveraging on digital tools to connect across the enterprise.;   About the role; Represent PwC in the design, deploy and configure solutions in the cloud; Working with customer to understand their current state from infra, applications, database, APIs to allow you to come out with future state of cloud environment; Working with customer to come out with the future state modernize application architecture ; Able to conduct workshop to explain what microservices are and explain the different techniques in modern application architecture; Hands on to delivery in implementation owning the architecture design and planning.; To craft cloud strategy, conduct cloud migration assessment, present the outcome; Design and Automate cloud operations, develop infrastructure automation scripts and participates in the continuous improvement of cloud solutions.; Be the technical SME in front of client. ; Participate in the specification, setup and run Proof of Concepts and demonstrations of cloud solutions.; Workstream lead to manage the migration engineering team where you are expected to drive meeting discussion, drive work review, provide advise to engineers, be prepared to do hands on during required period.; To prepare required documentation, review of documentation, and presentation of documentation; To impart your skills to juniors to build a team; To be involved in pre-sales and proposal development; Working within customer requirements and advising customers on best practices while upholding customer service standards;; Actively involve in cloud conversation with public sector and financial institute across Southeast Asia. ; ; About you; Bachelor\u2019s degree in information systems, Computer Science, Engineering, or a related field or foreign equivalent from a recognized university.; Minimum 10 years of experience, implementing private \/ public \/ hybrid cloud infrastructure solutions; Done at least 5 cycle of cloud implementation (migration or greenfield); Strong AWS\/Azure cloud concepts and hand on experience working with cloud services; Proficient in nodejs, reactjs or any of the js programming language; Proficient in designing event driven architecture \/ microservices architecture; Proficient in doing infra sizing for applications; Experience in the design of multi-cloud services.; Possess knowledge of cloud architecture, cloud native patterns, container management and cloud computing capabilities especially as offered by CSPs like Microsoft Azure and AWS; Strong background in Linux\/Unix and\/or Windows administration.; Solid understanding of networking and core Internet Protocols such as TCP\/IP, DNS, SMTP, HTTP and routing in distributed networks; Understanding of the various IT infrastructures and application stacks.; Utilizing cloud security controls, network boundary controls, and related toolsets.; Understanding agile software development and ability to apply DevOps concepts such as Infrastructure-as-Code using Azure Resource Manager \/ Biceps \/ Cloud Formation\/ Terraform\/ Ansible\/ Chef\/ Puppet; Experienced with scripting in one or more of the following languages: Python, Java, JavaScript and C#\/. NET; Added advantage if you are Enterprise Architect certified; Preferred to have one or more of the following technical certifications:; Microsoft Certified Azure Solution Architect Expert; AWS Solution Architect Professional; Education (if blank, degree and\/or field of study not specified); Degrees\/Field of Study required:Degrees\/Field of Study preferred:; Certifications (if blank, certifications not specified); Required Skills; Optional Skills; Accepting Feedback, Accepting Feedback, Active Listening, Algorithm Development, Alteryx (Automation Platform), Analytical Thinking, Analytic Research, Big Data, Business Data Analytics, Coaching and Feedback, Communication, Complex Data Analysis, Conducting Research, Creativity, Customer Analysis, Customer Needs Analysis, Dashboard Creation, Data Analysis, Data Analysis Software, Data Collection, Data-Driven Insights, Data Integration, Data Integrity, Data Mining, Data Modeling {+ 46 more}; Desired Languages (If blank, desired languages not specified); Travel Requirements; Not Specified; Available for Work Visa Sponsorship?; Yes; Government Clearance Required?; Yes; Job Posting End Date","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"83897037","Role":"Tencent Cloud - Senior Cloud Architect (R&D & Solution Design)","Company":"Tencent International Service Pte. Ltd.","Location":"Singapore","Publish_Time":"2025-06-17 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/83897037","job_desc":"Business Unit; Cloud & Smart Industries Group (CSIG) is responsible for promoting the company's cloud and industry Internet strategy. CSIG explores the interactions between users and industries to create innovative solutions for smart industries via technological advancements such as cloud, AI, and network security. While driving the digitalization of retail, medical, education, transportation and other industries, CSIG helps companies serve users in smarter ways, building a new ecosystem of intelligent industries that connect users and businesses. What the Role Entails; Position Overview:; As a Senior Cloud Architect at Tencent Cloud, you will be responsible for designing and implementing advanced cloud architectures, collaborating with internal product and R&D teams, and delivering tailored cloud solutions for our customers. This role requires deep technical expertise in cloud infrastructure and a passion for innovation, as you will also be leading R&D efforts to enhance our cloud platform.; This position requires travel to Indonesia, potentially on a frequent basis.; Responsibilities:; Design and architect scalable, secure, and reliable cloud infrastructure to support Tencent Cloud's expanding portfolio of products and services.; Lead the technical development and integration of cloud solutions, ensuring they meet business objectives and customer requirements.; Drive cloud R&D initiatives by exploring new technologies and developing innovative solutions to optimize our cloud platform.; Collaborate with cross-functional teams, including product development and external partners, to deliver end-to-end cloud solutions.; Provide technical leadership and guidance to teams on cloud architecture best practices, ensuring high availability, performance, and security.; Ensure the alignment of cloud architectures with industry standards and compliance requirements.; Support the achievement of revenue objectives through the adoption and migration of applications, software, and services onto the TencentCloud platform, in partnership with the sales team.; Troubleshoot and resolve complex issues related to cloud infrastructure, performance, and scalability.; Support the achievement of revenue objectives through the adoption and migration of applications, software, and services onto the Tencent Cloud platform, in partnership with the sales team.; Who We Look For; Bachelor's or Master's degree in Computer Science, Engineering, or a related field.; 5+ years of experience in cloud architecture design, with hands-on experience in cloud infrastructure, cloud-native technologies, and R&D.; Expertise in Tencent Cloud or similar platforms (e.g., AWS, Azure) with a deep understanding of IaaS, PaaS, and SaaS models.; Proven experience in designing and deploying cloud solutions, including microservices architecture, containers (Docker, Kubernetes), and CI\/CD pipelines.; Strong problem-solving skills with the ability to lead technical initiatives and propose innovative cloud solutions.; Excellent communication skills, with fluency in both English and Chinese (spoken and written) in order to manage internal and external stakeholders and craft technical documents in both languages effectively.; Preferred Skills:; Experience with cloud R&D projects, particularly in exploring emerging technologies like AI, big data, or edge computing.; Strong understanding of cloud security, monitoring, and compliance practices.; Familiarity with DevOps practices and automation tools for continuous integration and deployment.; Equal Employment Opportunity at Tencent; As an equal opportunity employer, we firmly believe that diverse voices fuel our innovation and allow us to better serve our users and the community. We foster an environment where every employee of Tencent feels supported and inspired to achieve individual and common goals.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85242809","Role":"Data Center Network Engineer (Assistant Manager\/Deputy Manager)","Company":"PSA Corporation Limited","Location":"Pasir Panjang","Publish_Time":"2025-06-28 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85242809","job_desc":"You will work alongside a team of energetic and passionate technology evangelists who are constantly thinking about making a difference to the way technology shapes one of the world's busiest port. Every ICT employee is given opportunity to innovate, influence and redefine how we work and to boldly pursue big ideas that power digital transformations for our Business Units. We envisage a future fully automated Port that is powered by cutting edge technologies that includes hybrid-multi cloud, cloud-native technologies, converged infrastructure, cybersecurity, DWDM Optical Networks, Software Defined Networks, and the emerging 5G mobile networks with edge computing, just to name a few. In short, we will constantly excite you, inspire you and develop you.;   The ideal candidate; An ideal candidate is someone who has experience in designing, deploying and supporting a robust and resilient enterprise network infrastructure that enables PSA's innovation and 24x7 mission critical port operations. You will work with a wide variety of modern networking solutions from top tier network product manufacturers and service providers. Alongside with your colleagues from multiple ICT pillars, you will identify and implement improvements that will drive measurable business results. You will leverage your technical expertise and soft skills to bring about great value and IT service experience to our Business Units and help the organization grow the business.;   Requirements:; \u2022 Possess a Diploma or Degree in Computer Engineering\/Electrical\/Electronic Engineering or Computer Science; \u2022 CCNP Enterprise or CCNP Data Centre certification or equivalent; \u2022 CCIE certification will be an added advantage; \u2022 Experience designing and building complex LAN and large WAN; \u2022 Expertise in routing\/switching technology, including BGP, OSPF, etc.; \u2022 At least 3 years of hands-on experience with Cisco campus and data center network products in an enterprise deployment; \u2022 Strong problem solving and analytical skills; \u2022 Good command of English, both written and spoken; \u2022 Excellent communication and interpersonal skills; \u2022 Ability to thrive under pressure, function and deliver effectively in a fast-paced environment; \u2022 Be prepared to work in a team providing 24x7 network support;   Good to have:; \u2022 Experience using ServiceNow for IT Service Management and IT Operations Management; \u2022 Experience using Splunk to support data driven network planning and troubleshooting; \u2022 Experience using Solarwinds to monitor network equipment; \u2022 Experience using Infoblox IPAM for IP address planning; \u2022 Experience designing and building network security (Palo Alto, Fortigate and Cisco firewall); \u2022 Experience in network automation (e.g. Ansible, Python, Netconf, and RESTful API etc); \u2022 Understanding of SDN concepts and technologies. Experience in solutions from Cisco (SD-WAN, ACI) and VMware (NSX-T) will be preferred.; \u2022 Familiarity with integration requirements for IP-based systems and storage technologies used in data centers such as Dell VxRail and PowerScale;   *Only shortlisted candidates are notified*","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"84986154","Role":"Staff Infrastructure Engineer","Company":"CME Group","Location":"Marina South","Publish_Time":"2025-06-17 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/84986154","job_desc":"An Infrastructure Engineer plays a critical role in designing, implementing, managing and maintaining the foundational technology systems that support CME Group\u2019s IT operations.  This role overlaps with both Data Center and Network Engineering responsibilities.; Core responsibilities:; Manage\/maintain the regional CDN network and EBS workstations.  The CDN network is an \u201cextranet\u201d that gets extended to customer sites.; Oversee installation, configuration and maintenance of physical infrastructure in regional data centers and office locations: servers, storage, power, cooling and cabling.; Ensure high availability, disaster recovery and backup systems are in place and tested.; Coordinate hardware lifecycle management and capacity planning.; Maintain data center environmental conditions and physical security.; Configure and manage core and edge network devices (routers, switches and firewalls).; Design and maintain LAN\/WAN architectures.; Implement and monitor network security (VPN, access control and firewalls).; Troubleshoot connectivity and performance issues.; Ensures infrastructure compliance with organizational policies and external regulations.; Use monitoring tools to proactively detect and resolve issues.; Work closely with other teams - Compute, CSG and SREs.; Requires in\/out of country field services support.; Requires flexibility in working hours to accommodate emergency situations.; Key skills and knowledge areas:; Have an understanding of all the DataCenter specific hardware \/ material.  Experience Corning data center accessories.; Knowledge of all parts to fully spec, assemble and test structured cabling between any 10g - 40G - 100g application for day to day use and specific to trading platforms. (Fiber, Copper, Coax, etc..); Full working knowledge of all cabinet level hardware, and industry standard cable management, documentation, and labeling standards \/ best practices.; Ability to distinguish the difference between all available power options at the cabinet level, and the ability to calculate full cabinet and RPP loads.; Understanding of the different trades required to complete low and high voltage work within a data center.; Have the fundamental networking skills needed to troubleshoot and configure a variety of network devices. (subnetting, vlans, etc.); Experience troubleshooting server OS, hardware (memory, motherboards, storage) and connectivity issues.; Servers, SAN\/NAS storage, UPS systems.; TCP\/IP, VLANs, BGP, OSPF, VPN, DHCP\/DNS.; Experience in DCIM system, e.g. Sunbird.; Google Cloud Platform would be desirable.; Scripting skills would be desirable.; Qualifications:; Bachelor#s degrees in Computer Science or related field.; BICSI RCDD \/ DCDC, Cisco CCNA or above.; CME Group: Where Futures are Made; CME Group is the world\u2019s leading derivatives marketplace. But who we are goes deeper than that. Here, you can impact markets worldwide. Transform industries. And build a career by shaping tomorrow. We invest in your success and you own it \u2013 all while working alongside a team of leading experts who inspire you in ways big and small. Problem solvers, difference makers, trailblazers. Those are our people. And we\u2019re looking for more.; At CME Group, we embrace our employees' unique experiences and skills to ensure that everyone\u2019s perspectives are acknowledged and valued. As an equal-opportunity employer, we consider all potential employees without regard to any protected characteristic.; Important Notice: Recruitment fraud is on the rise, with scammers using misleading promises of job offers and interviews to solicit money and personal information from job seekers. CME Group adheres to established procedures designed to maintain trust, confidence and security throughout our recruitment process. Learn more here.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85808751","Role":"Big Data Engineer","Company":"Unison Consulting Pte. Ltd.","Location":"Singapore","Publish_Time":"2025-07-16 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85808751","job_desc":"We are seeking a highly skilled and experienced Big Data Engineer to join our team. The ideal candidate will have a minimum of 5 years of experience managing data engineering jobs in big data environment e.g., Cloudera Data Platform. The successful candidate will be responsible for designing, developing, and maintaining the data ingestion and processing jobs. Candidate will also be integrating data sets to provide seamless data access to users.; SKILLS SET AND TRACK RECORD; * Good understanding and completion of projects using waterfall\/Agile methodology.; * Analytical, conceptualisation and problem-solving skills.; * Good understanding of analytics and data warehouse implementations; * Hands-on experience in big data engineering jobs using Python, Pyspark, Linux, and ETL tools like Informatica; * Strong SQL and data analysis skills. Hands-on experience in data virtualisation tools like Denodo will be an added advantage; * Hands-on experience in a reporting or visualization tool like SAP BO and Tableau is preferred; * Track record in implementing systems using Cloudera Data Platform will be an added advantage.; * Motivated and self-driven, with ability to learn new concepts and tools in a short period of time; * Passion for automation, standardization, and best practices; * Good presentation skills are preferred; The developer is responsible to:; * Analyse the Client data needs and document the requirements.; * Refine data collection\/consumption by migrating data collection to more efficient channels; * Plan, design and implement data engineering jobs and reporting solutions to meet the analytical needs.; * Develop test plan and scripts for system testing, support user acceptance testing.; * Work with the Client technical teams to ensure smooth deployment and adoption of new solution.; * Ensure the smooth operations and service level of IT solutions.; * Support production issues","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85817927","Role":"Data Engineer","Company":"TechnoPals Pte Ltd","Location":"West Region","Publish_Time":"2025-07-16 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85817927","job_desc":"Key Responsibilities:; Design, build, and optimize scalable data pipelines for data extraction, transformation, and loading (ETL\/ELT).; Develop and maintain data architectures, databases, and data warehouses.; Collaborate with data scientists and analysts to understand data needs and deliver clean, structured datasets.; Monitor and improve the performance of data systems.; Implement and enforce data quality, security, and governance practices.; Work with cloud platforms (e.g., AWS, Azure, GCP) for data storage and processing.; Automate data processes and workflows using orchestration tools like Airflow or similar.; Maintain documentation of data models, schemas, and systems.; Required Skills and Qualifications:; Bachelor's degree in Computer Science, Engineering, Information Technology, or related field.; Proven experience as a Data Engineer or in a similar role.; Proficient in programming languages such as Python, Scala, or Java.; Strong SQL skills and experience with relational and NoSQL databases (e.g., PostgreSQL, MySQL, MongoDB).; Hands-on experience with ETL tools and data pipeline frameworks (e.g., Apache Spark, Kafka, Airflow).; Experience with cloud data platforms like AWS (Redshift, Glue, S3), Azure (Data Factory, Synapse), or GCP (BigQuery, Dataflow).; Familiarity with data modeling, data warehousing concepts, and data lakes.; Strong problem-solving skills and attention to detail.; Preferred Qualifications:; Experience with big data technologies like Hadoop, Hive, or Presto.; Knowledge of CI\/CD and DevOps practices in data engineering.; Experience with version control tools (e.g., Git).; Understanding of data privacy and compliance standards (e.g., GDPR, HIPAA).","salary":"","work_type":"Kontrak\/Temporer","country":"singapore"}
{"Job_ID":"85817337","Role":"Data Engineer","Company":"SMRT Trains Ltd","Location":"Katong","Publish_Time":"2025-07-16 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85817337","job_desc":"Job Purpose; The Data Engineer will be part of the team to develop operation & maintenance decision-support tools to enhance train reliability and maintenance efficiency. This position involves designing, developing, and maintaining data pipelines, APIs, and cloud infrastructure for various rail-oriented applications. The ideal candidate will have expertise in data analysis, transformation, ingestion, database design, API development, and preferably, cloud infrastructure setup. Collaborating closely with software engineers, data scientists, and frontend developers, the Data Engineer will contribute to building efficient, scalable, and reliable systems.; Responsibilities; The duties and responsibilities for Data Engineer, are as listed below. The list is not comprehensive and related duties and responsibilities may be assigned from time to time.; Data Engineering & Processing:; Develop and maintain data pipelines for efficient data ingestion and transformation.; Work with structured and unstructured data to ensure optimal storage and retrieval.; Perform data analysis and report on results.; Database Design & Management:; Design and implement relational and NoSQL database schemas for scalability.; Optimize database performance through indexing, partitioning, and query tuning.; Implement data security and compliance best practices.; API Development & Backend Engineering:; Design and develop APIs for data access and application integration.; Implement authentication, authorization, and API security best practices.; Cloud Infrastructure & Deployment (Supporting Role):; Assist in design Azure cloud architectures; Work with IT infrastructure team to set up cloud infrastructure for application hosting, data storage and processing.; Collaboration & Best Practices:; Collaborate with internal stakeholders to understand their business needs.; Work with software engineers, data scientist, frontend developer to understand the data requirement and design architecture of the data platform.; Implement CI\/CD pipelines for automated testing, deployment and monitoring.; Write testable and maintainable code and documentation to deploy to production.; Engage continuously with end-user for feedback and improvements.; Qualifications & Work Experience; Degree in Science, Technology, Engineering or Mathematics (STEM); Previous experience as a data engineer or in a similar role; Data engineering certification is a plus; Knowledge of security best practices in cloud and database management is a plus; Skills; Technical skills include:; Programming and Data processing: MATLAB, Python, SQL, or similar languages.; Databases: My SQL, SQL Server, MongoDB or similar.; Cloud Platforms: Azure; DevOps & CI\/CD: Git Lab CI\/CD, Docker; Generic skills include:; Strong inclination and eager for continual learning and development; Strong team player; Critical thinking and problem-solving skills; Ability to understand and explain complex data and effective interactions with the stakeholders; Ability to think independently and actively propose solutions to the team.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85783909","Role":"Data Engineer (Python\/OpenSearch)","Company":"ASTEK Singapore Innovation Technology Pte. Ltd.","Location":"Central Region","Publish_Time":"2025-07-15 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85783909","job_desc":"Job Title: Data Engineer; Location: Singapore, CBD; Employment Type: Full-time Permanent; We are looking for a Data Engineer to join our team and support the development of a standardized Data Toolkit. This platform will streamline data ingestion and observability efforts across the organization. You will also assist in migrating data and systems from legacy tools to this new framework.; Key Responsibilities; Design, build, and test components of the Data Toolkit.; Integrate data systems to ensure consistency and efficiency in data workflows.; Support the migration of existing data and processes from legacy systems.; Collaborate with engineering and product teams to ensure seamless adoption.; Requirements; Minimum 3 years of full time working experience.; Proficiency in Python.; Experience with search platforms such as OpenSearch, Elasticsearch, or Solr.; Strong software development, analytical, and problem-solving skills.; Knowledge of SQL, ETL\/ELT, and data pipeline architecture will be advantageous.; Experience with CI\/CD, Terraform, and AWS cloud services is a plus.; Good communication skills and the ability to work well in a team.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85845477","Role":"Data Engineer","Company":"TOPPAN Ecquaria Pte Ltd","Location":"Braddell","Publish_Time":"2025-07-17 03:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85845477","job_desc":"Responsibilities:; Design and implement robust, scalable data pipelines and architectures to support data ingestion, processing, and storage. Including performance optimizations for data modeling and ingestion; Develop and optimize complex SQL queries and stored procedures for data extraction, transformation, and analysis.; Model data to meet different use casesng applications and automate data workflows.; Collaborate with data scientists and analysts to understand data requirements and deliver high-quality data solutions.; Lead the integration of data from various sources into data lakes and warehouses, ensuring data quality and consistency.; Monitor and troubleshoot data pipelines and workflows to ensure optimal performance and reliability.; Communicate with and support data users; Document data processes, data models, and architectural designs to ensure knowledge sharing and compliance with best practices.; Prerequisites:; Experience: Minimum 3 years in data engineering fields with system integration, and at least 1 year in system integration and implementation in cloud\/web-based environments.; Proven Solutions: Demonstrated experience in providing effective, working solutions and implementations, particularly in cloud-based environments.; Technical Skills:; Solid understanding of ETL processes, data warehousing concepts, and data modeling best practices.; Proficiency in Databricks, Azure Data lake, PowerBI, Tableau and related data processing and visualisation software.; Familiarity with Windows, Linux, AWS and\/or Azure platforms.; Strong programming skills in languages such as Python and R is a must; Proficiency in other programming languages such as Java, Scala and C# will be advantages; Experience in data processing frameworks (e.g., Apache Spark, Apache Flink); Preferred Exposure:; Experience with large-data management system with visualisation tools.; Experience with Data Integration and ETL Pipelines, Data Warehousing and BI reporting projects.; Experience with Singapore Government Project will be advantages; Personal Attributes:; Excellent problem-solving skills; Ability to work independently; Collaborative in a fast-paced environment; Why Join Us?; Be part of a forward-thinking team that is transforming government digital services. If you are passionate about technology and innovation, and thrive in a dynamic environment, we want to hear from you!; If you are passionate about building partnerships and driving growth, we would love to hear from you!; TOPPAN Ecquaria is an equal opportunity employer and values diversity within our company. We welcome all interested candidates to apply for this position, however, we regret to inform that only shortlisted candidates will be contacted by us for an interview.; Find us at www.topppanecquaria.com or www.linkedin.com\/company\/toppan-ecquaria; For more career opportunities, please visit our career site at:- https:\/\/toppanecquaria.com\/careers\/job-openings?utm_source=Jobstreet&utm_medium=Page&utm_campaign=2020 (Please copy & paste the above link onto your browser)","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85293565","Role":"BIG DATA PLATFORM ENGINEER","Company":"Matrix Process Automation Pte Ltd","Location":"Central Region","Publish_Time":"2025-07-01 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85293565","job_desc":"Operating Global Data Platform components (VM Servers, Kubernetes, Kafka) and applications (Apache stack, Collibra, Dataiku and similar).; Implement automation of infrastructure, security components, and Continuous Integration & Continuous Delivery for optimal execution of data pipelines (ELT\/ETL).; You have 5+ years of experience in building or designing large-scale, fault-tolerant, distributed systems, (for example: data lakes, delta lakes, data meshes, data lake houses, data platforms, data streaming solutions\u2026); In-depth knowledge and experience in one or more large scale distributed technologies including but not limited to: Hadoop ecosystem, Kafka, Kubernetes, Spark; Migration experience of storage technologies (e.g. HDFS to S3 Object Storage); Integration of streaming and file based data ingestion \/ consumption (Kafka, Control M, AWA); Experience in DevOps, data pipeline development, and automation using Jenkins and Octopus (optional: Ansible, Chef, XL Release, and XL Deploy); Expert in Python and Java or another static language like Scala\/R, Linux\/Unix scripting, Jinja templates, puppet scripts, firewall config rules setup; VM setup and scaling (pods), K8S scaling, managing Docker with Harbor, pushing Images through CI\/CD","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85813512","Role":"Data Engineer","Company":"Singapore Telecommunications Limited","Location":"Singapore","Publish_Time":"2025-07-16 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85813512","job_desc":"An empowering career at Singtel begins with a Hello. Our purpose, to Empower Every Generation, connects people to the possibilities they need to excel. Every \"hello\" at Singtel opens doors to new initiatives, growth, and BIG possibilities that takes your career to new heights. So, when you say hello to us, you are really empowered to say\u2026\u201cHello BIG Possibilities\u201d.; Be a Part of Something BIG!  ; Responsible for building and supporting data ingestion and transformation pipelines in a modern hybrid cloud platform; Independently develop basic batch and streaming pipelines, working with cloud tools such as Databricks and Kafka under the guidance of senior engineers; Contribute to the delivery of reliable, secure, and high-quality data for analytics, reporting, and machine learning use cases; Gain exposure to enterprise-scale data architecture, while growing into more advanced engineering responsibilities over time.; Make An Impact By; Build and maintain data ingestion pipelines for batch and streaming data sources using tools like Databricks and Kafka; Perform data transformation and cleansing using PySpark or SQL based on business and technical requirements; Monitor and troubleshoot data workflows to ensure data quality and pipeline reliability; Work closely with senior data engineers to understand platform architecture and apply best practices in pipeline design; Assist in integrating data from diverse source systems (files, APIs, databases, streaming); Help maintain metadata and pipeline documentation for transparency and traceability; Participate in integrating pipelines with tools such as Microsoft Fabric, Databricks, Delta Lake, and other platform components; Contribute to automation efforts using version control and CI\/CD workflows; Apply basic data governance and access control policies during implementatio; Skills to Succeed; Bachelor\u2019s degree in Computer Science, Engineering, or a related field; 1\u20133 years of experience in data engineering or data platform development; Proven ability to independently build basic batch or streaming data pipelines; Hands-on experience with Python and SQL for data transformation and validation; Familiarity with Apache Spark (especially PySpark) and large-scale data processing concepts; Self-starter with strong problem-solving skills and a keen attention to detail; Able to work independently while collaborating effectively with senior engineers and other stakeholders; Strong documentation and communication skills.; Rewards that Go Beyond; Full suite of health and wellness benefits  ; Ongoing training and development programs  ; Internal mobility opportunities; Your Career Growth Starts Here. Apply Now!; We are committed to a safe and healthy environment for our employees & customers and will require all prospective employees to be fully vaccinated.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85172239","Role":"Lead Big Data Engineer","Company":"PLOY ASIA PTE. LTD.","Location":"Singapore","Publish_Time":"2025-06-25 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85172239","job_desc":"What's on offer:; Location: Singapore; Client: End client user environment; ; Job Summary:; We are seeking a highly skilled and motivated Lead Big Data Engineer to join our data team. The ideal candidate will play a key role in designing, developing, and maintaining scalable big data solutions while providing technical leadership. This role will also support strategic Data Governance initiatives, ensuring data integrity, privacy, and accessibility across the organization.; Key Responsibilities:; Design, implement, and optimize robust data pipelines and ETL\/ELT workflows using SQL and Python.; Lead architecture discussions, including the creation and review of Entity Relationship Diagrams (ERDs) and overall system design.; Collaborate closely with Data Engineers, Analysts, and cross-functional engineering teams to meet evolving data needs.; Deploy and manage infrastructure using Terraform and other Infrastructure-as-Code (IaC) tools.; Develop and maintain CI\/CD pipelines for deploying data applications and services.; Leverage strong experience in AWS services (e.g., S3, Glue, Lambda, RDS, Lake Formation) to support scalable and secure cloud-based data platforms.; Handle both batch and real-time data processing effectively.; Apply best practices in data modeling and support data privacy and data protection initiatives.; Implement and manage data encryption and hashing techniques to secure sensitive information.; Ensure adherence to software engineering best practices including version control, automated testing, and deployment standards.; Lead performance tuning and troubleshooting for data applications and platforms.; ; Required Skills & Qualifications:; Strong proficiency in SQL for data modeling, querying, and transformation.; Advanced Python development skills with an emphasis on data engineering use cases.; Hands-on experience with Terraform for cloud infrastructure provisioning.; Proficiency with CI\/CD tools, particularly GitHub Actions.; Deep expertise in AWS cloud architecture and services.; Demonstrated ability to create and evaluate ERDs and contribute to architectural decisions.; Strong communication and leadership skills with experience mentoring engineering teams.; ; Preferred Skills:; Familiar AI\/ML RAG (Retrieval-Augmented Generation) MCP (Multi-Channel Processing) concepts; Understanding of data processing libraries (Pandas, NumPy); Familiarity with cloud platforms (AWS, GCP, or Azure); Knowledge of containerisation (Docker) and orchestration tools; Experience with CI\/CD pipelines; Basic understanding of data structures and algorithms","salary":"","work_type":"Kontrak\/Temporer","country":"singapore"}
{"Job_ID":"85818027","Role":"Staff Data Engineer","Company":"Network Guard","Location":"North Region","Publish_Time":"2025-07-16 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85818027","job_desc":"We are seeking a highly skilled and hands-on Staff Data Engineer to architect and maintain modern data infrastructure and pipelines. This is an individual contributor role focused on building scalable data platforms that power analytics, insights, and data-driven decisions across the company. You will work closely with analysts, data scientists, product, and engineering teams to design end-to-end data solutions using tools like AWS Redshift, Athena, Snowflake, and other leading cloud platforms.; ; ; What You\u2019ll Do; Design and build scalable data pipelines to ingest, process, and store large volumes of structured and unstructured data from diverse sources.; Develop and maintain robust data warehouse architectures leveraging tools such as AWS Redshift, Athena, and Snowflake.; Optimize data models, queries, and storage strategies for performance, scalability, and cost-effectiveness.; Collaborate with cross-functional stakeholders (analytics, product, ML) to gather requirements and deliver data solutions that support business goals.; Ensure data quality, security, and privacy through best practices in governance, testing, and monitoring.; Own and operate production data workflows, resolving incidents and ensuring reliability.; Stay up-to-date with the latest trends and advancements in data engineering and analytics.; ; ; Tech Stack; Languages & Tools: Python, SQL; Data Warehousing & Query Engines: Snowflake, AWS Redshift, Athena; Pipeline & Orchestration: Apache Airflow, AWS Glue, dbt; Cloud & DevOps: AWS (Lambda, S3, IAM), Docker, Terraform; Streaming : Kafka, Kinesis; BI & Analytics: Tableau, Power BI; ; ; What You\u2019ll Need To Succeed; Bachelor's or Master\u2019s degree in Computer Science, Engineering, or related field.; 5+ years of experience as a Data Engineer, with a strong focus on data pipeline development and data warehousing.; Deep proficiency in AWS Redshift, Athena, Snowflake, and experience working with large-scale data systems.; Strong programming and scripting skills in Python, SQL, and Shell.; Experience with pipeline orchestration tools (e.g., Airflow, Glue, dbt).; Solid understanding of data modeling, schema design, and ETL processes.; Familiarity with cloud infrastructure and DevOps practices (especially AWS).; Experience with BI platforms like Tableau or Power BI.; Excellent analytical and problem-solving skills.; Strong communication and the ability to work across functions and mentor others.; ; ; Nice to Have; Knowledge of data privacy and security best practices (e.g., GDPR, SOC2).; Exposure to MLOps and ML data pipelines.; Experience in high-growth, product-led or SaaS environments.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85686725","Role":"Senior Data Engineer","Company":"MasterCard Worldwide","Location":"Singapore","Publish_Time":"2025-07-11 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85686725","job_desc":"Our Purpose; Mastercard powers economies and empowers people in 200+ countries and territories worldwide. Together with our customers, we\u2019re helping build a sustainable economy where everyone can prosper. We support a wide range of digital payments choices, making transactions secure, simple, smart and accessible. Our technology and innovation, partnerships and networks combine to deliver a unique set of products and services that help people, businesses and governments realize their greatest potential.; Title and Summary; ; Senior Data Engineer; Overview; The Applied AI team is looking for a Senior Data Engineer to consistently innovate and problem-solve. The ideal candidate is passionate about building robust data infrastructure, highly motivated, intellectually curious, and thrives in a collaborative environment. You will play a key role in enabling data-driven decision-making across the organization by ensuring the availability of high-quality, scalable, and reliable data systems.; Role; In this position, you will:; \u2022 Design, build, and maintain scalable and efficient data pipelines to support analytics and machine learning initiatives.; \u2022 Collaborate with data scientists, analysts, and business stakeholders to understand data needs and deliver high-impact solutions.; \u2022 Develop and optimize ETL\/ELT processes to ensure data integrity, quality, and availability.; \u2022 Architect and manage cloud-based data infrastructure.; \u2022 Implement data governance, security, and compliance best practices.; \u2022 Monitor and troubleshoot data workflows to ensure reliability and performance.; \u2022 Drive continuous improvement in data engineering practices and mentor junior team members.; All About You; The ideal candidate for this position should:; \u2022 Have advanced knowledge of data engineering concepts, including data modelling, ETL\/ELT, and distributed systems.; \u2022 Be proficient in SQL and programming languages such as Python or Scala.; \u2022 Demonstrate hands-on experience with cloud platforms and modern data tools (e.g., Spark, Airflow, NiFi, Kafka).; \u2022 Be skilled at designing and optimizing large-scale data architectures and pipelines.; \u2022 Communicate complex technical concepts clearly to both technical and non-technical audiences.; Desirable capabilities:; \u2022 Experience working with big data technologies.; \u2022 Understanding of machine learning workflows and how to support them with data infrastructure.; \u2022 Passion for learning and fostering a culture of knowledge sharing.; ; Corporate Security Responsibility; ; All activities involving access to Mastercard assets, information, and networks comes with an inherent risk to the organization and, therefore, it is expected that every person working for, or on behalf of, Mastercard is responsible for information security and must:; Abide by Mastercard\u2019s security policies and practices;; Ensure the confidentiality and integrity of the information being accessed;; Report any suspected information security violation or breach, and; Complete all periodic mandatory security trainings in accordance with Mastercard\u2019s guidelines.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85575223","Role":"Data Engineer","Company":"Synpulse Singapore Pte. Ltd.","Location":"Singapore","Publish_Time":"2025-07-08 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85575223","job_desc":"We are an established, globally active management consulting company with offices in Switzerland, Germany, Austria, UK, USA, Singapore, Hong Kong, the Philippines, Australia, Indonesia and India. We are a valued partner to many of the world\u2018s largest international financial services and insurance firms. We support our clients at all project management stages from the development of strategies and operational frameworks to the technical implementation and handover. Our expertise in business and technology combined with our methodic approach enable us to create sustainable added value for our clients business.; About the job: ; Develop processes of the ingestion of data using various programming languages, techniques and tools from systems implemented using Oracle, Teradata, SAP, and Hadoop technology stack ; Evaluate and make decisions around dataset implementations designed and proposed by peer engineers ; Build large consumer database models for financial planning & analytics including Balance Sheet, Profit and Loss, Cost Analytics and Related Ratios ; Develop ETL, real time and batch data processes feeding into in-memory data infrastructure ; Perform and document data analysis, data validation, and data mapping\/design ; Work with clients to solve business problems in fraud, compliance and financial crime and present project results ; Use emerging and open-source technologies such as Spark, Hadoop, and Scala ; Collaborate on scalability issues involving access to massive amounts of data and information ; You should be comfortable with working with high profile clients on their sites ; About you:; Requirements; Bachelor's degree in computer science, Physics, Mathematics, or similar degree or equivalent ; Experience with open source big-data tools, such as Spark, Hadoop, and specially Scala ; 2 to 6 years of experience working in the Financial Services sector on big data project implementations ; Demonstrate strong analytical and problem-solving skills and the ability to debug and solve technical challenges with sometimes unfamiliar technologies ; Client facing experience, good communication and presentation skills ; Strong technical communication skills with demonstrable experience of working in rapidly changing client environments ; Quantexa Certification preferred ; Why us:; Flexible working hours with part-time working models and hybrid options; Attractive fringe benefits and salary structures in line with the market; Modern and central office space with good public transport connections; Can-do mentality and one-spirit culture; Varied events and employee initiatives; Your documents to start the process:; Resume; Job references; Qualifications (bachelor\/ master diploma, etc.) with certificate of grades; Motivation letter: Why Synpulse? Why you? Why this function?; Recommendation letters (optional); Do you approach your tasks with commitment and enjoyment and are you convinced that teamwork achieves better results than working alone? Are you proactive and willing to go the extra mile for your clients? Are you motivated not only to design solutions but also to implement them? As a flexible and goal-oriented person, you will quickly assume entrepreneurial responsibility with us.;  ; Do you appreciate the spirit of a growing international company with Swiss roots and a strong corporate culture? Then we look forward to receiving your online application at http:\/\/synpulse.com","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85577454","Role":"Big Data Developer","Company":"KG Sowers Group Pte Ltd","Location":"North Region","Publish_Time":"2025-07-08 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85577454","job_desc":"Roles and Responsibilites:; Developing and optimising ETL (Extract, Transform, Load) processes to ingest and transform large volumes of data from multiple sources.; Must have experience in investment banking, payment and transaction banking domains.; Developing and deploying data processing applications using Big Data frameworks such as Hadoop, Spark, Kafka, or similar technologies.; Proficiency in programming languages and scripting (e.g., Java, Scala, Python, SQL) for data processing and analysis.; Experience with cloud platforms and services for Big Data (e.g., AWS, Azure, Google Cloud); Requirements:; Primary Skills:; Designing, building, and maintaining systems that handle large volumes of data, enabling businesses to extract valuable insights and make data-driven decisions.; Creating scalable and efficient data pipelines, implementing data models, and integrating various data sources.; Developing and deploying data processing applications using Big Data frameworks such as Hadoop, Spark, Kafka; Write efficient and optimised code in programming languages like Java, Scala, Python to manipulate and analyse data; Creating scalable and efficient data pipelines, implementing data models, and integrating diverse data sources to enable businesses to extract valuable insights; Secondary Skills:; Designing, developing, and implementing scalable and efficient data processing pipelines using Big Data technologies.; Implementing a Kafka-based pipeline to feed event-driven data into a dynamic pricing model, enabling real-time pricing adjustments based on market conditions and customer; Conduct testing and validation of data pipelines and analytical solutions to ensure accuracy, reliability, and performance.; Strong experience in Spring Boot and microservices architecture.; Strong experience in distributed computing principles and Big Data ecosystem components (e.g., Hadoop, Spark, Hive, HBase).; More than 8 years of working experience in IT industry; More than 5 years of relevant experience","salary":"","work_type":"Kontrak\/Temporer","country":"singapore"}
{"Job_ID":"85823545","Role":"Data Engineer","Company":"Innowave Tech Pte Ltd","Location":"Paya Lebar East","Publish_Time":"2025-07-17 01:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85823545","job_desc":"About Innowave Tech Singapore ; Innowave Tech is an Artificial Intelligence (AI) company offering solutions for the Semiconductor and Advanced Manufacturing industry. Utilizing deep industrial domain knowledge, proven experience, and innovation, we provide expert AI solutions and systems to address various industry pain points. ;  Roles & Responsibilities ; We are seeking Data Engineer to establish and lead our data infrastructure. The successful candidate will be responsible for building our data engineering practice from the ground up, implementing robust data systems for industrial AI applications, and establishing best practices that will power our semiconductor manufacturing AI solutions. ;  Your Role and Impact ; As our first Data Engineer, you will have a foundational role in building robust data infrastructure to handle manufacturing data and LLM applications, while establishing secure data practices that power our AI solutions for advanced manufacturing operations. ;  What You\u2019ll Do ; Select and manage on-premises technologies suitable for secure and efficient operations. ; Build robust pipelines to collect, clean, and transform diverse datasets including process data, sensor data, image data, and human annotations. ; Ensure secure, maintainable, and scalable deployment of data infrastructure. ; Define and enforce best practices in data governance, privacy, and access control. ; Collaboration & Deployment. ;  What We\u2019re Looking For ; Educational Background: ; Minimum Poly or Bachelor Degree in Computer Science, Engineering, or a related field. ; * We welcome applications from Singapore Citizens, Permanent Residents (PRs), Malaysians, and local graduates bonded for local employment, in accordance with MoM regulations.;  Technical Expertise: ; 3+ years of experience in data engineering roles, ideally with on-premises or hybrid infrastructure. ; Proven track record of building scalable data systems from ground up in a startup environment. ; Proficiency in Python and\/or Java for data pipeline development. ; Solid experience with ETL frameworks (e.g., Apache Airflow, Dagster) and streaming systems (e.g., Kafka). ; Experience designing and maintaining SQL and NoSQL databases. ; Experience building and operating data lakes and data catalog. ; Familiarity with containerization (Docker), version control (Git), and CI\/CD practices. ;  Soft Skills: ; Excellent communication skills and ability to collaborate with cross-functional technical and non-technical teams. ; Excellent problem-solving and debugging abilities. ; Ability to balance engineering tradeoffs. ;  Bonus Skills: ; Experience with manufacturing data systems, especially SPC, SCADA, and industrial sensor protocols (e.g., OPC UA, MQTT, Modbus). ; Familiarity with AI\/ML pipelines and tools (e.g., MLflow). ; Knowledge in vector databases and LLM data infrastructure. ; Prior experience working in or with regulated industries (e.g., semiconductor, automotive, aerospace). ; What we Offer ; \u2022 A leading role in cutting-edge AI projects within the semiconductor industry. ; \u2022 The opportunity to work with an learn from experts in the field of AI and data science. ; \u2022 A dynamic, innovative, and supportive work environment. ; \u2022 Competitive salary and benefits package. ; \u2022 Career growth opportunities in a fast-paces technology company.","salary":"$5,333 \u2013 $8,000 per month (SGD)","work_type":"Full time","country":"singapore"}
{"Job_ID":"85833382","Role":"Data Engineer","Company":"Synapxe","Location":"Singapore","Publish_Time":"2025-07-16 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85833382","job_desc":"Work with stakeholders to understand needs for data structure, availability, scalability and accessibility; Support translation of data business needs into technical system requirements ; Identify opportunities for improvements and optimisation; Translate concepts into user flows, wireframes, mockups and prototypes that lead to intuitive user experiences.; Design and deliver wireframes, user stories, user journeys, and mockups optimized for a wide range of devices and interfaces.; Develop tools to improve data flows between internal\/external systems and the data lake\/warehouse ; Build robust and reproducible data ingest pipelines to collect, clean, harmonise, merge and consolidate data sources; Design and build API gateways to expose data to systems via secure means; Integrate and collate data sources with data systems, with compliance to data security and organisational governance standards; Implement critical data infrastructure on cloud including AWS EC2, S3, EMR, Redshift, Workspaces; Contribute to defining data retention policies","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85600186","Role":"Data Engineer Intern","Company":"Innowave Tech Pte Ltd","Location":"Paya Lebar","Publish_Time":"2025-07-09 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85600186","job_desc":"About Innowave Tech; Innowave Tech is an Artificial Intelligence (AI) company offering solutions for the Semiconductor and Advanced Manufacturing industry. Utilizing deep industrial domain knowledge, proven experience, and innovation, we provide expert AI solutions and systems to address various industry pain points.; Job Description; We are seeking a highly motivated and detail-oriented Data Engineer Intern to join our data team. In this role, you will assist in designing, building, and maintaining data pipelines and infrastructure that support data analytics and decision-making across the organization. This is an excellent opportunity to gain hands-on experience with real-world data engineering tools, workflows, and cloud platforms.; Key Responsibilities; \u00b7 Assist in developing and maintaining data pipelines using ETL\/ELT tools.; \u00b7 Support data integration from various internal and external sources into data warehouses.; \u00b7 Work with structured and unstructured data to transform it into usable formats.; \u00b7 Help ensure data quality, consistency, and availability across systems.; \u00b7 Collaborate with data scientists, analysts, and engineers to support data needs.; \u00b7 Document data workflows, schemas, and system processes.; \u00b7 Monitor data pipelines and resolve any issues or failures.; Requirements; \u00b7 Currently pursuing a degree in Computer Science, Data Science, Engineering, or a related field.; \u00b7 Basic knowledge of SQL and experience with a programming language such as Python, Java, or Scala.; \u00b7 Familiarity with relational databases (e.g., MySQL, PostgreSQL) and\/or data warehousing solutions (e.g., BigQuery, Redshift, Snowflake).; \u00b7 Experience in C# development.; \u00b7 Understanding of ETL concepts and data pipeline architecture.; \u00b7 Strong analytical thinking and problem-solving skills.; \u00b7 Willingness to learn and work in a team-oriented environment.; \u00b7 Internship duration should be at least 3 months full time.; \u00b7 Resume should indicate your forecasted internship dates.; Preferred Qualifications:; \u00b7 Exposure to cloud platforms like AWS, GCP, or Azure.; \u00b7 Experience with data processing frameworks (e.g., Apache Spark, Airflow).; \u00b7 Knowledge of version control tools (e.g., Git).","salary":"$800 \u2013 $1,000 per month (SGD)","work_type":"Kontrak\/Temporer","country":"singapore"}
{"Job_ID":"85308711","Role":"AI Data Engineer","Company":"InnoCellence Systems Pte Ltd","Location":"Central Region","Publish_Time":"2025-07-01 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85308711","job_desc":"We are looking for a skilled and experienced AI Data Engineer to join our team. The ideal candidate will be responsible for designing, building, and maintaining robust data pipelines to support the processing and analysis of clinical study and digital device sensor data. As a Data Engineer, you will work closely with data scientists and software engineers to ensure the efficient and reliable flow of data from source systems to analytical tools and platforms.; Responsibilities:; Design, develop, and maintain scalable data pipelines to ingest, transform, and load clinical study data from various sources, including digital device sensors.; Optimize data storage and retrieval processes in cloud-based platforms to ensure high performance and reliability.; Collaborate with data scientists to integrate data processing pipelines with AI-powered algorithms and third-party analytical tools or platforms.; Implement data quality checks and monitoring mechanisms to ensure the integrity and accuracy of the data.; Troubleshoot and resolve issues related to data pipeline performance, reliability, and scalability.; Work closely with software developers, system architects and other cross-functional teams to develop data-driven business solutions.; Stay up-to-date with emerging technologies in AI & Cloud computing and best practices in data engineering to continuously improve data processing pipelines and infrastructure.; Requirements:; Bachelor's or Master's degree in Computer Science, Engineering, or a related field.; Proven experience in designing and building data pipelines using ETL tools and frameworks such as Apache Spark, Apache Beam, or Apache Airflow.; Proficiency in programming languages such as Python, Java, or Scala.; Strong understanding of database systems, data warehousing concepts, SQL and NoSQL.; Experience with AI and Cloud Computing: Hands-on experience with cloud platforms like AWS and familiarity with AI solutions in these environments.; Excellent problem-solving and troubleshooting skills with a strong attention to detail and quality.; Effective communication and collaboration skills with the ability to work in a team environment.; Preferred Qualifications:; Experience with containerization and orchestration tools such as Docker and Kubernetes.; Familiarity with big data technologies such as Hadoop, Hive, or Presto.; Knowledge of distributed computing frameworks such as Apache Hadoop or Apache Spark.; Familiarity with ElasticSearch or AWS OpenSearch is plus; Prior experience working with healthcare or clinical data is a plus.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85702523","Role":"Senior Data Engineer","Company":"Singapore Telecommunications Limited","Location":"Singapore","Publish_Time":"2025-07-09 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85702523","job_desc":"An empowering career at Singtel begins with a Hello. Our purpose, to Empower Every Generation, connects people to the possibilities they need to excel. Every \"hello\" at Singtel opens doors to new initiatives, growth, and BIG possibilities that takes your career to new heights. So, when you say hello to us, you are really empowered to say\u2026\u201cHello BIG Possibilities\u201d.; Be a Part of Something BIG!  ; Design, build, and maintain scalable data pipelines and processing systems that power analytics and AI use cases across a hybrid data platform; Contribute to design conversation of AIDA\u2019s new data and AI platform; Collaborate with platform, analytics, and governance teams to deliver high-quality, secure, and well-documented data assets; Lead team of data engineers to ensure timely availability of accurate, well-documented data for use in AI use case; Make An Impact By; Design and implement batch and streaming data ingestion pipelines from diverse sources (e.g., files, APIs, Kafka, databases); Develop real-time and near-real-time data workflows using tools like Apache Flink, Kafka Streams; Optimize performance for high-volume and high-velocity datasets; Implement data quality checks and automatic monitoring system to ensure consistently accurate and available data; Design and manage storage solutions such as Microsoft Fabric, Delta Lake, and Databricks; Apply best practices for schema design, partitioning, and data lifecycle management; Support data discovery and cataloguing in coordination with governance tools; Work with data scientists, analysts, and business users to understand data needs and translate them into technical solutions; Partner with DevSecOps and platform engineers to automate deployment and orchestration of data pipelines; Document data flows, transformations, and quality checks in accordance with governance standard; Skills to Succeed; Bachelor\u2019s or Master\u2019s degree in Computer Science, Engineering, or a related field; 5 years of experience in data engineering; 1 year in a senior technical role delivering data engineering projects; Deep expertise in Spark, Databricks, and data processing frameworks; Strong knowledge of streaming technologies such as Apache Kafka, Apache Flink, or Azure Event Hub; Experience working with data lake and\/or lakehouse architectures such as Hadoop, Delta Lake, Iceberg and Microsoft OneLake; Proficient in Python and SQL; Familiar with workflow orchestration (e.g., Apache Airflow) and CI\/CD principles; Analytical mindset with a focus on data quality, performance, and maintainability; Able to work independently and collaboratively in a dynamic environment; Strong communication and documentation skills to support cross-functional collaboration; Knowledge of data types across network and IT in telco environment; Rewards that Go Beyond; Full suite of health and wellness benefits  ; Ongoing training and development programs  ; Internal mobility opportunities; Your Career Growth Starts Here. Apply Now!; We are committed to a safe and healthy environment for our employees & customers and will require all prospective employees to be fully vaccinated.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85273433","Role":"Data Engineer Lead","Company":"DCS Card Centre Pte. Ltd.","Location":"East Region","Publish_Time":"2025-06-30 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85273433","job_desc":"Key Responsibilities:; Lead the development of a financial-grade real-time data platform from 0 to 1 (covering data collection, storage, and computation layers), designing an architecture that supports daily processing of petabyte-scale transactional data.; Establish a cloud-native (AWS) data warehouse system integrated with Spark\/Flink for unified stream and batch processing, enabling millisecond-level metric computation for payment data.; Interface with the company\u2019s full-stack business (e.g., acquiring and issuing) and design data masking workflows and audit solutions that comply with PCI DSS.; Build and maintain data dashboards to support business teams in data-driven decision-making.; Integrate with payment systems to ensure seamless data flow and consolidation.; Responsible for data collection, processing, modeling, and structuring to improve data quality and usability.; Support cross-departmental data needs and promote data standardization and automation.; Work closely with teams such as risk control, compliance, and product to provide data support.; Requirements:; Over 5 years of experience in data engineering or data warehousing, with the capability to independently build data platforms.; Familiar with mainstream big data technology stacks, and proficient in performance tuning for Hadoop\/Spark\/Flink (e.g., shuffle optimization, checkpoint mechanism).; Skilled in data modeling (Dimensional\/Star Schema), ETL processes, and data governance.; Preferred background in payments, finance, or internet industries; familiarity with payment data structures is a plus.; Strong communication, collaboration, and business understanding skills, with the ability to work effectively with product, risk, and compliance teams.; Preferred Qualifications:; Experience in data-related roles within fintech or payment companies; Hands-on experience building data platforms or dashboards from scratch.; Familiarity with compliance-related data processing policies and workflows.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85816210","Role":"Software Data Engineer","Company":"Allegis Group Singapore Pte Ltd","Location":"Singapore","Publish_Time":"2025-07-16 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85816210","job_desc":"Design and implement optimal data structures and algorithms; Develop, maintain, and enhance data pipelines; Integrate applications with relational databases (e.g., Snowflake, Oracle, MS-SQL); We are seeking an experienced Data Engineer with at least 5 years of professional experience in software engineering or platform engineering.; The ideal candidate will possess strong expertise in designing efficient and scalable applications, working with relational databases, and leveraging modern cloud and big data technologies.; This role requires a combination of technical proficiency, attention to detail, and excellent communication skills to collaborate with data analysts, business users, and vendors in delivering robust data solutions.; Responsibilities:; Design and implement optimal data structures and algorithms to create efficient and scalable applications using Python.; Develop, maintain, and enhance data pipelines, ensuring high levels of data quality and reliability.; Integrate applications with relational databases (e.g., Snowflake, Oracle, MS-SQL) to support data processing and analytics.; Collaborate with stakeholders, including data analysts, business users, and vendors, to design and develop solutions that meet business requirements.; Employ best practices for code versioning, testing, Continuous Integration\/Continuous Deployment (CI\/CD), and code documentation.; \u2022Apply knowledge of data quality tools for profiling, cleansing, and monitoring data pipelines.; The Candidate:; A degree in Computer Science, Information Technology, or a related field.; 5+ years of experience as a software engineer.; Solid experience in Python, relational databases and SQL and Object-Oriented Programming (OOP) principles.; Strong analytical skills with a passion for solving complex problems through innovative solutions.; Excellent interpersonal and communication skills to interact effectively with diverse stakeholders.; A detail-oriented approach with a focus on operational excellence.; Preferred Qualifications; \u2022Experience with Snowflake, Oracle, and MS-SQL; \u2022Familiarity with cloud services such as AWS Glue, EKS, and S3; knowledge of Presto, Trino, AWS Athena, or similar tools.; \u2022Experience of Financial Services - if no experience, at least an interest in the products.; ; We regret to inform that only shortlisted candidates will be notifieid.; EA Registration No: R1984122, HILDER SEAN KIPLING, R1110424 Allegis Group Singapore Pte Ltd, Company Reg No. 200909448N, EA License No. 10C4544","salary":"","work_type":"Kontrak\/Temporer, Full time","country":"singapore"}
{"Job_ID":"85187447","Role":"Data Engineer","Company":"Goldtech Resources Pte Ltd","Location":"Singapore","Publish_Time":"2025-06-26 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85187447","job_desc":"Data Engineer (Full Time); Location In Singapore: Islandwide; Salary(SGD): 5000 - 7000; Apply; Job Description:; 1.Data Analysis and Insight Discovery: Conduct thorough analysis of structured and unstructured data sets to uncover trends, patterns, and valuable insights that inform business decisions and strategies.; 2.Data Visualization and Communication: Create user-friendly, visually engaging dashboards, reports, and presentations to convey complex data analyses to stakeholders, facilitating data-driven decision-making at all organizational levels.; 3.Data Modeling and Transformation: Plan, execute, and maintain data models, ETL pipelines, and data integration processes, ensuring data accuracy, integrity, and accessibility.; 4.Data Quality Assurance: Enforce rigorous data validation and cleansing procedures to guarantee the accuracy and dependability of data used for analysis and reporting.; 5.Identifying Data Requirements: Collaborate with stakeholders across different departments to comprehend their data needs and deliver insights aligned with their business objectives.; 6.Continuous Enhancement: Stay updated on industry trends and best practices in data analysis and engineering. Propose and implement process enhancements to improve data quality, efficiency, and overall performance.; 7.Data Security & Compliance: Establish, uphold, and oversee data security and privacy measures on the data platform.; 8.Cross-Functional Collaboration: Work closely with other teams to ensure the successful execution of projects and initiatives.; 9.Operational Support: Maintain and provide day-to-day operational assistance on the data platform, including:; -> Monitoring data pipelines, databases and analytical processes to ensure uninterrupted operation.; -> Promptly identify and resolve data-related issues, performance bottlenecks, and data quality concerns.; Requirements:; \u00b7     Bachelor's degree in Computer Science, Data Science, Statistics, or a closely related field.; \u00b7    Have a minimum of 5 years of practical experience in data platforms, data analytics, or related projects.; \u00b7 Data Analysis Proficiency: Demonstrate a proven track record as a Data Analyst, showcasing the ability to translate intricate data sets into actionable insights.; \u00b7 Data Visualization Skills: Exhibit strong expertise in data visualization tools like Tableau, Power BI, or similar, enabling the creation of compelling data visualizations and reports.; \u00b7  Programming Aptitude: Display proficient programming skills in languages such as Python, SQL, SAS, or R for data analysis and data engineering tasks.; \u00b7 Data Engineering Experience: Prior experience as a Data Engineer, with a demonstrated capability to design and implement data models, ETL processes, and data integration solutions.; \u00b7 Technology Familiarity: Be acquainted with cloud-based data platforms like Azure Data Factory, Azure Purview, Databricks, Snowflake, Kafka, or equivalent, as well as data storage technologies such as SQL and NoSQL databases.; \u00b7  Having prior experience with Microsoft SQL Server, Microsoft SSIS, or Ali Cloud (Dataphin) is considered a plus.; \u00b7  Problem-Solving Acumen: Possess excellent problem-solving skills, particularly in dealing with large and complex data sets.; \u00b7 Attention to Detail: Maintain a strong commitment to detail, ensuring data accuracy and reliability in all analyses.; \u00b7 Effective Communication: Demonstrate effective communication skills to convey complex technical concepts to non-technical stakeholders.; \u00b7  Collaborative Team Player: Prove the ability to work independently as well as collaboratively in a fast-paced, team-oriented environment.; \u00b7  Certifications: Hold certifications in Tableau Data Analyst, Microsoft Power BI Data Analyst, SnowPro (Snowflake), Databricks, or their equivalents.; Apply","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85578382","Role":"DATA ENGINEER","Company":"Continental Technology Solutions Pte Ltd","Location":"Kampong Ubi","Publish_Time":"2025-07-07 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85578382","job_desc":"Responsibilities:; Design, implement, and maintain scalable and reliable data pipelines.; Build and optimize data architectures (data lakes, data warehouses, ETL\/ELT processes).; Collaborate with data scientists, analysts, and other stakeholders to understand data needs.; Develop data models and structures for reporting and analytics.; Ensure data quality, integrity, security, and compliance with governance policies.; Monitor and troubleshoot performance issues with data systems.; Automate data workflows using modern orchestration tools.; Document processes, systems, and data flows.; Requirements:; Bachelor's degree in Computer Science, Engineering, or a related field.; 2+ years of experience in a data engineering or similar role.; Strong SQL skills and experience with relational databases (e.g., PostgreSQL, MySQL).; Proficiency in Python, Scala, or Java for data manipulation and pipeline development.; Experience with ETL\/ELT tools (e.g., Apache Airflow, dbt, Talend).; Familiarity with data warehouse solutions (e.g., Snowflake, Redshift, BigQuery).; Experience with cloud platforms (AWS, GCP, or Azure) and their data services.; Understanding of data modeling, data governance, and best practices.","salary":"","work_type":"Kontrak\/Temporer","country":"singapore"}
{"Job_ID":"85783746","Role":"Data Engineer (Informatica\/Python)","Company":"ASTEK Singapore Innovation Technology Pte. Ltd.","Location":"Central Region","Publish_Time":"2025-07-15 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85783746","job_desc":"Job Title: Data Engineer - Mid-level\/Senior; Location: Singapore (CBD); Job Type: Full-Time Permanent; Key Responsibilities:; Design and maintain scalable ETL pipelines using Informatica.; Optimize SQL queries and stored procedures for data manipulation.; Lead data migration projects ensuring data integrity and quality.; Collaborate with business teams to create data solutions.; Integrate data into .NET applications.; Qualifications (MUST HAVE):; Min 4 years of working experience in data engineering.; Proficient in Informatica.; Strong knowledge of Python.; Solid SQL skills, including experience with Stored Procedures.; Proven experience in data migration and engineering.; Experience with .Net is highly advantageous.; *Apply only if you meet all the must-have requirements.*","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85813539","Role":"Data Engineer - TEKsystems (Allegis Group Singapore Pte Ltd)","Company":"Allegis Group Singapore Pte Ltd","Location":"Singapore","Publish_Time":"2025-07-16 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85813539","job_desc":"Design and implement optimal data structures and algorithms; Develop, maintain, and enhance data pipelines; Integrate applications with relational databases (e.g., Snowflake, Oracle, MS-SQL); We are seeking an experienced Data Engineer with at least 5 years of professional experience in software engineering or platform engineering.; The ideal candidate will possess strong expertise in designing efficient and scalable applications, working with relational databases, and leveraging modern cloud and big data technologies.; This role requires a combination of technical proficiency, attention to detail, and excellent communication skills to collaborate with data analysts, business users, and vendors in delivering robust data solutions.; Responsibilities:; Design and implement optimal data structures and algorithms to create efficient and scalable applications using Python.; Develop, maintain, and enhance data pipelines, ensuring high levels of data quality and reliability.; Integrate applications with relational databases (e.g., Snowflake, Oracle, MS-SQL) to support data processing and analytics.; Collaborate with stakeholders, including data analysts, business users, and vendors, to design and develop solutions that meet business requirements.; Employ best practices for code versioning, testing, Continuous Integration\/Continuous Deployment (CI\/CD), and code documentation.; \u2022Apply knowledge of data quality tools for profiling, cleansing, and monitoring data pipelines.; The Candidate:; A degree in Computer Science, Information Technology, or a related field.; 5+ years of experience as a software engineer.; Solid experience in Python, relational databases and SQL and Object-Oriented Programming (OOP) principles.; Strong analytical skills with a passion for solving complex problems through innovative solutions.; Excellent interpersonal and communication skills to interact effectively with diverse stakeholders.; A detail-oriented approach with a focus on operational excellence.; Preferred Qualifications; \u2022Experience with Snowflake, Oracle, and MS-SQL; \u2022Familiarity with cloud services such as AWS Glue, EKS, and S3; knowledge of Presto, Trino, AWS Athena, or similar tools.; \u2022Experience of Financial Services - if no experience, at least an interest in the products.; ; We regret to inform that only shortlisted candidates will be notifieid.; EA Registration No: R1984122, HILDER SEAN KIPLING, R1110424 Allegis Group Singapore Pte Ltd, Company Reg No. 200909448N, EA License No. 10C4544; information_technology","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85840488","Role":"Sr. Data Engineer","Company":"VISA WORLDWIDE PTE. LIMITED","Location":"Singapore","Publish_Time":"2025-07-17 09:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85840488","job_desc":"Company Description; Visa is a world leader in payments and technology, with over 259 billion payments transactions flowing safely between consumers, merchants, financial institutions, and government entities in more than 200 countries and territories each year. Our mission is to connect the world through the most innovative, convenient, reliable, and secure payments network, enabling individuals, businesses, and economies to thrive while driven by a common purpose \u2013 to uplift everyone, everywhere by being the best way to pay and be paid.; Make an impact with a purpose-driven industry leader. Join us today and experience Life at Visa.; Job Description; Visa\u2019s Technology Organization is a community of problem solvers and innovators reshaping the future of commerce. We operate the world\u2019s most sophisticated processing networks, capable of handling more than 65k secure transactions a second across 80M merchants, 15k Financial Institutions, and billions of everyday people. You\u2019ll work on complex distributed systems and solve massive scale problems centered on new payment flows, business and data solutions, cybersecurity, and B2C platforms.; In addition, Value Added Services (VAS) - VAS Digital Marketing is a key growth strategy for Visa globally, aimed at diversifying Visa\u2019s revenue with products and solutions that differentiate its network and deliver valuable solutions across other networks.; ; The Opportunity:; We are developing and executing a shared strategic vision for Digital Marketing platforms and products that enable Visa to be the world-leading data-driven payments company. As a Senior Data Engineer, you will be part of a world-class team of Engineers to define, drive and execute on this vision. We are looking for a self-motivated, versatile and energetic individual with software engineering skills and expertise with Java, Big data & Web technologies, who embraces solving complex challenges on a global scale. The candidate will be extensively involved in hands-on activities including POCs, design, development, testing, and managing applications globally used by Visa cardholders. Candidate must be flexible and willing to switch tasks based on team's needs.; ; You will use your Java skills and experience with various technologies to design, develop, test, and deploy high-quality code that meets stringent business, security, and resiliency requirements. You will collaborate with other teams, vendors, and stakeholders to ensure the smooth delivery and operation of the application. You will have the opportunity to learn and apply new technologies and frameworks, such as AI and generative AI, to enhance the functionality and performance of the application.; Primary responsibilities will include:; Design, develop, test, document, and implement new applications and enhance existing systems to ensure high performance and reliability.; Write secure, maintainable, and efficient code that adheres to Java\/J2EE best practices, organizational and security standards.; Create and maintain comprehensive technical documentation, including design changes and architectural decisions, using Wiki or similar tools.; Participate in code and design review sessions to ensure high-quality deliverables and adherence to development standards.; Collaborate with architects, product owners, and technical stakeholders to deliver products that meet business requirements and leverage modern technologies.; Identify and recommend opportunities for process improvements, enhancements, and adoption of best practices within the development team.; Mentor and support junior developers, fostering knowledge sharing and contributing to the development of departmental procedures and standards.; Coordinate and contribute to Continuous Integration (CI) activities and the implementation of automated testing frameworks.; Develop proof-of-concepts (POCs) and prototypes to validate ideas and quickly iterate new features or enhancements.; Communicate technical solutions, project status, issues, and risks effectively to both technical and non-technical stakeholders.; Ensure the delivery of high-quality, defect-free code and take accountability for meeting project timelines and quality standards.; This is a hybrid position. Expectation of days in office will be confirmed by your Hiring Manager.; Qualifications; Preferred Qualifications; \u20223 or more years of work experience with a Bachelor\u2019s Degree or more than 2 years of work experience with an Advanced Degree (e.g. Masters, MBA, JD, MD); \u20224\u20137 years of relevant experience in Java\/J2EE enterprise applications.; \u2022Strong skills in Core Java, J2EE, Spring Framework, Spring Boot, Hibernate, and Web services.; \u2022Proficiency in object-oriented design and software design principles.; \u2022Experience with secure coding practices.; \u2022Strong SQL skills with experience in relational (MySQL, PostgreSQL) and NoSQL (MongoDB) databases.; \u2022Understanding of data warehousing concepts and tools.; \u2022Exposure to data engineering frameworks such as Apache Spark, Hadoop, or Kafka is an advantage.; \u2022Basic understanding of ETL processes and data pipeline development.; \u2022Hands-on experience with containerization and orchestration tools (Docker, Kubernetes).; \u2022Proficiency in version control systems (Git\/Stash), build tools (Maven), and CI\/CD tools (Jenkins).; \u2022Familiarity with Unix\/Linux operating systems and shell scripting.; \u2022Experience with UI frameworks and frontend development using Angular or React, Next.js, JavaScript, HTML, and CSS.; \u2022AI and generative AI skills are highly desirable.; \u2022Experience working in all phases of the software development life cycle.; \u2022Experience with Agile methodologies (Scrum, sprints) and tools (Jira).; \u2022Understanding of DevOps practices.; \u2022Solid foundation in computer science, including data structures and algorithms.; \u2022Willingness to learn and improve coding skills, especially in Java or Scala.; Additional Information:; Skills\/Abilities; \u2022Strong analytical and problem-solving abilities.; \u2022Quick to learn and adapt to new technologies and challenges.; \u2022Excellent organizational skills with the ability to manage multiple tasks and deadlines in a fast-paced environment.; \u2022Outstanding written and verbal communication skills for conveying ideas and implementation plans to team members and stakeholders.; \u2022Highly detail-oriented, resourceful, and results-driven.; \u2022Self-motivated with a demonstrated ability to work independently and meet commitments.; \u2022Comfortable collaborating in dynamic, fast-paced, and highly interactive team settings.; \u2022Eager to learn new skills, embrace new initiatives, and contribute to team success.; \u2022Proven ability to maintain a positive attitude and have fun while working as part of a team.; Additional Information; Visa is an EEO Employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability or protected veteran status. Visa will also consider for employment qualified applicants with criminal histories in a manner consistent with EEOC guidelines and applicable local law.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85688508","Role":"Cloud Data Engineer","Company":"Unison Consulting Pte. Ltd.","Location":"Singapore","Publish_Time":"2025-07-11 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85688508","job_desc":"Design and architect data storage solutions, including databases, data lakes, and warehouses, using AWS services such as Amazon S3, Amazon RDS, Amazon Redshift, and Amazon DynamoDB, along with Databricks' Delta Lake. Integrate Informatica IDMC for metadata management and data cataloging.; Create, manage, and optimize data pipelines for ingesting, processing, and transforming data using AWS services like AWS Glue, AWS Data Pipeline, and AWS Lambda, Databricks for advanced data processing, and Informatica IDMC for data integration and quality.; Integrate data from various sources, both internal and external, into AWS and Databricks environments, ensuring data consistency and quality, while leveraging Informatica IDMC for data integration, transformation, and governance.; Develop ETL (Extract, Transform, Load) processes to cleanse, transform, and enrich data, making it suitable for analytical purposes using Databricks' Spark capabilities and Informatica IDMC for data transformation and quality.; Monitor and optimize data processing and query performance in both AWS and Databricks environments, making necessary adjustments to meet performance and scalability requirements. Utilize Informatica IDMC for optimizing data workflows.; Requirements; Good experience in data engineering, with expertise in AWS services, Databricks, and\/or Informatica IDMC.; Proficiency in programming languages such as Python, Java, or Scala for building data pipelines.; Evaluate potential technical solutions and make recommendations to resolve data issues especially on performance assessment for complex data transformations and long running data processes.; Strong knowledge of SQL and NoSQL databases.; Familiarity with data modeling and schema design.; AWS certifications (e.g., AWS Certified Data Analytics - Specialty, AWS Certified Data Analytics - Specialty), Databricks certifications, and Informatica certifications are a plus.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85816706","Role":"Associate Data Engineer - ETL (Engineering & Ops)","Company":"Synapxe","Location":"Singapore","Publish_Time":"2025-07-16 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85816706","job_desc":"Bachelor's degree in Computer Science, Information Technology, or a related field; At least 4 years of experience in the IT industry, including:; a. Development, implementation, and maintenance of IT systems, preferably in Data Warehousing, ETL rules, data  modeling, and BI applications; b. Operations support and business analysis experience.; c. Strong MS-SQL and Oracle Database scriptin; Experience in diagnosing, troubleshooting, and performing root cause analysis; Ability to diagnose and troubleshoot problems with BI reports and ETL processes; Experience with AWS, Data Lake, Databricks, and the healthcare domain is a plus; Able to work independently and as an effective team player with a strong desire to deliver results; Adaptable, meticulous, and possess strong analytical skills; Good communication skills (both written and spoken); Strong team player","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85578684","Role":"DATA ENGINEER","Company":"Continental Technology Solutions Pte Ltd","Location":"Kampong Ubi","Publish_Time":"2025-07-07 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85578684","job_desc":"Roles&Responsibilities:; Design, build, and maintain scalable ETL\/ELT pipelines and data workflows.; Develop and manage job schedules using Control-M to automate complex data workflows.; Collaborate with data analysts, data scientists, and business users to understand data requirements.; Ensure reliable data ingestion from internal and external sources into data lakes and data warehouses.; Monitor job execution, troubleshoot failures, and optimize system performance.; Ensure data quality, consistency, and security across systems.; Document data flows, Control-M job configurations, and operational procedures.; Participate in on-call rotations and incident response as needed.; Requirements:; Bachelor\u2019s degree in Computer Science, Engineering, Information Systems, or related field.; 3+ years of experience in a data engineering or data integration role.; Strong hands-on experience with Control-M (job scheduling, development, troubleshooting, and automation).; Proficiency in SQL and scripting languages (Python, Shell, or PowerShell).; Solid understanding of ETL processes, data pipelines, and workflow orchestration.; Experience with relational databases (e.g., Oracle, SQL Server, PostgreSQL).; Familiarity with data warehouse platforms (e.g., Snowflake, Redshift, BigQuery).; Experience with cloud platforms (AWS, GCP, or Azure) and their data services.; Knowledge of CI\/CD practices and version control systems (Git).","salary":"","work_type":"Kontrak\/Temporer","country":"singapore"}
{"Job_ID":"85232560","Role":"Reporting and Analytics Developer\/Data Engineer 0642B","Company":"USER EXPERIENCE RESEARCHERS PTE. LTD","Location":"Singapore","Publish_Time":"2025-06-27 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85232560","job_desc":"We are seeking a highly skilled and experienced Big Data Engineer to join our team. The ideal candidate will have a minimum of 4 years of experience managing data engineering jobs in big data environment e.g., Cloudera Data Platform. The successful candidate will be responsible for designing, developing, and maintaining the data ingestion and processing jobs. Candidate will also be integrating data sets to provide seamless data access to users.; Responsibilities; Analyse the Authority's data needs and document the requirements.; Refine data collection\/consumption by migrating data collection to more efficient channels.; Plan, design and implement data engineering jobs and reporting solutions to meet the analytical needs.; Develop test plan and scripts for system testing, support user acceptance testing.; Build reports and dashboards according to user requirements; Work with the Authority's technical teams to ensure smooth deployment and adoption of new solution.; Ensure the smooth operations and service level of IT solutions.; Support production issues.; What we are looking for:; Good understanding and completion of projects using waterfall\/Agile methodology.; Strong SQL, data modelling and data analysis skills are a must.; Hands-on experience in big data engineering jobs using Python, Pyspark, Linux, and ETL tools like Informatica.; Hands-on experience in a reporting or visualization tool like SAP BO and Tableau is must.; Hands-on experience in DevOps deployment and data virtualisation tools like Denodo will be an advantage.; Track record in implementing systems using Hive, Impala and Cloudera Data Platform will be preferred.; Good understanding of analytics and data warehouse implementations.; Ability to troubleshoot complex issues ranging from system resource to application stack traces.; Track record in implementing systems with high availability, high performance, high security hosted at various data centres or hybrid cloud environments will be an added advantage.; Passion for automation, standardization, and best practices.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85608678","Role":"Data Engineer","Company":"TROYTECH","Location":"Central Region","Publish_Time":"2025-07-09 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85608678","job_desc":"Requirements for Data Engineer:; Responsibilities:; Working with a back-office development team, you will be responsible for the design and delivery of the software solution for a high volume and performance intensive back office of an Automatic Fare Collection (AFC) system.; The activities required to be performed shall include the following:; Requirements gathering;; Design, development, and maintenance of the software;; Preparation and submission of deliverables throughout the software development lifecycles such as Business Rules, Software Requirement Specifications, Software Architecture Document, Design Specification, Interface Specifications, Source Codes, Testing Specification, Operation and Maintenance Manual, Training Manual and others documents as required in the standard adopted by the Authority;; Assist the Authority with day-to-day application support issues; and; Liaise with users on the requirement, design and testing issues.; Qualifications & Requirements:; Recognised degree in Computer Science, Data Analytics \/ Science, IT or engineering with relevant experience in software development;; Gone through at least one full software development lifecycle (SDLC) with experience in producing SDLC documentations;; Experience in analysing requirements, designing, development and delivering complex enterprise applications in Java, web programming on UNIX \/ LINUX and Windows environment;; Extensive experience in web-based applications development and database application interfacing (JDBC, SQL);; Experience in Agile development methodology;; Experience in DevSecOps and CI\/CD;; Experience in testing (unit, integration, and end-to-end) to ensure system stability and reliability;; Experience in Automatic Fare Collection (AFC) system will be an added advantage;; Experience in EMV and Payment Card Industry Data Security Standard (PCI DSS) will be an added advantage;; Design and documentation such as UML modelling;; Strong analytical and problem-solving skills; and; Good written and verbal communication skills in English.; Advanced knowledge of SQL and star schema design, and Snowflake;; Experience in Data Analytics techniques such as clustering, timeseries analysis and anomaly detection;; Experience in creating Python programmes for data transformation and analysis, dataframe programming using Pandas and Snowpark;; Experience in developing in AWS environment, and familiarity with AWS Sagemaker will be an added advantage;; Experience in dashboarding libraries such as Streamlit or Tableau will be an added advantage;; Experience in creating Java API microservices with Spring Boot will be added advantage.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"84453257","Role":"Data Engineer","Company":"LYNX ANALYTICS PTE. LTD.","Location":"Singapore","Publish_Time":"2025-06-17 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/84453257","job_desc":"The Data Engineer works within a cross-functional project team and is responsible for automating and productizing advanced analytics pipelines. S\/he will work with a variety of technologies, including Generative AI, and our proprietary big graph analysis framework.; WHAT YOU\u2019LL DO; A Data Engineer\u2019s responsibility is to implement and deploy data analysis pipelines at various clients of Lynx Analytics. This includes participating in:; Understanding deeply the business problem that we are trying to solve by our analytical solutions; Through continuous consultations with employees of our client, discover the client\u2019s existing data sources that are relevant to the problem we try to solve. This includes discussions with client\u2019s IT teams, data owners, future business owners etc.; Working together with the client\u2019s IT teams to define the technical architecture for the analytical solution that we are to deploy; Implement the data ingestion subsystem: This is the system responsible for moving all the necessary data sources to a single location where the actual analysis will happen; Implement the data analysis pipelines; Integrate the results into business UIs developed by Lynx or pre-existing client software systems; SKILLS AND EXPERIENCE; Skills You Should Have:; Python (pytest, pre-commit, venv, etc.); Airflow, dbt, or other orchestration tool; Apache Spark, PostgreSQL, BigQuery, or other SQL\/NoSQL engines; Docker, Docker Compose, Terraform or other IaC tools; Linux OS; One of major cloud platforms like GCP, AWS, or Azure; What You Might Also Work With:; CI\/CD tools like GitHub Actions; FastAPI or Flask for service endpoints; Network setup (DNS, TLS, SSH, IP routes, etc.); You Might Be a Fit If You:; Are a generalist who thrives in ambiguity and loves figuring things out.; Enjoy breaking down big, vague, and unfamiliar problems into concrete actions.; Value pragmatism over perfection: you look for MVPs, iterative delivery, and quick feedback loops.; View that DevOps isn\u2019t someone else\u2019s job\u2014you\u2019re comfortable setting up pipelines, fixing flaky builds, or tweaking a reverse proxy config.; Are comfortable switching hats quickly\u2014from setting up data pipelines and crawling websites to tuning SQL queries or debugging backend servers.; Are passionate about learning and applying unfamiliar technologies and tools.; Have some prior experience in Generative AI (ideally RAG) and data science \/ analytics.; Get excited about opportunities to travel and work abroad!; Why You\u2019ll Love It Here:; High ownership, zero micromanagement; Rapid learning opportunities and diverse challenges; Flexible work hours, remote-friendly setup; A collaborative, culture that values real outcomes; Flat organisational hierarchy with high visibility and accessibility to our leaders; P.S. Mention Tech in Asia Jobs when you apply! Helps keep the good stuff coming \ud83d\ude09","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85018947","Role":"Data Engineer-Consultant","Company":"WSH Experts Pte Ltd","Location":"Singapore","Publish_Time":"2025-06-18 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85018947","job_desc":"Job Description; Create and manage a single master record for each business entity, ensuring data consistency, accuracy, and reliability.; Implement data governance processes, including data quality management, data profiling, data remediation, and automated data lineage.; Create and maintain multiple robust and high-performance data processing pipelines within Cloud, Private Data Centre, and Hybrid data ecosystems.; Assemble large, complex data sets from a wide variety of data sources.; Collaborate with Data Scientists, Machine Learning Engineers, Business Analysts, and Business users to derive actionable insights and reliable foresights into customer acquisition, operational efficiency, and other key business performance metrics.; Develop, deploy, and maintain multiple microservices, REST APIs, and reporting services.; Design and implement internal processes to automate manual workflows, optimize data delivery, and re-design infrastructure for greater scalability.; Establish expertise in designing, analyzing, and troubleshooting large-scale distributed systems.; Support and work with cross-functional teams in a dynamic environment.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85043774","Role":"Data Engineer","Company":"Tabernacle Pte Ltd","Location":"Central Region","Publish_Time":"2025-06-19 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85043774","job_desc":"Role Overview:; We are looking for a skilled Remote Fabric Data Engineer to support our ongoing data platform migration and transformation efforts. This role focuses on migrating existing Databricks workloads into Microsoft Fabric, implementing efficient transformation logic, and ensuring complete lineage documentation and performance optimization.; Key Responsibilities:; Migrate existing Databricks Silver\/Gold layer SQL transformations into Microsoft Fabric SQL and Dataflows.; Design and implement transformation logic, including deduplication, joins, field mappings, and business rules in Fabric Dataflows.; Develop dimensional data models with surrogate key generation, row-level security (RLS) attributes, and role-based filtering.; Build and maintain metadata lineage pipelines to track data flow across the lifecycle \u2014 from source systems through transformation to target layers.; Tune performance of Fabric solutions by applying partitioning strategies, indexing, and aggregations where applicable.; Produce clear, concise documentation including:; Dataflow structures and logic; Dimensional model definitions; End-to-end lineage path summaries for audit and traceability; Qualifications:; Diploma or Degree in Information Technology, Computer Science, or related field.; Any other related certifications;  Experience and Skills:; Strong experience in Microsoft Fabric (Dataflows, Pipelines, Fabric SQL, and Lakehouses).; Prior hands-on experience migrating Databricks SQL logic or similar workloads into Microsoft environments.; Solid understanding of data modeling, data warehousing, and ETL\/ELT transformations.; Proficiency in performance tuning and working with large-scale data systems using partitioning and indexing.; Experience with metadata management, data lineage, and documentation practices.; Comfortable working in a fully remote setting with distributed teams.; Excellent written communication skills to produce technical documentation and collaborate asynchronously.","salary":"","work_type":"Kontrak\/Temporer","country":"singapore"}
{"Job_ID":"85751373","Role":"Snowflake Data Engineer","Company":"Blue Ocean Systems Infotech Lte Ltd","Location":"Central Region","Publish_Time":"2025-07-14 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85751373","job_desc":"Hi Immediate hiring; Job Overview:; We are seeking a highly skilled Snowflake Data Engineer to join our data engineering team. The ideal candidate will have hands-on experience with Snowflake, strong SQL skills, and a solid understanding of cloud-based data warehousing, ETL processes, and data modeling. The role will focus on designing, developing, and optimizing scalable data pipelines using Snowflake and other modern data stack tools.; Key Responsibilities:; Design and implement scalable and efficient data pipelines using Snowflake.; Develop and maintain ETL\/ELT workflows to ingest, transform, and deliver clean data.; Optimize Snowflake performance (query tuning, resource management, clustering).; Collaborate with data analysts, data scientists, and business stakeholders to gather requirements and deliver robust data solutions.; Implement data governance, security, and access control best practices within Snowflake.; Monitor and troubleshoot data pipeline issues and performance bottlenecks.; Create and maintain documentation related to data processes and architecture.; Required Skills & Qualifications:; 5+ years of experience in data engineering or a similar role.; 2+ years of hands-on experience with Snowflake (SnowSQL, Streams, Tasks, Cloning, etc.).; Proficient in SQL, with deep understanding of analytical and complex queries.; Experience with ETL\/ELT tools such as dbt, Informatica, Talend, Matillion, or Apache Airflow.; Strong knowledge of data modeling concepts (Star\/Snowflake schemas).; Experience with cloud platforms (AWS, Azure, or GCP) and integrating Snowflake with cloud storage (S3, Blob, GCS).; Familiarity with programming languages like Python or Scala for scripting and automation.; Strong problem-solving skills and attention to detail.; Preferred Qualifications:; Snowflake certification (SnowPro Core or Advanced Architect) is a plus.; Experience with CI\/CD, version control (Git), and DevOps practices.; Familiarity with BI tools like Power BI, Tableau, or Looker.; Regards; Kshama; +91 9833964181; kshama.raj@blueocean.systems","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"83482854","Role":"Data Engineer","Company":"SMRT Corporation Ltd","Location":"Singapore","Publish_Time":"2025-06-17 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/83482854","job_desc":"The duties and responsibilities for Data Engineer, are as listed below. The list is not comprehensive and related duties and responsibilities may be assigned from time to time.; Data Engineering & Processing:; Develop and maintain data pipelines for efficient data ingestion and transformation.; Work with structured and unstructured data to ensure optimal storage and retrieval.; Perform data analysis and report on results.; Database Design & Management:; Design and implement relational and NoSQL database schemas for scalability.; Optimize database performance through indexing, partitioning, and query tuning.; Implement data security and compliance best practices.; API Development & Backend Engineering:; Design and develop APIs for data access and application integration.; Implement authentication, authorization, and API security best practices.; Cloud Infrastructure & Deployment (Supporting Role):; Assist in design Azure cloud architectures; Work with IT infrastructure team to set up cloud infrastructure for application hosting, data storage and processing. ; Collaboration & Best Practices:; Collaborate with internal stakeholders to understand their business needs.; Work with software engineers, data scientist, frontend developer to understand the data requirement and design architecture of the data platform.; Implement CI\/CD pipelines for automated testing, deployment and monitoring.; Write testable and maintainable code and documentation to deploy to production.; Engage continuously with end-user for feedback and improvements.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85706732","Role":"Senior Data Engineer, AI\/Machine Learning Lab","Company":"Changi Airport Group (Singapore) Pte. Ltd.","Location":"Changi","Publish_Time":"2025-06-17 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85706732","job_desc":"About the job; As a Machine Learning Data Engineer at CAG, you will be responsible for designing, implementing, and maintaining the data pipelines and infrastructure that support our machine learning projects. You will work closely with data scientists, machine learning engineers, cloud engineer and other cross-functional teams to ensure the availability, reliability, and performance of our data systems. Your role will be critical in enabling the development and deployment of advanced machine learning models that drive key business insights and innovations.; This role requires a blend of technical expertise, project management skills, and the ability to deliver robust, scalable data solutions in a fast-paced environment.;   Responsibilities:; Architect and implement scalable data solutions to address complex business challenges, leveraging advanced analytics, statistical methods, and machine learning techniques. Apply advanced data preprocessing, transformation, and enrichment techniques to ensure high-quality inputs for machine learning models.; Partner with data scientists and ML engineers to translate data requirements into actionable insights, optimizing feature engineering processes and model deployment strategies.; Construct and manage modern data infrastructure, including data warehouses and data lakes, to facilitate seamless data access for analysis and model training.; Continuously optimize data pipelines for performance, scalability, and cost-effectiveness, considering factors such as data volume, processing speed, and resource utilization.; Collaborate closely with DevOps and IT teams to ensure smooth deployment, monitoring, and maintenance of data pipelines in production environments.; Work cross-functionally to ensure adherence to data governance, security, and privacy regulations.; Stay at the forefront of data engineering and machine learning advancements, driving the adoption of best practices within the team.; Mentor junior team members and contribute to the overall data strategy of the organization.; Requirements:; Bachelor's or Master's degree in Computer Science, Engineering, or a related field with at least 5 years\u2019 work experience; Extensive hands-on experience with cloud platforms, particularly AWS and GCP, including their data services and analytics offerings.; Strong coding skills with proficiency in:; Infrastructure as Code (e.g., Terraform, CloudFormation); Shell scripting; Python; SQL; Deep understanding of big data technologies, distributed computing, and modern data architecture patterns. Proven track record in designing and implementing large-scale data solutions, including data pipelines, data warehouses, and data lakes.; Demonstrated ability to successfully deliver projects, meet milestones, and drive initiatives from conception to production.; Experience with data streaming technologies (e.g., Kafka, Kinesis) and batch processing frameworks (e.g., Spark, Hadoop).; Familiarity with containerization and orchestration technologies (e.g., Docker, Kubernetes).; Strong problem-solving skills and ability to translate complex business requirements into technical solutions.; Excellent communication skills with the ability to collaborate effectively across cross-functional teams.","salary":"","work_type":"","country":"singapore"}
{"Job_ID":"85156590","Role":"Senior Data Engineer (Networks)","Company":"Singapore Telecommunications Limited","Location":"Singapore","Publish_Time":"2025-06-18 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85156590","job_desc":"At Singtel, we believe in the strength of a vibrant, diverse and inclusive workforce where backgrounds, perspectives and life experiences of our people help us innovate and create strong connections with our customers. We strive to ensure all our people practices are non-discriminatory and provide a fair, performance-based work culture that is diverse, inclusive and collaborative. ; Join us and experience what it\u2019s like to be with an Employer of Choice*. Together, let\u2019s create a brighter digital future for all. *Awarded at the HR Fest Awards 2020.; Make an Impact by:; Develop new and improve existing data pipeline on Big Data platform (Hadoop, mapR or equivalent).; Build new and enhance existing data application for streaming and batch datasets.; Work on Apache Airflow on data pipeline, Python, Spark, PySpark, Scala, Java, SQL, etc for data application open sources technologies.; Responsible in developing frontend dashboards and integration to back end. Using frontend and backend API Frameworks such as React, Angular Django, FastAPI, Springboot.; Perform R&D and conduct PoC (proof-of-concept) for new data solution.; Perform metadata and data management, data security.; Drive optimization, testing and tooling to improve data quality & efficiency in data lake and streaming platform.; Lead and monitor the performance of Junior Data Engineer and providing them with practical guidance, solution validation and implementation.; Lead and manage DevOps, DataOps and Streaming Operations (or equivalent) in Data Engineering.; Design high level & detailed design to ensure that the solution delivers to the business needs and align to the data & analytics architecture principles and roadmap.; Collaborate with different stakeholders from business, technical, project management and operation to design and implement the solution.; Ability to lead troubleshooting efforts for complex design and eliminate application issue faced by the project and operation team.; Skills for Success:; Bachelor degree in Computer Science, Math, Data Analytics or related field with at least 5 years of hands-on development experiences.; Knowledge in Big Data Platform such as Hadoop, mapR, Cloudera, HPE, etc.; Hands-on experience in Programming language including Python, PysSpark, SQL, noSQL, kSQL, prefer to have experience in Big Data application and Linux shell scripting.; Experience in Data API exposure (Spring Boot, Flask or equivalent) and web development.; Experience in Open Source like Apache Airflow, Zeppelin Notebook for Data Exploratory Analysis, et; Rewards that Go Beyond:; Hybrid work arrangements; Fulll suite of health and wellness benefits ; Ongoing training and development programs; Internal mobility opportunities;  Are you ready to say hello to BIG Possibilities; Take the leap with Singtel to unlock new opportunities and accelerate your growth. Apply now and start your empowering career!","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85784453","Role":"Data Engineer (Cloud Migration)","Company":"ANTAS PTE. LTD.","Location":"Singapore","Publish_Time":"2025-07-15 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85784453","job_desc":"Role Overview:; Seeking a seasoned data engineer (7+ years) to design, build, and optimize scalable data solutions using AWS (S3, RDS, Redshift, Glue, Lambda), Databricks (Delta Lake, Spark), and Informatica IDMC.; Role Overview:; Seeking a seasoned data engineer (7+ years) to design, build, and optimize scalable data solutions using AWS (S3, RDS, Redshift, Glue, Lambda), Databricks (Delta Lake, Spark), and Informatica IDMC.; Key Responsibilities:; Architect data platforms including lakes, warehouses, and databases on AWS and Databricks; Develop and automate ETL\/data pipelines with AWS Glue, Lambda, Databricks, and Informatica IDMC; Integrate and transform data from various sources ensuring quality and governance; Monitor and tune performance of data workflows and queries; Implement security, compliance, and cost optimization best practices; Maintain documentation and collaborate across teams to meet data needs; Skills Required:; 7+ years of experience in data engineering with hands-on work in AWS, Databricks, and\/or Informatica IDMC; Skilled in Python, Java, or Scala; strong SQL and NoSQL knowledge; Experience in data modeling, performance tuning, and debugging; Strong problem-solving, communication, and collaboration skills; Certifications in AWS, Databricks, or Informatica are a plus; Experience with Apache Spark, Hadoop, and Informatica IDMC for data governance; Knowledge of Tableau or Power BI; Familiarity with Docker, Kubernetes, Git, CI\/CD, and DevOps principles","salary":"","work_type":"Kontrak\/Temporer","country":"singapore"}
{"Job_ID":"85187458","Role":"Big Data","Company":"Goldtech Resources Pte Ltd","Location":"Singapore","Publish_Time":"2025-06-26 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85187458","job_desc":"Big Data (Full Time); Location In Singapore: Islandwide; Salary(SGD): 6000 - 10000; Apply; Job Description:; \u00b7 Analysing requirements, designing, developing, testing & supporting application and Data warehouses from build to production (including proof-of-concept); \u00b7   Ability to develop test plans and lead testing cycles.; \u00b7  Provide product and application support and maintenance.; \u00b7   Able to communicate pros and cons of different options to end users.; \u00b7 Leads in the assessment of the customer environment and the design of the solution architecture based on the requirements specifications.; \u00b7     Ensures appropriate documentation, customer involvement and sign-off.; \u00b7    Develop development framework and assign development effort to team members.; \u00b7    Actively leads and manages team members to a successful project.; Requirements:; \u00b7 Diploma\/Degree in Computer Science, Information Technology or equivalent.; \u00b7 Have 5 - 8 years\u2019 experience in installation and configuration of software in Linux\/Microsoft platforms with the understanding of security hardening, firewall and etc.; \u00b7Have 5 - 8 years\u2019 experience in Business Intelligence\/Data Warehouse\/Analytics Projects involving in requirements gathering designing, development, deployment, conducting knowledge transfer and post deployment support.; \u00b7  Have 5 - 8 years\u2019 experience familiar with Data Platform\/ETL\/Business Intelligence technologies such as:; o  SAP BI\/BODS; o  Tableau; o  Talend; o  MapR\/Cloudera\/Hortonworks platform; \u00b7   Preferably have experience actively leading and managing a team of 3 to 5 members.; \u00b7  Independent with ability to work effectively in a team and who takes initiative and engages their colleagues.; \u00b7Excellent communication and interpersonal skills with ability to communicate with clarity and confidence with colleagues and customers.; \u00b7 Likes technology, taking initiative to learn more and share knowledge with juniors and within the team.; \u00b7 Proven abilities to take initiative, innovative and the ability to develop creative solutions for challenging client needs.; \u00b7 Knowledge in any of following tools or technologies are not mandatory but will be an added advantage:; o  Data Modelling using dimensional modelling techniques (e.g. star and snow flake schemas) and designing the metadata layer for self-service analytics; o  SAP related skillsets S\/4 HANA, SAP Analytics Cloud, SAP BW; o  Cloud and network concepts; o  Databases such as MariaDB, MySQL, PostgreSQL; Apply","salary":"","work_type":"","country":"singapore"}
{"Job_ID":"85072996","Role":"Data Engineer (Azure Synapse Analytics, PySpark)","Company":"SembCorp Utilities Pte. Ltd.","Location":"East Region","Publish_Time":"2025-06-20 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85072996","job_desc":"About Sembcorp; Sembcorp is a leading energy and urban solutions provider headquartered in Singapore. Led by its purpose to drive energy transition, Sembcorp delivers sustainable energy solutions and urban developments by leveraging its sector expertise and global track record.; Play a role in Powering Asia\u2019s Energy Transition; Drive Asia\u2019s energy transition with us! Our Gas & Related Services segment is a key growth engine, delivering reliable and efficient energy to industries and communities across multiple countries. We support Asia\u2019s growing energy needs while advancing the shift to a lower-carbon future.; Purpose and Scope; We are seeking a highly skilled and self-driven Azure Data Engineer with expertise in PySpark, Python, and modern Azure data services including Synapse Analytics and Azure Data Explorer. The ideal candidate will design, develop, and maintain scalable data pipelines and architectures, enabling effective data management, analytics, and governance.; Key Roles and Responsibilities; Design, develop, and maintain scalable and efficient data pipelines (both batch and real-time streaming) using modern data engineering tools; Build and manage data lakes, data warehouses, and data marts using Azure Data Services; Integrate data from various sources including APIs, structured\/unstructured files, IoT devices, and real-time streams; Develop and optimize ETL\/ELT workflows using tools such as Azure Data Factory, Databricks, and Apache Spark; Implement real-time data ingestion and processing using Azure Stream Analytics, Event Hubs, or Kafka; Ensure data quality, availability, and security across the entire data lifecycle; Collaborate with analysts, data scientists, and engineering teams to deliver business-aligned data solutions; Contribute to data governance efforts and ensure compliance with data privacy standards; Establish and manage source system connectivity (on-prem, APIs, sensors, etc.); Handle deployment and migration of data pipeline artifacts between environments using Azure DevOps; Design, develop, and troubleshoot PySpark scripts and orchestration pipelines; Perform data integration using database joins and other transformations aligned with project requirements; Any assigned ad-hoc duties.; Requirements:; Bachelor\u2019s Degree in Computer Science, Engineering, or related field; 3\u20135 years of experience in Azure-based data engineering, PySpark, and Big Data technologies; Strong hands-on experience with Azure Synapse Analytics for pipeline orchestration and data handling; Expertise in SQL, data warehousing, data marts, and ingestion using PySpark and Python; Solid experience building and maintaining cloud-based ETL\/ELT pipelines, especially with Azure Data Factory or Synapse; Familiarity with cloud data environments such as Azure and optionally AWS; Experience with Azure DevOps for CI\/CD and artifact deployment; Excellent communication, problem-solving, and interpersonal skills; Good to Have:; 1\u20132 years of experience working with Azure Data Explorer (including row-level security and access controls).; Experience with Azure Purview for metadata management, data lineage, governance, and discovery; Ability to work independently and take full ownership of assignments; Proactive in identifying and resolving blockers and escalating when needed; Exposure to real-time processing with tools like Azure Stream Analytics or Kafka; Our Culture at Sembcorp; At Sembcorp, our culture is shaped by a strong set of shared behaviours that guide the way we work and uphold our commitment to driving the energy transition.; We foster an institution-first mindset, where the success of Sembcorp takes precedence over individual interests. Collaboration is at the heart of what we do, as we work seamlessly across markets, businesses, and functions to achieve our goals together. Accountability is a core principle, ensuring that we take ownership of our commitments and deliver on them with integrity and excellence. These values define who we are and create a workplace where our people can thrive while making a meaningful impact on driving energy transition.; Join us in making a real impact!","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85006790","Role":"Data Engineer-Consultant","Company":"WSH Experts Pte Ltd","Location":"Singapore","Publish_Time":"2025-06-17 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85006790","job_desc":"Job responsibilities; Create and manage a single master record for each business entity, ensuring data consistency, accuracy, and reliability.; Implement data governance processes, including data quality management, data profiling, data remediation, and automated data lineage.; Create and maintain multiple robust and high-performance data processing pipelines within Cloud, Private Data Centre, and Hybrid data ecosystems.; Assemble large, complex data sets from a wide variety of data sources.; Collaborate with Data Scientists, Machine Learning Engineers, Business Analysts, and Business users to derive actionable insights and reliable foresights into customer acquisition, operational efficiency, and other key business performance metrics.; Develop, deploy, and maintain multiple microservices, REST APIs, and reporting services.; Design and implement internal processes to automate manual workflows, optimize data delivery, and re-design infrastructure for greater scalability.; Establish expertise in designing, analyzing, and troubleshooting large-scale distributed systems.; Support and work with cross-functional teams in a dynamic environment.; Job Requirements; Experience building and operating large-scale data lakes and data warehouses.; Experience with Hadoop ecosystem and big data tools, including Spark and Kafka.; Experience with Master Data Management (MDM) tools and platforms such as Informatica MDM, Talend Data Catalog, Semarchy xDM, IBM PIM & IKC, or Profisee.; Familiarity with MDM processes such as golden record creation, survivorship,reconciliation, enrichment, and quality.; Experience in data governance, including data quality management, data profiling, data remediation, and automated data lineage.; Experience with stream-processing systems including Spark-Streaming.; Experience working with Cloud services using one or more Cloud providers such as Azure, GCP, or AWS.; Experience with Delta Lake and Databricks.; Advanced working experience with relational SQL and NoSQL databases, including Hive, HBase, and Postgres.; Deep understanding of SQL and the ability to optimize data queries.; If the requirement matches with your profile, kindly share your updated CV\/resume to Aparna at aparna@wshexperts.com.sg","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85653083","Role":"Data Engineer (DSAD)","Company":"SkillsFuture Singapore Agency","Location":"Paya Lebar Air Base","Publish_Time":"2025-07-10 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85653083","job_desc":"SSG is a dynamic and forward-thinking organization dedicated to empowering individuals and shaping the future of Singapore's workforce. As a statutory board under the Ministry of Education, SSG leads the charge in driving the SkillsFuture movement, a national initiative that promotes lifelong learning and skills development. With a strong focus on innovation and collaboration, SSG works closely with employers, training providers, and individuals to create a vibrant ecosystem of learning and growth. By offering a wide range of initiatives, programs, and funding schemes, SSG enables individuals to unlock their full potential, acquire new competencies, and stay ahead in a rapidly changing job market.; Join the Data Strategy & Analytics Division (DSAD) team to grow SSG into a data-mature organization where we use data pervasively and securely to deliver on our business missions:; ; \u2022 Develop and drive SSG\u2019s data strategy, including LOIs, data competency, and maintaining strong links with CIO Office; \u2022 Develop, implement and maintain comprehensive data governance and management framework that complies with WOG requirements Implement Lines of Inquiries; \u2022 Support data management and exploitation; \u2022 Develop MLOps and instil best practices; What you will be working on; As a Data Engineer within DSAD, you will play a pivotal role in managing and optimizing data pipelines, ensuring the seamless flow and integrity of data across our organization. Your expertise in data pipeline management, automation, MLOps, data governance, and infrastructure architecture will contribute significantly to our mission of leveraging data to drive innovation and operational excellence. We are looking for a candidate who embodies our core values of integrity, collaboration, and continuous improvement, and who is passionate about harnessing the power of data to achieve our strategic objectives.; Data Pipeline Management, Automation, and MLOps Proficiency; - Manage and optimize multi-stage data pipelines for various use cases, from data acquisition to consumption; - Automate data preparation and integration tasks using innovative tools and techniques; - Apply MLOps principles to integrate machine learning models efficiently with data pipelines and infrastructure; Data Governance, Cataloging, and Collaboration; - Work closely with data governance teams to ensure data is accurately cataloged and available for governed reuse; - Collaborate with AI engineers to optimize data solutions for machine learning projects, focusing on data flow and accessibility; Infrastructure Architecture and Data Modeling; - Design and manage scalable data infrastructure using cloud services like AWS; - Develop efficient data models and pipelines for large-scale data sets, ensuring data quality and availability; Data Quality Assurance, Documentation, and Continuous Improvement; - Lead initiatives to establish standards for data accuracy and reliability; - Maintain comprehensive documentation on data architectures and management practices; - Stay updated on data engineering advancements, advocating for and implementing improvements; - Facilitate knowledge sharing and technical training sessions to enhance organizational data competency; What we are looking for; Possess the required competencies to execute the job duties proficiently; A minimum of six years of experience in data management disciplines, including data integration, modeling, optimization, and data quality; At least three years of experience in cross-functional team collaboration and supporting data management\/analytics initiatives; Demonstrated ability to work creatively and collaboratively with both business and IT teams; Strong interpersonal skills with the ability to engage and earn the respect of stakeholders at all levels; Confident, energetic self-starter with a commitment to ethics, regulatory compliance, customer service, and business integrity; Highly adaptable and committed to excellence in a dynamic business environment; Successful candidates will be offered a 2-year contract in the first instance and may be considered for an extension or be placed on a permanent tenure.; Candidates are encouraged to sign up for a Careers & Skills Passport (CSP) account and include your CSP public profile in your resume. Please check out www.myskillsfuture.gov.sg for details on the CSP.","salary":"","work_type":"Kontrak\/Temporer","country":"singapore"}
{"Job_ID":"85054557","Role":"Data Engineer","Company":"Assurity Trusted Solutions Pte Ltd","Location":"Singapore","Publish_Time":"2025-06-17 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85054557","job_desc":"Assurity Trusted Solutions (ATS) is a wholly owned subsidiary of the Government Technology Agency (GovTech). As a Trusted Partner over the last decade, ATS offers a comprehensive suite of products and services ranging from infrastructure and operational services, authentication services, governance and assurance services as well as managed processes. In a dynamic digital and cyber landscape, where trust & collaboration are key, ATS continues to drive mutually beneficial business outcomes through collaboration with GovTech, government agencies and commercial partners to mitigate cyber risks and bolster security postures.; We are seeking a Data Engineer to join our dynamic team to develop and strengthen our data ecosystem for the maritime sector. This role is pivotal in bridging the gap between business\/regulatory needs and our maritime data ecosystem, so that we enhance Singapore\u2019s position as a leading global maritime hub. The ideal candidate will leverage stakeholder insights and agile methodologies to develop data ecosystem products that will be relevant for a range of stakeholders in our maritime ecosystem, which would include our port operators, international shipping lines, technology companies, among others.; The maritime sector in Singapore faces significant challenges, as well as opportunities. It is critical for MPA to position Singapore to capture benefits from growing maritime trade, while addressing various challenges including our manpower constraints and global pressures for maritime decarbonisation. This position offers a unique opportunity to enhance Singapore\u2019s global positioning as a leading maritime hub by advancing new value proposition that can be unlocked via data for our maritime ecosystem.; Responsibilities:; Design, Develop, Test, Deploy and Maintain data pipelines (ETL) on the Enterprise Data Warehouse and Big Data Platform; Design and Develop the API \/Web Services framework for curation of new datasets whether internal or external (Internet), and to interface with other systems (both internal and external); Explore and source new data sets to address emerging business use case needs; Support stakeholder engagement, development, implementation and maintenance of systems for data collection, storage, access, and analytics at scale.; Develop and manage continuous improvement of data architecture and ensure alignment with business requirements, data management and governance policies.; Design, develop, and maintain interactive dashboards that provide insights and data to support business decision-making.; Support the design and definition of the data architecture framework, standards, and principles, including modelling, metadata, privacy, security, reference data and master data; Requirements; 3+ years of related work experience as a Data Engineer; Good grasp of Software Engineering principles such as Requirements Gathering (both functional and non-functional), Modular & Re-usable Design.; Proficient in ETL using programming language \/tools such as Python and\/or SSIS and\/or Informatica Power Centre; Able to develop data applications including integration with ICT systems, build APIs and web applications via .Java and\/or Python; Familiarity with MS SQL, PostgreSQL or Oracle is preferred.; Proficient in Data Modelling and Data Mining.; Experience in designing and building scalable database schema for applications.; Understanding of Object-Oriented Design.; Knowledge of or prior work experience on Big Data platforms such as Hadoop or using Spark.; Experience in the cloud environment setup; Excellent organizational, analytical, and problem-solving skills.; Experience collaborating with business and product teams.; Join us and discover a meaningful and exciting career with Assurity Trusted Solutions!; The remuneration package will commensurate with your qualifications and experience. Interested applicants, please click \"Apply Now\".; We thank you for your interest and please note that only shortlisted candidates will be notified.; By submitting your application, you agree that your personal data may be collected, used and disclosed by Assurity Trusted Solutions Pte. Ltd. (ATS), GovTech and their service providers and agents in accordance with ATS\u2019s privacy statement which can be found at: https:\/\/www.assurity.sg\/privacy.html or such other successor site.; Benefits; A wholly-owned subsidiary of GovTech.; We promote a learning culture and encourage you to grow and learn.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85507921","Role":"Data Engineer","Company":"NOVADE SOLUTIONS PTE LTD","Location":"Singapore","Publish_Time":"2025-06-21 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85507921","job_desc":"Headquartered in Singapore, Novade is a leading field management platform designed to digitalize processes on construction and industrial sites. We help clients manage progress, quality, and safety with our cutting-edge solutions, empowering teams to achieve better performance and compliance. With millions of records processed daily, Novade transforms how teams collaborate and leverage data to drive success.; About Our Data Team; Our Data Team is at the forefront of innovation, with missions including:; Developing scalable data pipelines that process millions of records, offering powerful, customizable analytics to help clients improve their performance; Training predictive models to detect risks; Developing Generative AI use cases to push boundaries in digital transformation; Managing internal analytics to enable data-driven decisions for our teams and delivering top-tier consulting to our clients; Key Responsibilities; Design, develop, and maintain data pipelines to ensure efficient data flow and high data quality; Implement scalable data models and visualizations for our clients; Perform data analysis on client and business data; Communicate findings and insights effectively through presentations and reports; Conduct feature engineering for predictive analytics; Create beautiful and efficient dashboards to visualize data and derive actionable insights; Occasionally, train machine learning models once other tasks have been mastered; What You Need for This Position; Technical skills:; Proficiency in Python (including PySpark, Pandas, NumPy); Strong SQL skills; Experience with building ETL pipelines; Experience with BI tools, preferably Power BI; Familiarity with machine learning techniques; Strong analytical skills with an understanding of statistical principles; Knowledge of Databricks or Apache Airflow is a plus; Computer science skills are a plus; Soft skills:; Autonomy and initiative \u2014 you know when to dive deep and when to seek help; Reliability, particularly in managing production pipelines and reacting swiftly to issues; Team spirit and adaptability; Eagerness to learn and grow; Communication and presentation abilities; Experience (internship) with SaaS platforms is a plus; What You Will Get With Us; Learn and contribute to the development of a state-of-the-art, scalable data platform; Be part of a fun and rapidly growing company making waves in the industry; Work alongside a dynamic team of young but experienced professionals; Experience a culture of respect, collaboration, and innovation; Make a tangible impact\u2014join us in revolutionizing the digital landscape of the construction industry; Gain access to a unique dataset within the construction sector, unlocking invaluable learning opportunities; If you\u2019re passionate about data engineering, eager to work on impactful projects, and ready to learn from a team of experts, we\u2019d love to hear from you!","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85607529","Role":"Cloud Data Engineer Lead (AWS, Databricks, and Informatica IDMC)","Company":"Synapxe","Location":"One North","Publish_Time":"2025-07-09 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85607529","job_desc":"Roles And Responsibilities:; \u2022 Design and architect data storage solutions, including databases, data lakes, and warehouses, using AWS services such as Amazon S3, Amazon RDS, Amazon Redshift, and Amazon DynamoDB, along with Databricks' Delta Lake. Integrate Informatica IDMC for metadata management and data cataloging.; \u2022 Create, manage, and optimize data pipelines for ingesting, processing, and transforming data using AWS services like AWS Glue, AWS Data Pipeline, and AWS Lambda, Databricks for advanced data processing, and Informatica IDMC for data integration and quality.; \u2022 Integrate data from various sources, both internal and external, into AWS and Databricks environments, ensuring data consistency and quality, while leveraging Informatica IDMC for data integration, transformation, and governance.; \u2022 Develop ETL (Extract, Transform, Load) processes to cleanse, transform, and enrich data, making it suitable for analytical purposes using Databricks' Spark capabilities and Informatica IDMC for data transformation and quality.; \u2022 Monitor and optimize data processing and query performance in both AWS and Databricks environments, making necessary adjustments to meet performance and scalability requirements. Utilize Informatica IDMC for optimizing data workflows.; \u2022 Implement security best practices and data encryption methods to protect sensitive data in both AWS and Databricks, while ensuring compliance with data privacy regulations. Employ Informatica IDMC for data governance and compliance.; \u2022 Implement automation for routine tasks, such as data ingestion, transformation, and monitoring, using AWS services like AWS Step Functions, AWS Lambda, Databricks Jobs, and Informatica IDMC for workflow automation.; \u2022 Maintain clear and comprehensive documentation of data infrastructure, pipelines, and configurations in both AWS and Databricks environments, with metadata management facilitated by Informatica IDMC.; \u2022 Collaborate with cross-functional teams, including data scientists, analysts, and software engineers, to understand data requirements and deliver appropriate solutions across AWS, Databricks, and Informatica IDMC.; \u2022 Identify and resolve data-related issues and provide support to ensure data availability and integrity in both AWS, Databricks, and Informatica IDMC environments.; \u2022 Optimize AWS, Databricks, and Informatica resource usage to control costs while meeting performance and scalability requirements.; \u2022 Stay up-to-date with AWS, Databricks, Informatica IDMC services, and data engineering best practices to recommend and implement new technologies and techniques.; Requirements \/ Qualifications; \u2022 Bachelor\u2019s or master\u2019s degree in computer science, data engineering, or a related field.; \u2022 Minimum 10 years of experience in data engineering, with expertise in AWS services, Databricks, and\/or Informatica IDMC.; \u2022 Proficiency in programming languages such as Python, Java, or Scala for building data pipelines.; \u2022 Evaluate potential technical solutions and make recommendations to resolve data issues especially on performance assessment for complex data transformations and long running data processes.; \u2022 Strong knowledge of SQL and NoSQL databases.; \u2022 Familiarity with data modeling and schema design.; \u2022 Excellent problem-solving and analytical skills.; \u2022 Strong communication and collaboration skills.; \u2022 AWS certifications (e.g., AWS Certified Data Analytics - Specialty, AWS Certified Data Analytics - Specialty), Databricks certifications, and Informatica certifications are a plus.; Preferred Skills:; \u2022 Experience with big data technologies like Apache Spark and Hadoop on Databricks.; \u2022 Knowledge of data governance and data cataloguing tools, especially Informatica IDMC.; \u2022 Familiarity with data visualization tools like Tableau or Power BI.; \u2022 Knowledge of containerization and orchestration tools like Docker and Kubernetes.; \u2022 Understanding of DevOps principles for managing and deploying data pipelines.; \u2022 Experience with version control systems (e.g., Git) and CI\/CD pipelines","salary":"","work_type":"Kontrak\/Temporer, Full time","country":"singapore"}
{"Job_ID":"85019312","Role":"Senior Data Engineer","Company":"GC ASIA DENTAL PTE LTD","Location":"Tampines","Publish_Time":"2025-06-18 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85019312","job_desc":"Job Responsibilities; Microsoft Fabric Expertise: Serve as the primary subject matter expert for Microsoft Fabric, including but not limited to Lakehouse, Data Factory, Synapse Data Engineering (Spark), Data Warehousing, Real-Time Analytics, and Power BI integration.; Data Solution Design & Development: Lead the end-to-end design, development, and implementation of robust, scalable, and efficient data pipelines and solutions within the Microsoft Fabric ecosystem. This includes data ingestion, transformation, orchestration, and consumption layers.; Architectural Leadership: Contribute to and drive the architectural vision for our data platform on Microsoft Fabric, ensuring best practices for performance, security, reliability, and cost-effectiveness.; Independent Project Management: Take full ownership of data engineering projects, including planning, resource allocation (where applicable), timeline management, risk mitigation, and successful delivery.; Stakeholder Collaboration: Work closely with data analysts, business stakeholders, and other engineering teams to understand data requirements, translate them into technical specifications, and deliver impactful data solutions.; Data Governance & Quality: Implement and enforce data governance, quality, and security best practices within the Microsoft Fabric environment.; Performance Optimization & Troubleshooting: Proactively monitor, troubleshoot, and optimize data pipelines and data models for performance and efficiency.; Mentorship & Best Practices: Champion best practices in data engineering, data modeling, and Microsoft Fabric utilization. Potentially mentor junior team members and foster a culture of continuous learning.; Documentation: Create and maintain comprehensive technical documentation for data pipelines, data models, and architectural designs.; Job Requirements; Minimum 5+ years of experience in data engineering, with a strong focus on building and maintaining enterprise-grade data platforms.; Demonstrable expert-level proficiency in Microsoft Fabric, with hands-on experience across multiple components (Lakehouse, Data Factory, Synapse Data Engineering\/Spark, Data Warehousing, Real-Time Analytics).; Proven track record of successful project management in data-related initiatives, including planning, execution, and delivery.; Extensive experience with Spark (PySpark\/Scala) for data transformation and processing within a distributed environment.; Strong understanding of data warehousing concepts, dimensional modeling, and data lake architectures.; Proficiency in SQL for data manipulation and analysis.; Experience with Azure ecosystem components (e.g., Azure Data Lake Storage, Azure DevOps, Azure Functions) is a strong plus.; Experience with version control systems (e.g., Git).; Excellent problem-solving, analytical, and critical thinking skills.; Exceptional communication and interpersonal skills, with the ability to articulate complex technical concepts to non-technical stakeholders and work effectively in a team-oriented environment.; Highly self-motivated and able to work independently with minimal supervision, taking initiative and ownership of tasks.; Microsoft Certified: Azure Data Engineer Associate, or other relevant Microsoft Azure\/Fabric certifications.; Experience with real-time data streaming technologies.; Familiarity with CI\/CD practices for data pipelines.; Experience with Power BI for data visualization and reporting.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"80701591","Role":"Data Engineer (Data Operations) (In Partnership with IMDA)","Company":"Phillip Securities Pte Ltd","Location":"Singapore","Publish_Time":"2025-06-17 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/80701591","job_desc":"Responsibilities:; Define the overall data architecture and governance framework.; Ensure consistency and alignment with business objectives; Design and develop data pipelines and automation workflows using DataOps tools.; Implement CI\/CD pipelines for data pipelines and applications; Implement data quality checks and validation processes.; Ensure system reliability, data security and compliance.; Requirements:; Degree in Computer Science, Data Science, Mathematics or a related IT field; Excellent time management, prioritization, and multitasking skills.; Strong interpersonal and communication skills.; Team-oriented, self-motivated and adaptable.; If you are looking for an environment of growth and opportunities, please write in with a detailed resume stating the position applied and expected salaries to the HR department via recruitment@phillip.com.sg.; We regret that only shortlisted candidates will be notified.; Brought to you by Phillip Securities Pte Ltd (A member of PhillipCapital)","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"83649507","Role":"Senior Data Engineer","Company":"Mediacorp Pte Ltd","Location":"Singapore","Publish_Time":"2025-06-17 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/83649507","job_desc":"Job Description; PURPOSE OF THIS ROLE; The purpose of the Senior Data Engineer role is to provide strategic leadership in data engineering, overseeing the development and maintenance of data infrastructure, and ensuring the availability, reliability, and performance of data solutions. This position plays a critical role in supporting data-driven decision-making across the organization.; Key responsibilities include:; Providing strategic direction and technical leadership in data engineering.; Leading the design, development, and deployment of ETL processes and data pipelines.; Integrating data from various sources and transforming it for analytics.; Maintaining data quality and implementing data governance best practices.; Managing and mentoring a team of data engineers for professional growth.; Collaborating with stakeholders to understand and fulfil data requirements.; Documenting data processes, architectures, and best practices.; Challenges in this role may include:; Ensuring data security and compliance in a rapidly evolving regulatory environment.; Optimizing data infrastructure for performance and scalability.; Resolving data-related issues efficiently.; Managing a dynamic team in a fast-paced data environment.; FOUNDATIONAL\/LEADERSHIP COMPETENCIES; Strong leadership and team management skills.; Excellent problem-solving and communication skills.; FUNCTIONAL COMPETENCIES; Proficiency in programming languages such as Java, or Scala, SQL, Python.; Extensive experience with ETL tools, data integration, and data transformation.; In-depth knowledge of data storage technologies (relational databases, NoSQL, distributed file systems).; Expertise in data modelling, database design and data warehousing.; Familiarity with data governance, security, and compliance standards.; Experience with big data technologies (e.g., Hadoop, Spark, Hive, Azure, Databricks) is a plus.; Experience in container orchestration framework like Kubernetes is a plus.; Experience in Infrastructure as Code and DevOps, MLOps and DataOps is a plus.; Job Requirements; Bachelor's or Master's degree in Computer Science, Information Technology, or a related field.; Relevant certifications in data engineering and Cloud computing are advantageous.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"84995480","Role":"Senior Data Engineer - A25054","Company":"Activate Interactive Pte Ltd","Location":"Central Region","Publish_Time":"2025-06-18 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/84995480","job_desc":"About the job; Activate Interactive Pte Ltd (\"Activate\") is a leading technology consultancy headquartered in Singapore with a presence in Malaysia and Indonesia. Our clients are empowered with quality, cost-effective, and impactful end-to-end application development, like mobile and web applications, and cloud technology that remove technology roadblocks and increase their business efficiency.; ; We believe in positively impacting the lives of people around us and the environment we live in through the use of technology. Hence, we are committed to providing a conducive environment for all employees to realise their full potential, who in turn have the opportunity to continuously drive innovation.; ; We are searching for our next team members to join our growing team.; ; If you love the idea of being part of a growing company with exciting prospects in mobile and web technologies that create positive impact on people's lives, then we would love to hear from you.; ; Technology Solutions Office is looking for Senior Data Engineer; ; Internal Code: A25054; ; What will you do?; Lead a team of data engineers to design and develop robust data architectures that meet key business needs; Design, build, and maintain scalable data pipelines to handle large volumes of data; Collaborate with cross-functional teams to define data requirements and deliver quality solutions; Implement data governance and data quality methodologies to ensure data integrity; Utilize advanced analytics and data transformation techniques to derive insights and provide strategic recommendations; Mentor and train junior data engineers to enhance their skills and knowledge in data technologies; Requirements; ; What are we looking for?; At least 5 years of experience in data engineering, with a proven track record of leading data projects; Strong proficiency in data engineering tools and technologies including AWS, Azure, SQL, Python, and Spark; Extensive experience with data modeling, ETL processes, and big data technologies; Familiarity with data visualization tools like Tableau, Power BI, or similar; Experience with machine learning concepts and frameworks is a plus; Demonstrated ability to communicate effectively with technical and non-technical stakeholders; Strong problem-solving skills and ability to work in a fast-paced environment; Bachelor's or Master's degree in Computer Science, Data Science, or a related field; Benefits; ; What do we offer in return?; Fun working environment; Employee Wellness Program; Does it sound like something you are interested in exploring further? Please be in touch with our team for an initial chat at careers@activate.sg; ; Activate Interactive Singapore is an equal opportunity employer. Employment decisions will be based on merit, qualifications and abilities. Activate Interactive Pte Ltd does not discriminate in employment opportunities or practices on the basis of race, colour, religion, national origin, age, disability, marital status or any other characteristics protected by law.; ; Protecting your privacy and the security of your data are longstanding top priorities for Activate Interactive Pte Ltd.; ; Your personal data will be processed for the purposes of managing Activate Interactive Pte Ltd's recruitment related activities, which include setting up and conducting interviews and tests for applicants, evaluating and assessing the results, and as is otherwise needed in the recruitment and hiring processes.; ; Please consult our Privacy Notice (https:\/\/www.activate.sg\/privacy-policy) to know more about how we collect, use, and transfer the personal data of our candidates. Here you can find how you can request for access, correction and\/or withdrawal of your Personal Data.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"84701300","Role":"G07 - Data Engineer","Company":"FPT Asia Pacific Pte Ltd","Location":"Singapore","Publish_Time":"2025-06-17 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/84701300","job_desc":"What you will be working on:; Architect and build ingestion pipelines to collect, clean, merge, and harmonize data from different source systems.; Translate data requirements from business users into technical specifications.; Work with Agency's project teams to develop user stories, functional\/technical specifications and acceptance criteria; Design and build secure mechanisms for end users and systems to access data in data warehouse.; Research, propose and develop new technologies and processes to improve agency data infrastructure.; Implement data quality checks and validation processes to ensure data accuracy and consistency; Review data pipelines to ensure adherence to data management standards, policies and procedures.; Analyse impact of requested changes and propose improvements to continuously address changing business needs, and work with support team to understand and address technical problems (Operations & Maintenance phase); What we are looking for; Diploma\/Degree in Computer Science, Infocomm Technology, Computer or Electronics Engineering or related subject area with minimum 3 years of relevant experience.; Data manipulation using scripting languages like Python or using ETL tools Visual analytics technologies like Tableau, Power BI End-to-end analytics architecture, preferably with some working knowledge of big data stack; Experience with SQL\/NoSQL databases; Experience in AWS Cloud especially Glue and RedShift; Experience working with CI\/CD setups; Strong knowledge of algorithms and database structures; Strong knowledge of database integration and migration strategy; Strong knowledge in designing and implementing scalable data infrastructure","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"84822778","Role":"G08 - Data Engineer","Company":"FPT Asia Pacific Pte Ltd","Location":"Singapore","Publish_Time":"2025-06-17 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/84822778","job_desc":"We are looking for experienced data engineers to join our team who will be responsible for:; Data Engineering and Platform Integration; Design, develop, and maintain data pipelines and ETL processes using AWS services (Glue, Athena, S3, RDS); Work with data virtualisation tools like Denodo and develop VQL queries; Ingest and process data from various internal and external data sources; Perform data extraction, cleaning, transformation, and loading operations; Implement automated data collection processes including API integrations when necessary; Data Architecture; Design and implement data models (conceptual, logical, and physical) using tools like ER Studio; Develop and maintain data warehouses, data lakes, and operational data stores; Develop and maintain data blueprints; Create data marts and analytical views to support business intelligence needs using Denodo, RDS; Implement master data management practices and data governance standards; Technical Architecture and Integration; Ensure seamless integration between various data systems and applications; Implement data security and compliance requirements; Design scalable solutions for data integration and consolidation; Development and Analytics; Develop Python scripts in AWS Glue for data processing and automation; Write efficient VQL\/SQL queries and stored procedures; Design and develop RESTful APIs using modern frameworks and best practices for data services; Work with AWS Sagemaker for machine learning model deployment and integration; Manage and optimise database performance, including indexing, query tuning, and maintenance; Work in an Agile environment and participate in sprint planning, daily stand-ups, and retrospectives; Implement and maintain CI\/CD pipelines for automated testing and deployment; Participate in peer code reviews and pair programming sessions; Documentation and Best Practices; Create and maintain technical documentation for data models and systems; Follow industry-standard coding practices, version control, and change management procedures; Stakeholder Collaboration; Partner with cross-functional teams on data engineering initiatives; Gather requirements, conduct technical discussions, implement solutions, and perform testing; Collaborate with Product Managers, Business Analysts, Data Analysts, Solution Architects, UX Designers to build scalable, data-driven products; Provide technical guidance and support for data-related queries; Qualifications and Experience:; At least 3 years of experience in data engineering or similar role; Strong proficiency in Python, VQL, SQL; Experience with AWS services (Glue, Athena, S3, RDS, Sagemaker); Knowledge of data virtualisation concepts and tools (preferably Denodo); Experience with BI tools (preferably Tableau, Power BI); Understanding of data modelling and database design principles; Familiarity with data governance and master data management concepts; Experience with version control systems (Gitlab) and CI\/CD pipelines; Experience working in Agile environments with iterative development practices; Strong problem-solving skills and attention to detail; Excellent communication skills and ability to work in a team environment; Knowledge of AI technologies (AWS Bedrock, Azure AI, LLMs) would be advantageous","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"83729756","Role":"Senior Data Engineer","Company":"WSH Experts Pte Ltd","Location":"Singapore","Publish_Time":"2025-06-17 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/83729756","job_desc":"Job Description; Create and manage a single master record for each business entity, ensuring data consistency, accuracy, and reliability.; Implement data governance processes, including data quality management, data profiling, data remediation, and automated data lineage.; Create and maintain multiple robust and high-performance data processing pipelines within Cloud, Private Data Centre, and Hybrid data ecosystems.; Assemble large, complex data sets from a wide variety of data sources.; Collaborate with Data Scientists, Machine Learning Engineers, Business Analysts, and Business users to derive actionable insights and reliable foresights into customer acquisition, operational efficiency, and other key business performance metrics.; Develop, deploy, and maintain multiple microservices, REST APIs, and reporting services.; Design and implement internal processes to automate manual workflows, optimize data delivery, and re-design infrastructure for greater scalability.; Establish expertise in designing, analyzing, and troubleshooting large-scale distributed systems.; Support and work with cross-functional teams in a dynamic environment.; Job Requirement; Experience building and operating large-scale data lakes and data warehouses.; Experience with Hadoop ecosystem and big data tools, including Spark and Kafka.; Experience with Master Data Management (MDM) tools and platforms such as Informatica MDM, Talend Data Catalog, Semarchy xDM, IBM PIM & IKC, or Profisee.; Familiarity with MDM processes such as golden record creation, survivorship,reconciliation, enrichment, and quality.; Experience in data governance, including data quality management, data profiling, data remediation, and automated data lineage.; Experience with stream-processing systems including Spark-Streaming.; Experience working with Cloud services using one or more Cloud providers such as Azure, GCP, or AWS.; Experience with Delta Lake and Databricks.; Advanced working experience with relational SQL and NoSQL databases, including Hive, HBase, and Postgres.; Deep understanding of SQL and the ability to optimize data queries.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85043745","Role":"Data Engineer","Company":"Tabernacle Pte Ltd","Location":"Central Region","Publish_Time":"2025-06-19 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85043745","job_desc":"We are seeking a hands-on Fabric Onsite Data Engineer to support our data modernization initiatives by leveraging Microsoft Fabric. This role is critical in transitioning legacy ADF pipelines to Fabric Pipelines and Dataflows Gen2 while ensuring high data quality, robust metadata management, and effective collaboration with business and technical stakeholders.; Key Responsibilities:; Rebuild and optimize existing Azure Data Factory (ADF) ingestion processes using Microsoft Fabric Pipelines and Dataflows Gen2.; Coordinate with SL10\/EQS source system owners to align data schemas, field mappings, and ingestion specifications.; Perform onsite data validation and integrity checks across ingestion workflows to ensure accuracy and completeness.; Capture and log file-level and field-level metadata (e.g., file name, source system, ingestion timestamp) to enhance traceability and auditability.; Support User Acceptance Testing (UAT), triage user-reported issues, and provide technical support to business users during validation phases.; Train and guide end users on accessing and using Power BI reports, data glossary resources, and Baobao Q&A tools for self-service analytics.; Proactively respond to data queries from business users, ensuring data understanding and alignment with operational needs.; Qualifications:; Diploma or Degree in Information Technology, Computer Science, or related field.; Proven experience with Microsoft Fabric, including Pipelines and Dataflows Gen2.; Strong knowledge of Azure Data Factory, data ingestion strategies, and ETL\/ELT best practices.; Familiarity with data modeling, schema design, and metadata logging.; Experience working with business users and system owners to align technical data structures with business requirements.; Solid understanding of data quality assurance, validation techniques, and troubleshooting in production environments.; Excellent communication skills to support user training and documentation.; Ability to work onsite and collaborate across cross-functional teams in a dynamic environment.","salary":"","work_type":"Kontrak\/Temporer","country":"singapore"}
{"Job_ID":"85233867","Role":"Data Engineer (GIC)","Company":"AvePoint","Location":"Singapore","Publish_Time":"2025-06-27 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85233867","job_desc":"About AvePoint: ; Securing the Future. AvePoint is a global leader in data management and data governance, and over 21,000 customers worldwide rely on our solutions to modernize the digital workplace across Microsoft, Google, Salesforce and other collaboration environments. AvePoint\u2019s global channel partner program includes over 3,500 managed service providers, value added resellers and systems integrators, with our solutions available in more than 100 cloud marketplaces. To learn more, visit www.avepoint.com.; At AvePoint, we are committed to investing in our people. Agility, passion and teamwork set us up to do our best work and foster a culture where you are empowered to craft your career, make an impact, and own (y)our future. Unleash the power of you!; About the role:; \u2022Support Data Movement project.; Project Summary:; \u2022The successful candidate will be involved in building, integrating and testing a Data Toolkit to standardize efforts across the organization to align Data Ingestion and Observability.; \u2022Executing and supporting system and data migrations to the \u201cData Toolkit\u201d from Legacy systems and processes.; Skillset (Must have) ; Possess a degree in Computer Science\/Information Technology or related fields.; Proficient in Python or Java\/Kotlin (Must upskill and use Python).; Familiarity with OpenSearch, Elasticsearch or Solr.; Strong software engineering, analytical and problem-solving skills.; Good team player and communication skill.; Skillset (Good to have); Good foundation in Data such as SQL, ELT\/ETL and Pipelines.; AvePoint is proud to employ talent from many different backgrounds, experiences, and identities. We believe that diversity and inclusion drives our success and is at the core of how we hire, communicate, and collaborate to deliver value and excellence. We are committed to fostering an environment where people can bring their whole selves to work and feel a sense of belonging, and we continue to work toward creating a workforce that represents the diversity of our customers and communities.  ; Any personal data you share with us during the application process will be processed strictly in compliance with applicable data protection laws and our .","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85207528","Role":"Data Engineer (Intern)","Company":"LHN Group Pte Ltd","Location":"East Region","Publish_Time":"2025-06-25 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85207528","job_desc":"We are seeking a highly motivated and inquisitive Data Engineering Intern to assist in the design, development, and maintenance of a new data warehouse on the AWS cloud platform. You will play a crucial role in building and optimizing data pipelines to extract, transform, and load (ETL) data from various sources. This is an excellent opportunity to gain hands-on experience with real-world data engineering challenges and contribute to the growth of a dynamic company.; Key Responsibilities:; \u00b7 Assist in the design and development of the data warehouse architecture on AWS.; \u00b7 Develop and maintain data pipelines using AWS services like AWS Glue, AWS S3, AWS Redshift\/Aurora.; \u00b7 Extract, transform, and load data from various sources (NetSuite, Salesforce, SharePoint, other SaaS & internal systems) into the data warehouse.; \u00b7 Ensure data quality and integrity throughout the data pipeline.; \u00b7 Collaborate with data analysts and business users to understand their data needs and requirements.; \u00b7 Assist in the development of data quality checks and monitoring processes.; \u00b7 Learn and apply best practices in data engineering and cloud computing.; \u00b7 Document all data pipelines and processes.; \u00b7 Support the development and maintenance of data models.; Requirements; \u00b7 Currently pursuing a Bachelor's or Master's degree in Computer Science, Data Science, or a related field.; \u00b7 Strong understanding of data warehousing concepts and principles.; \u00b7 Experience with SQL and a scripting language like Python.; \u00b7 Basic understanding of cloud computing concepts and AWS services (preferred).; \u00b7 Familiarity with data modeling and data visualization tools (a plus).; \u00b7 Excellent analytical and problem-solving skills.; \u00b7 Strong communication and interpersonal skills.; \u00b7 Ability to work independently and as part of a team.; \u00b7 Passion for learning new technologies and solving challenging problems.","salary":"","work_type":"Kontrak\/Temporer","country":"singapore"}
{"Job_ID":"85603113","Role":"Platform Engineer - Data & AI","Company":"Equinix Asia Pacific","Location":"Singapore","Publish_Time":"2025-07-09 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85603113","job_desc":"Who are we?; Equinix is the world\u2019s digital infrastructure company\u00ae, operating over 260 data centers\u202facross the globe. Digital leaders harness Equinix's trusted platform to bring together and interconnect foundational infrastructure at software speed. Equinix enables organizations to access all the right places, partners and possibilities to scale with agility, speed the launch of digital services, deliver world-class experiences and multiply their value, while supporting their sustainability goals. ; Our culture is based on collaboration and the growth and development of our teams.\u202f We hire hardworking people who thrive on solving\u202fchallenging\u202fproblems and give them opportunities to hone new skills and try new approaches, as we grow our product portfolio with new software and network architecture solutions. We embrace diversity in thought and contribution and are committed to providing\u202fan equitable work environment that is foundational to our core values as a company and is vital to our success.; Job Summary; We\u2019re looking for a Senior Platform Engineer with a strong foundation in data architecture, distributed systems, and modern cloud-native platforms to architect, build, and maintain intelligent infrastructure and systems that power our AI, GenAI and data-intensive workloads.; You\u2019ll work closely with cross-functional teams, including data scientists, ML & software engineers, and product managers & play a key role in designing a highly scalable platform to manage the lifecycle of data pipelines, APIs, real-time streaming, and agentic GenAI workflows, while enabling federated data architectures. The ideal candidate will have a strong background in building and maintaining scalable AI & Data Platform, optimizing workflows, and ensuring the reliability and performance of Data Platform systems.; Responsibilities; Platform & Cloud Engineering; Develop and maintain real-time and batch data pipelines using tools like Airflow, dbt, Dataform, and Dataflow\/Spark; Design and develop event-driven architectures using Apache Kafka, Google Pub\/Sub, or equivalent messaging systems; Build and expose high-performance data APIs and microservices to support downstream applications, ML workflows, and GenAI agents; Architect and manage multi-cloud and hybrid cloud platforms (e.g., GCP, AWS, Azure) optimized for AI, ML, and real-time data processing workloads; Build reusable frameworks and infrastructure-as-code (IaC) using Terraform, Kubernetes, and CI\/CD to drive self-service and automation; Ensure platform scalability, resilience, and cost efficiency through modern practices like GitOps, observability, and chaos engineering; Data Architecture & Governance; Lead initiatives in data modeling, semantic layer design, and data cataloging, ensuring data quality and discoverability across domains; Implement enterprise-wide data governance practices, schema enforcement, and lineage tracking using tools like DataHub, Amundsen, or Collibra; Guide adoption of data fabric and mesh principles for federated ownership, scalable architecture, and domain-driven data product development; AI & GenAI Platform Integration; Integrate LLM APIs (OpenAI, Gemini, Claude, etc.) into platform workflows for intelligent automation and enhanced user experience; Build and orchestrate multi-agent systems using frameworks like CrewAI, LangGraph, or AutoGen for use cases such as pipeline debugging, code generation, and MLOps; Experience in developing and integrating GenAI applications using MCP and orchestration of LLM-powered workflows (e.g., summarization, document Q&A, chatbot assistants, and intelligent data exploration); Hands-on expertise building and optimizing vector search and RAG pipelines using tools like Weaviate, Pinecone, or FAISS to support embedding-based retrieval and real-time semantic search across structured and unstructured datasets; Engineering Enablement; Create extensible CLIs, SDKs, and blueprints to simplify onboarding, accelerate development, and standardize best practices; Streamline onboarding, documentation, and platform implementation & support using GenAI and conversational interfaces; Collaborate across teams to enforce cost, reliability, and security standards within platform blueprints; Work with engineering by introducing platform enhancements, observability, and cost optimization techniques; Foster a culture of ownership, continuous learning, and innovation; Qualifications; 5+ years of hands-on experience in Platform or Data Engineering, Cloud Architecture, AI Engineering roles; Strong programming background in Java, Python, SQL, and one or more general-purpose languages; Deep knowledge of data modeling, distributed systems, and API design in production environments; Proficiency in designing and managing Kubernetes, serverless workloads, and streaming systems (Kafka, Pub\/Sub, Flink, Spark); Experience with metadata management, data catalogs, data quality enforcement, and semantic modeling & automated integration with Data Platform; Proven experience building scalable, efficient data pipelines for structured and unstructured data; Experience with GenAI\/LLM frameworks and tools for orchestration and workflow automation; Experience with RAG pipelines, vector databases, and embedding-based search; Familiarity with observability tools (Prometheus, Grafana, OpenTelemetry) and strong debugging skills across the stack; Experience with ML Platforms (MLFlow, Vertex AI, Kubeflow) and AI\/ML observability tools; Prior implementation of data mesh or data fabric in a large-scale enterprise; Experience with Looker Modeler, LookML, or semantic modeling layers; Why You\u2019ll Love This Role; Drive technical leadership across AI-native data platforms, automation systems, and self-service tools; Collaborate across teams to shape the next generation of intelligent platforms in the enterprise; Work with a high-energy, mission-driven team that embraces innovation, open-source, and experimentation; Equinix is committed to ensuring that our employment process is open to all individuals, including those with a disability.  If you are a qualified candidate and need assistance or an accommodation, please let us know by completing this form.; Equinix is an Equal Employment Opportunity and, in the U.S., an Affirmative Action employer.  All qualified applicants will receive consideration for employment without regard to unlawful consideration of race, color, religion, creed, national or ethnic origin, ancestry, place of birth, citizenship, sex, pregnancy \/ childbirth or related medical conditions, sexual orientation, gender identity or expression, marital or domestic partnership status, age, veteran or military status, physical or mental disability, medical condition, genetic information, political \/ organizational affiliation, status as a victim or family member of a victim of crime or abuse, or any other status protected by applicable law.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"84219657","Role":"Cloud Data Engineer","Company":"PLOY ASIA PTE. LTD.","Location":"Singapore","Publish_Time":"2025-06-17 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/84219657","job_desc":"Cloud Data Engineer; Whats on offer:; Job Type: Permanent; Package: Base x 12 + AWS; Location: Singapore; Industry: Consulting; Required Skills and Experience:; 5+ years of Data consulting experience, or other relevant experience in AI & Analytics domain, with a proven track record of building and maintaining client relationships; Collaborate with customers and account partners to identify new Data and AI opportunities, and build proposals and pitch materials to position the firm as a trusted partner for AI & Data transformation; Organize and lead educational and ideation AI and Generative AI workshops for customers; Understand customer's needs and assess their data maturity; Evaluate and recommend appropriate data pipelines, frameworks, tools and platforms; Lead data feasibility studies and PoC projects to demonstrate the value and viability of Data and AI solutions; Develop end to end AI PoC projects using Python, Flask, FastAPI and Streamlit; Have experience in cloud services of AWS \/ GCP \/ Azure to deploy PoCs and pipelines; Have experience with big data frameworks on prem and on cloud; Collaborate with AI architects and engineers to data scientist and develop Data and AI solutions tailored to the clients' requirements; Lead integration and deployment of data and AI products; Stay updated with the latest advancements in big data and GenAI along with best practices to develop thought leadership and PoV; Work with cross-functional team and partners to develop\/ enhance and package AI offerings and assets for customer-facing discussions; In-depth understanding of big data technologies, streaming applications and data modelling; Experience working with customers in different industries and understanding their specific challenges and requirements; Preferred Skills and Experience:; Bachelor's degree in computer science, Information Security, or a related field Skilled in planning, organization, analytics, and problem-solving.; Cloud certifications related to data, analytics and architecture in one of AWS, GCP, Azure; Excellent communication and interpersonal skills to work collaboratively with clients and team members.; Comfortable working with statistics.; Advanced skills in PowerPoint and Excel.; Strong Business acumen and understanding of industry trends and challenges in Data, AI and Generative AI .; Strong leadership skills and proactive and self-driven attitude.; Advanced communication and storytelling skills.; Highly adaptable and flexible through shifting priorities.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85073505","Role":"Data Engineer (Singapore Citizen Only)","Company":"ASTEK Singapore Innovation Technology Pte. Ltd.","Location":"Central Region","Publish_Time":"2025-06-20 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85073505","job_desc":"Astek is a technology and engineering consulting firm headquartered in France with a global presence.; We are currently looking for a Mid-Level Data Engineer to join one of our projects based in Singapore.; The ideal candidate should have a minimum of 3 years\u2019 experience in Data Engineering, with proven expertise in Python, SQL, and Tableau\/Power BI.; Due to the high-security requirements of the project, only Singapore Citizens will be considered.; Responsibilities:; Work closely with business (research analysts, quantitative strategist, end-users, etc.), data engineers, data scientists to implement and support data solutions that support common functions in the data lifecycle e.g. data onboarding, data distribution.; Contribute, influence, and validate the design of lifecycle data solutions to ensure they meet both business and operational needs.; Manage day-to-day operations for lifecycle data solutions such as collation of metadata, analysis of system operations data (e.g. data quality metrics, usage level of data distribution services).; Perform data access control operations to ensure data ingested and distributed comply with enterprise data governance and data handling standards.; Function as the Center of Excellence for the common data lifecycle services offered, defining standards and best practices, and designing process workflows to smoothen operationalization and to ensure compliance.; Monitor, analyse, investigate, and resolve day-to-day operational incidents and provide advisory to users.; Identify opportunities for continuous improvement in the assigned area of work.; Manage and maintain strong stakeholder relationship to ensure continuous support and knowledge for existing and future data lifecycle needs.; Requirements:; CAT-1 cleared\/eligible candidate is required for this role. (Only Singapore Citizen due to high security project); Possess a degree in Accounting, Finance, Engineering, Computer Science\/Information Technology or related fields from recognized tertiary institution.; 3 to 7 years of relevant experience in data management, data analytics, data engineering user. Preferably within investment and banking industry.; Good understanding of the principles, key controls and processes relating to data management as well as data integration and data distribution.; Good knowledge and experience in programming languages such as Python, Java, SQL etc.; Familiar with data visualization tool (e.g. Tableau, PowerBI) and data virtualization technology such as Denodo is advantageous.; Good at working with details and is meticulous for operations.; Able to design and implement solution, and perform code review.; Excellent written and verbal communication skills.; Strong interpersonal skill to interact with diverse stakeholders.; Agile, fast learner and able to adapt to changes.; Good team player, with strong analytical skill and enjoy complex problem solving; Experience in the Systems Development Life Cycle implementation methodology (SDLC) and\/or Agile methodologies like Scrum and Kanban.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"83923444","Role":"Data Engineer, ITD (1 year contract)","Company":"Public Service Division","Location":"Singapore","Publish_Time":"2025-06-17 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/83923444","job_desc":"[What the role is]; About EDB; The Singapore Economic Development Board (EDB), a government agency under the Ministry of Trade and Industry, is responsible for strategies that enhance Singapore\u2019s position as a global centre for business, innovation, and talent. We undertake investment promotion and industry development, and work with international businesses, both foreign and local, by providing information, connection to partners and access to government incentives for their investments. Our mission is to create sustainable economic growth, with vibrant business and good job opportunities for Singapore.; For more information on EDB, please visit www.edb.gov.sg; Why join EDB?; As a Data Engineer, you will be assisting in the maintenance and support of EDB\u2019s data products. You will be working closely with our divisions on their needs to leverage data for strategy formulation, policy implementation and decision making.; The work streams that you could be involved include:; \u2022 Data Science: You will work closely with our Data Scientist(s) to procure and prepare the data, and to convert them into operating models for businesses\u2019 day-to-day use; \u2022 Business Intelligence: You will assist in the integration and structuring of data from various sources to create interactive, real-time dashboards for decision making.; \u2022 Data Architecture and Governance: You will assist in defining scalable data architecture for EDB\u2019s analytics and reporting needs, building data pipelines and related elements of the architecture, and developing the governance to ensure data quality.; [What you will be working on]; Your roles and responsibilities would include the following:; Support the daily operations of the data platform (e.g. handling error notifications, job monitoring & recovery, testing data pipelines);; Investigate and troubleshoot reported incidents related to technical setup (e.g. connectivity failure, API timeout, service failure);; Document changes to existing setup (e.g. data sources, pipelines, accounts);; Research, propose and document technical requirements;; Review and implement fixes for reported security vulnerabilities;; Collaborate with the infrastructure team on the data team requests;; Support product managers in cloud transformation journey for data products;; Provide technical support and consultancy to business users;; Develop reports to monitor usage, performance and security events;; Generate mockup data and create unit tests;; Set up DevOps pipelines;; Optimise performance of data pipelines;; Review data pipelines to ensure adherence to data management standards, policies and procedures.; JOB REQUIREMENTS; To meet the challenges of this role, you must have\/ be:; Minimum of Bachelor\u2019s Degree in Computer Science, Computer Engineering, or related disciplines;; 5-8 years of work experience in Data Engineering; AWS experience in implementing or operating a data management solution for analytics; SQL scripting experience to analyze, transform and integrate data sources;; Proficient in building a data pipeline using Python and pySpark; Ability to work effectively under time constraints and potentially changing priorities, while maintaining a high level of attention to details;; Ability to work independently; and; Singaporean.; ; Good to have:; Experience in developing dashboards;; Experience IT infrastructure (server, database, network administration experience is an advantage);; Experience or certification in Tableau Server administration;; Experience or certification in Talend administration;; Experience or certification in MS SQL Server administration;; Proficient in PowerShell;; Experience in SHIP-HATS or DevOps tools;; Proficient in Terraform or IaC;; Experience in dashboard UX design;; Experience with agile or other rapid application development methodologies;; [What we are looking for]","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"84921566","Role":"Data Engineer\/Senior Data Engineer, DXD (Digital Excellence & Products...","Company":"Public Service Division","Location":"Singapore","Publish_Time":"2025-06-17 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/84921566","job_desc":"[What the role is]; The Government Technology Agency (GovTech) is the lead agency driving Singapore\u2019s Smart Nation initiatives and public sector digital transformation. As the Centre of Excellence for Infocomm Technology and Smart Systems (ICT & SS), GovTech develops the Singapore Government\u2019s capabilities in Data Science & Artificial Intelligence, Application Development, Smart City Technology, Digital Infrastructure, and Cybersecurity.; At GovTech, we offer you a purposeful career to make lives better. We empower our people to master their craft through continuous and robust learning and development opportunities all year round. Our GovTechies embody our Agile, Bold and Collaborative values to deliver impactful solutions.; GovTech aims to transform the delivery of Government digital services by taking an \"outside-in\" view, putting citizens and businesses at the heart of everything we do.; Play a part in Singapore\u2019s vision to build a Smart Nation and embark on your meaningful journey to build tech for public good. Join us to advance our mission and shape your future with us today!; Learn more about GovTech at tech.gov.sg.; [What you will be working on]; ; \"To Mould the Future of Our Nation\"; At MOE, we believe in enabling every learner to thrive in a rapidly changing world. As part of our mission, we are building internal AI capabilities to improve student learning outcomes, enhance educator productivity, and strengthen our ability to innovate sustainably. Through the responsible and meaningful application of AI, we aim to advance personalised learning, support teaching, and transform educational operations.; As a Data Engineer, you will play a key role in shaping MOE\u2019s AI capabilities by leading the evaluation, optimisation, and deployment of AI models for education. You will partner closely with product managers, engineers, curriculum specialists, and policy teams to design solutions that are pedagogically relevant, technically robust, and ready for scale.; Our Team; You will be part of the Digital Excellence & Products Division (DXD), a cross-functional team driving MOE\u2019s digital transformation across platforms, policies, and products. Our Forward-Deployed AI\/Data Science Team focuses on rapidly applying AI to real-world problems in education \u2014 from personalised learning and curriculum support to school operations.; What you will be working on:; Translate data requirements from business users into technical specifications.; Collaborate with partner agency\u2019s IT teams on technology stack, infrastructure and security alignment.; Build out data product as part of a data team:; Architect and build ingestion pipelines to collect, clean, merge, and harmonize data from different source systems.; Day-to-day monitoring of databases and ETL systems, e.g., database capacity planning and maintenance, monitoring, and performance tuning; diagnose issues and deploy measures to prevent recurrence; ensure maximum database uptime;; Construct, test, and update useful and reusable data models based on data needs of end users.; Design and build secure mechanisms for end users and systems to access data in data warehouse.; Research, propose and develop new technologies and processes to improve agency data infrastructure.; Collaborate with data stewards to establish and enforce data governance policies, best practices and procedures.; Maintain data catalogue to document data assets, metadata and lineage.; Implement data quality checks and validation processes to ensure data accuracy and consistency.; Implement and enforce data security best practices, including access control, encryption, and data masking, to safeguard sensitive data; [What we are looking for]; A Bachelor\u2019s Degree, preferably in Computer Science, Software Engineering, Information Technology, or related disciplines. ; Deep understanding of system design, data structure and algorithms, data modelling, data access, and data storage.; Demonstrated ability in using cloud technologies such as AWS, Azure, and Google Cloud.; Experience in architecting data and IT systems.; Experience with orchestration frameworks such as Airflow, Azure Data Factory.; Experience with distributed data technologies such as Spark, Hadoop.; Proficiency in programming languages such as Python, Java, or Scala.; Proficiency in writing SQL for databases\\; Familiarity with building and using CI\/CD pipelines.; Familiarity with DevOps tools such as Docker, Git, Terraform.; Preferred requirements:; Experience in designing, building, and maintaining batch and real-time data pipelines. ; Experience with Databricks.; Experience with implementing technical processes to enforce data security, data quality, and data governance.; Familiarity with government systems and government's policies relating to data governance, data management, data infrastructure, and data security.; Our employee benefits are based on a total rewards approach, offering a holistic and market-competitive suite of perks. These include leave benefits to meet your work-life needs and employee wellness programmes. ; We champion flexible work arrangements (subject to your job role) and trust that you will manage your own time to deliver your best, wherever you are, and whatever works best for you. ; Learn more about life inside GovTech at go.gov.sg\/GovTechCareers.; Stay connected with us on social media at go.gov.sg\/ConnectWithGovTech.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85167337","Role":"Contract Data Engineer (2-year contract)","Company":"Public Service Division","Location":"Singapore","Publish_Time":"2025-06-17 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85167337","job_desc":"[What the role is]; [What the role is]; The role of the Data Engineer is to collaborate with the existing team of data scientists, data engineers and analysts to create data tools, develop data ingestion and processing pipelines, ensuring optimized data processing, and ensuring that data systems meet STB's business requirements. The role requires working closely with the data science team to set up and deploying data pipelines to support machine learning models and analytics scripts, developing data integrations, assembling complex datasets and implementing process improvements. The Data Engineer plays a key role in enhancing data reliability and quality while ensuring scalable business processes and supporting the team's data-related initiatives.; [What you will be working on]; [What you will be working on]; 1. Project Management; a) Project manage and work closely with vendors and internal stakeholders to deliver on data engineering related implementations ensuring that deliverables and objectives are met within agreed scope and timelines.; b) Collaborate with cross-functional teams, including data scientists, data engineers, DevOps engineers, product managers, business analysts and business stakeholders, to integrate and deploy models into current analytics platforms and production systems.; c) Plan, execute and monitor project milestones and ensure timely update to management on project progress and issues.;   2. Application of Engineering Disciplines in Support of Strategic Business Objectives; a) Prepare, process, cleanse and verify the integrity of data collected for analysis.; b) Design, develop and implement self-managed data processing and compilation pipelines related to key enterprise data domains so that data compilation business logic can be managed and maintained in-house to retain agility in responding to changing operational needs.; c) To review the design and implementation of data pipelines developed by the vendor to ensure that they meet the operational requirements of STB\u2019s business and are integrated back to the self-managed data compilation pipelines for a seamless data processing and compilation process.; d) Work closely with vendors and internal stakeholders to project manage and coordinate Data Science & Analytics's (DS&A) data ingestion and data processing pipelines across platforms which can include mobile apps, SaaS platforms, on-premise and partner systems; e) Help architect DS&A\u2019s data integrations and data processing flows between external \/ 3rd party data sources, AWS Cloud datawarehouses (e.g. Redshift, RDS) and internal on-premise systems for workloads at scale; f) Provide guidance to internal teams on best practices for Cloud data integrations; g) Identify, design and implement internal process improvements: automating manual processes, optimising data delivery, re-designing infrastructure for greater scalability, etc.; h) Develop monitoring toolkits to ensure that integration is executed successfully and alerts where integrations have failed; i) Implement best practice DataOps processes to ensure continuous integration, deployment and governance of our data pipelines across the entire data lifecycle from data preparation to reporting.;   3. Data Integration and Data Management; a) Collaborate with current team to review the existing data integration processes and make improvements to the current data processing pipelines.; b) Work with data and agency partners to assemble large, complex datasets that meet functional and non-functional business requirements.; c) Provide inputs to the design and development of an integrated data model to allow analysis across multiple structured and unstructured datasets.; d) Recommend different ways to constantly improve data reliability and quality, including helping review and enhance the existing data collection procedures to include data for building analytics models relevant for industry transformation; e) Analyse and assess the effectiveness and accuracy of data sources (e.g., datasets received from stakeholders) and ensure that they meet STB's Data Quality standards.; [What we are looking for]; Strong project management, planning, time management and organisational skills.; Experience supporting and working with cross-functional teams in a dynamic environment.; Experienced data pipeline builder and data wrangler who enjoys optimising data systems and building them from ground up.; Experience in using Qlik Sense and AWS services (e.g., SageMaker, Athena, RDS, ECR, ECS, EMR, Lambda, Redis) will be advantageous.; The following certifications would be advantageous:; Certified AWS Cloud Architect \/ Data Engineer \/ DevOps Engineer; Certified Qlik Sense Data Architect; Degree from a recognised university in a quantitative or engineering discipline: Computer Science, Computer Engineering, Informatics \/ Information Systems, Applied Mathematics or Statistics.; At least 5 years of work experience in a related field with demonstrable skills in developing, deploying and maintaining data workflows.; Proven track record in managing internal and external stakeholders and delivering on objectives according to project timelines and successfully deploying at least 1 medium to large scale analytics system.; Good command of written and spoken English with good presentation and communication skills with ability to express complex ideas, data \/ concepts and outcomes of analysis clearly to business audiences.; Strong analytical skills with a good eye for detail and possess an aptitude\/experience in solving engineering problems to produce quality deliverables.; Ability to integrate and synthesise research and data across multiple sources to derive meaningful conclusions.; Experience working with structured and unstructured datasets is essential.; Proficient in statistical programming tools (e.g., R, Python), and database scripting languages (e.g., SQL - DQL, DML, DDL); Experience with DataOps and deploying models and data workflows through DevOps process will be advantageous; [What we are looking for]","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85783657","Role":"Lecturer (Data Analytics & Data Engineering)","Company":"DigiPen Institute of Technology Singapore Pte Ltd","Location":"Punggol","Publish_Time":"2025-07-15 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85783657","job_desc":"Position; Full-time Lecturer (Data Analytics & Data Engineering) in the Department of Continuing Education, DigiPen Institute of Technology Singapore.; Description; We are seeking a seasoned professional to teach and develop a combined curriculum covering the end-to-end data lifecycle\u2014from data collection and preprocessing through analytics, visualization, pipeline design, and big-data engineering. The successful candidate will design and deliver lectures, hands-on labs, and project-based assignments to equip students with practical skills and theoretical foundations.; Key Responsibilities; \u00b7 Develop and deliver course materials covering core concepts and practical applications in data analytics, data engineering, machine learning, and related areas of data science.; \u00b7 Design and grade practical lab exercises and real-world projects.; \u00b7 Mentor and provide feedback to students on assignments and projects.; \u00b7 Collaborate with industry partners to keep curriculum aligned with current best practices.; \u00b7 Assess student performance and maintain accurate records.; \u00b7 Participate in curriculum review and continuous improvement initiatives.; Required Qualifications; \u00b7 Bachelor\u2019s or Master\u2019s degree in Computer Science, Data Science, Engineering, or a related field.; \u00b7 A minimum of 3 years of industry experience in data analytics and data engineering roles for candidates with a Master\u2019s degree, or 5 years of experience for those with a Bachelor\u2019s degree; \u00b7 ACLP certification is preferred; \u00b7 Proficiency in Python, SQL\/NoSQL databases, Apache Spark, and data pipeline tools (e.g., Kafka).; \u00b7 Experience with data visualization tools (e.g., Tableau, Power BI) and cloud platforms (AWS, Azure).; \u00b7 Strong communication and mentoring skills.; \u00b7 Passion for teaching and ability to engage learners.; ; ; Application: Review of applications will begin immediately and continue until the position is filled. For a complete application, please email the following documents:; \u00b7 A letter of interest,; \u00b7 Curriculum vitae,; \u00b7 Transcript of highest degree earned, and; \u00b7 Portfolio showcase (if available); Electronic submission is required with documents in Word or PDF format attached to an email with a subject line \u201cFull-time Lecturer (Data Analytics & Data Engineering) Position in CE\u201d sent to:; Ms. Priyanka Bhoyar; Program Manager for Continuing Education; Email: priyanka.bhoyar@digipen.edu; singapore-jobs@digipen.edu; You will be required to present your original transcripts and certificates if you are shortlisted for an interview, which is an integral part of the hiring process.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85702527","Role":"Senior Data & Integration Architect","Company":"Singapore Telecommunications Limited","Location":"Singapore","Publish_Time":"2025-07-09 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85702527","job_desc":"An empowering career at Singtel begins with a Hello. Our purpose, to Empower Every Generation, connects people to the possibilities they need to excel. Every \"hello\" at Singtel opens doors to new initiatives, growth, and BIG possibilities that takes your career to new heights. So, when you say hello to us, you are really empowered to say\u2026\u201cHello BIG Possibilities\u201d.; Be a Part of Something BIG!  ; The Senior Data & Integration Architect plays a critical role in designing and implementing the data and integration layer of Singtel\u2019s next-generation AI platform. This role focuses on orchestrating secure, scalable, and interoperable data architectures and API integration frameworks that support advanced AI and ML workloads across the enterprise.; You will work closely with the AI architecture, security, platform, and business teams to ensure seamless data movement, governance alignment, and real-time system interoperability for serving as the bridge between distributed data systems, cloud services, and AI-enabled applications.; Make An Impact By; Design, build and implement enterprise-wide data and API integration frameworks to support AI\/ML platforms across hybrid cloud and on-premise environments; Work with system owners and data domain leads to design and deliver scalable end-to-end data flows across operational, analytical, and AI systems; Define and develop secure, reusable API interfaces (REST, GraphQL, event-driven) and data interface (batch or streaming) that enable seamless interoperability between internal systems and AI services; Oversee and evaluate new data integration approaches and pipeline designs to ensure efficient, secure, and scalable data flow between data sources and AI platforms.; Collaborate with Security and Data Governance teams to ensure integration designs align with compliance, privacy, and policy requirements (e.g., PDPA, data classification); Design and enable data access strategies for LLMs and agent-based workflows, ensuring context-rich, real-time connectivity to distributed enterprise systems; Implement and maintain integration middleware and tooling (e.g. Kafka, Azure ML\/Foundry, Databricks, etc) to support data orchestration, synchronization, and reliability; Contribute integration expertise to data or AI experimentation, PoCs, and platform upgrades, ensuring architectural consistency and production-readiness; Define and enforce data and integration design standards, focusing on scalability, resilience, observability, and system decoupling; Work closely with business units, IT, and Networks to align integration plans with enterprise priorities and ensure successful data exchange across functional boundaries; Skills to Succee; Bachelor\u2019s in Computer Science, Engineering, Data, AI\/ML, or related field.; At least 3 years of experience in data architecture, system and API integration engineering.; Demonstrated experience in designing integration flows for large-scale, real-time systems across cloud and legacy environments.; Experience in designing and implementing data integration frameworks across hybrid cloud and on-premise environments, including building scalable and secure data pipelines for AI\/ML platforms.; Proficient in data integration design, with solid knowledge of data pipelines, data lakes, data warehouses, and data lakehouse architectures.; Good knowledge of modern data orchestration and middleware tools such as Apache Kafka, Azure Data Factory, Databricks, Airflow, and experience in managing data flow between operational, analytical, and AI environments.; Working knowledge of data security, data protection and data quality management including implementation of encryption, RBAC, masking, and alignment with regulatory frameworks such as PDPA and internal data classification policies.; Proven experience integrating data systems with AI\/ML workflows, including model training, serving, monitoring, and enabling context-aware access for LLMs and agent-based automation.; Effective collaboration skills to work across data, platform, machine learning engineering and API integration teams, with a clear communication style to bridge business and technical stakeholders; Good internal (IT, Networks, business) and external (suppliers, government) stakeholders management skills; Strong technical writing and presentation skills, with the ability to communicate complex concepts clearly to both technical and non-technical stakeholders.; Proactive and fast learner with a strong drive to stay current on emerging technologies and industry trends.; Rewards that Go Beyond; Full suite of health and wellness benefits  ; Ongoing training and development programs  ; Internal mobility opportunities; Your Career Growth Starts Here. Apply Now!; We are committed to a safe and healthy environment for our employees & customers and will require all prospective employees to be fully vaccinated.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"83448441","Role":"ART 1284 Data Engineer","Company":"FPT Asia Pacific Pte Ltd","Location":"Singapore","Publish_Time":"2025-06-17 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/83448441","job_desc":"Responsibilities:; Develop and maintain data pipelines and ETL\/ELT processes, and ensuring maintainability through unit and integration testing.; Collaborate with data teams to understand requirements and automate deployment and monitoring.; Optimize data storage and troubleshoot issues to enhance performance.; Skillset:; Possess a degree in Computer Science\/Information Technology or related fields.; At least 5 years of relevant working experience in software or data engineering with proficiency in Python.; Strong experience in unit and integration testing.; Familiarity with DevOps practices and Agile methodologies.; Strong software engineering, analytical and problem-solving skills.; Experience in AWS and Kubernetes (K8s).; Familiarity with data platforms such as Snowflake, Apache Spark, or Apache Hive, as well as orchestration tools like Apache Airflow, Dagster, or Prefect.; Familiarity with GitHub workflows and Datadog.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85579388","Role":"Engineer, Data Science & AI (12 Months Contract)","Company":"Thales","Location":"Queenstown","Publish_Time":"2025-07-07 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85579388","job_desc":"About Us:; At Thales Research and Technology (TRT), we extend the influence of Thales within the scientific and technical communities, offering a platform for innovation and knowledge sharing, and attracting talented research engineers to further develop our expertise. Together with our academic partners in the universities, we deliver impactful innovations that prepare for future applications.; We are seeking a talented and driven Engineer\/Senior Engineer specializing in Data Science & AI, with a strong emphasis on computer vision and machine learning, to join our dynamic team. In this role, you will contribute to our pioneering projects in intelligent systems that transform operational data into actionable information that enable End Users to make trusted decisions.; To be successful in this role, you should possess curiosity to explore the State-of-the-Art, willingness to challenge the status quo, being open-minded to new possibilities and be a team player in a dynamic research environment. You should thrive on self-learning and value hands-on experience in upskilling. You should be proactive to propose and champion new ideas towards stakeholders and create new values from the application of Data Science & AI in Thales business domains.; Responsibilities:; \u2022 Develop and implement computer vision algorithms and machine learning models for a range of applications, including object detection, change detection, anomaly detection, multiclass classification, behaviour analysis, and video stitching and scaling.; \u2022 Contribute to cross-functional teams developing proof-of-concepts and conduct experiments in realistic environments to validate algorithms, optimize performance, and ensure the robustness of models in real-world applications.; \u2022 Able to generate good quality documentation and capture patentable inventions.; \u2022 Stay current with the latest advancements in AI, machine learning, and computer vision technologies to continually enhance our capabilities.; \u2022 Motivated to share knowledge via presentations in internal and external events.; \u2022 In the case of a Senior Engineer profile, act as the computer vision subject matter expert and provide technical guidance and mentorship in the team.; Requirements:; \u2022 Bachelor\u2019s or Master\u2019s degree in Engineering, Computer Science, or a related field, with specialisation in Data Science and AI.; \u2022 Hands-on experience in computer vision and machine learning, with a diverse portfolio of projects.; \u2022 Proficiency in programming languages such as Python, C++, or C, and experience with machine learning frameworks (e.g., TensorFlow, PyTorch, OpenCV) and MLOps concepts (e.g. KubeFlow).; \u2022 Solid understanding of deep learning architectures and algorithms like CNN and vision transformer, particularly in the context of image and video analysis.; \u2022 Strong analytical skills and the ability to work with large datasets.; \u2022 Excellent communication skills, with the ability to present complex technical concepts to non-technical stakeholders.; \u2022 Experience in a collaborative, team-oriented environment. Desired Skills:; \u2022 Familiarity with cloud computing platforms and big data technologies.; \u2022 Knowledge of embedded systems and real-time processing.; \u2022 Experience in specific domains such as aerospace, automotive, defence, or security is a plus.","salary":"","work_type":"Kontrak\/Temporer","country":"singapore"}
{"Job_ID":"85013526","Role":"Data Architect - Kubernetes & Big Data","Company":"PLOY ASIA PTE. LTD.","Location":"Singapore","Publish_Time":"2025-06-18 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85013526","job_desc":"Data Architect - Kubernetes & Big Data (Banking); ; About the Role:; We're hiring a Data Architect to design and lead the deployment of a secure, scalable big data platform for a major banking environment. The role involves working across on-prem and Azure cloud infrastructure, with a focus on Kubernetes, Spark, Kafka, and enterprise-grade data governance.; Location: Singapore, Onsite; Employment Type: Contract - with a view of extension; Industry: IT Consultant; ; Key Responsibilities:; Architect and manage hybrid big data platforms using Kubernetes (on-prem & AKS); Deploy and optimize Apache Spark, Kafka, and Ranger; Design logical data layers using ADLS Gen2 (Hot\/Cold) via S3 protocol; Lead data migrations across Hadoop and Azure (1-5+ PB scale); Enforce data security: TDE, TLS, RBAC, and Protegrity tokenization; Integrate with monitoring tools (Grafana) and observability platforms; Define CI\/CD pipelines with Azure DevOps (ADO); Collaborate with InfoSec for compliance and audits; ; Qualifications:; 10+ years in data\/platform architecture; Deep expertise in Kubernetes, Azure, Spark, Kafka, and CI\/CD; Strong understanding of data security, regulatory compliance, and governance in banking; Proven experience with large-scale data migrations and hybrid cloud platforms; ; Nice to Have:; Experience with Apache Ranger, Protegrity, or similar tools; Prior experience in financial services or regulated industries","salary":"","work_type":"Kontrak\/Temporer","country":"singapore"}
{"Job_ID":"85617821","Role":"Data Platform Engineer","Company":"International Baccalaureate Organization","Location":"Singapore","Publish_Time":"2025-07-10 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85617821","job_desc":"The Data Platform Engineer will be responsible for the design, build, maintenance, security and performance of the data analytics platform, serving both Digital Office stakeholders and lines of business.  The work will range from larger technology delivery initiatives, working on a shared Digital Office Data Analytics product backlog, providing maintenance support, and ensuring the smooth operation of live applications and platforms. This role requires hands-on development experience implementing new features and functionalities within the platform.; The International Baccalaureate provides world-class educational services to over 5500 schools across 159 countries. A career at IB is not just a job; it\u2019s an opportunity to work with an innovative world leader of education services and contribute to our 50-year mission of creating a better and more peaceful world. Apply now to join our global organization where we empower our employees to thrive and make a difference.; About the Job; Technical Design ; Analyze business requirements, understand underlying data sources, transformation requirements, data mapping, data modelling and metadata for reporting solutions.; Translate these business needs to a simple, scalable and secure technical design.; Design EDW, data mart layers with appropriate enterprise considerations like architectural fit, performance, flexibility, maintainability, automation etc.; Technical Build ; Build infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources, applications and platforms.; Play an active role in the development scrum team, delivering features with appropriate quality and velocity in the product backlog according to IB\u2019s DevOps practices including refining of user stories, acceptance criteria, write code, conduct unit testing, documentation and troubleshooting.; Maintenance & continuous improvement; Perform ITIL Incident, Problem, and Change Management practices in accordance to SLAs and follow processes.; Identify key problem areas within the application and implement improvements. Evaluate and improve existing data analytics systems.; Data Expertise ; Understand the IB\u2019s main business processes and how it relates to data that is generated or captured.; Understand associated data flows and dependencies between different enterprise systems.; About You; BSc\/BA in Computer Science, Engineering or relevant field.; Able to integrate multiple data sources & user-end applications with databases into one system. (to store the data and its retrieval from the databases).; Solid experience in designing and implementing robust data pipelines and ETL\/ELT framework.; Proven experience as a data warehouse architect & developer, including full implementation of data warehousing solution.; Experience in data engineering solutions built on modern data lake or Lakehouse architectures, including Delta Lake or equivalent frameworks e.g. Microsoft Fabric.; In-depth understanding of database management systems, online analytical processing (OLAP), SQL queries (Azure SQL DB).; Expertise with Azure Resource Management and templates is an added advantage.; Exposure to cloud technologies (MS Azure, AWS) & desire to learn and deliver new things on a needs-basis. (big data, BI, data science, etc.).; Strong expertise in data warehouse design methodologies and technologies, data modelling (Data Vault modelling methodology experience is preferable), data quality and metadata.; Ability to work within a fast-paced environment to meet deadlines, multitask and cope with multiple activities.; In addition to your salary, we offer an attractive range of benefits including: ; 20% employer's CPF contribution ; S$1,200 yearly flexible credits; 20 Days annual leave, plus public holidays, with the choice to buy or sell up to 3 days additional annual leave\u202fusing flexible credits; Life assurance 2x annual salary ; Flexible working hours due to nature of work; Organisation sponsored learning opportunities for professional development; Corporate passes to Singapore Zoo, River Wonders and Gardens By The Bay","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85430135","Role":"Data Architect \u2013 SAS VI","Company":"THAKRAL ONE PTE LTD","Location":"Singapore","Publish_Time":"2025-07-03 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85430135","job_desc":"Role; Data Architect \/ Data Management Lead \u2013 SAS (Fraud & Investigation Focus); Job Description; We are seeking a highly experienced Data Architect \/ Data Management Lead with strong domain knowledge in fraud analytics, case management, and investigation\/intelligence systems. The ideal candidate will lead the design, development, and deployment of data integration and analytics components within SAS environments, with hands-on exposure to SAS Viya, data pipelines, and real-time streaming tools. This role is ideal for someone who thrives at the intersection of data strategy, architecture, and stakeholder collaboration.; Key Responsibilities; Collaborate with business analysts to gather and interpret data-related requirements for fraud\/case management\/investigation solutions.; Define and implement a comprehensive data architecture, including data sourcing, integration, cleansing, storage, and provisioning strategies.; Translate business requirements into technical work products across the ETL, data engineering, and reporting layers.; Design and lead batch and real-time data ingestion processes from various sources into SAS or big data platforms.; Develop data pipelines, orchestration frameworks, and visualization\/reporting structures using tools such as SAS Viya, DS2, and Python.; Lead integration and testing of developed components, and support UAT activities.; Oversee documentation, project tracking, and technical troubleshooting.; Ensure compliance with data governance, metadata, and quality management standards.; Experience and Skills Requirements; 10+ years of relevant experience in Business Analytics, Data Architecture, or Data Warehousing.; Proven end-to-end delivery of at least 5 enterprise projects, preferably involving SAS solutions in fraud, surveillance, or case management domains.; Strong hands-on experience in:; SAS Viya, SAS DS2, SQL Programming; Python, Spark, JSON, XML; Job scheduling, Linux commands, data streaming tools (Kafka\/SAS ESP); Postman\/SOAPUI, REST APIs, DevOps\/DataOps; Working knowledge of:; Data model design, source-to-target mapping; Real-time event processing, metadata management, and orchestration; Background in fraud analytics, investigative systems, or intelligence\/case management frameworks is essential.; Familiarity with tools such as SAS Intelligent Decisioning, RTDM, PEGA, or similar is a strong plus.; Experience working in government, public sector, or regulated environments is highly preferred.; Strong communication, stakeholder management, and leadership skills.; Degree in Computer Science, IT, Statistics, or related field.; Number of Vacancies; 1; Philippines","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85167492","Role":"[LTA-TRO] EXECUTIVE\/ ENGINEER, ROAD DATA MANAGEMENT","Company":"Public Service Division","Location":"Singapore","Publish_Time":"2025-06-24 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85167492","job_desc":"[What the role is]; As a Data Engineer in Roads Data Management team, you will use data engineering skills and traffic domain knowledge to onboard, prepare and enhance the various datasets needed by TRO group for analytics and reporting purposes. You will also be part of a core team of GIS professionals, data engineers and data analysts who are building the next generation of user applications and data models.; [What you will be working on]; 1) Collaborate with T-Tech Subgroup, ITCD Group and LTA Contractors to ensure the data pipelines meet TRO\u2019s requirements; 2) Collaborate with Data Analysts and Users within TRO to determine the dataset views required.; 3) Build the dataset views required together with ITCD Group.; 4) Analyse and implement data cleaning and post processing methods to ensure the data meets the required standards; 5) Assist on the development and implementation of data quality standards and governance; [What we are looking for]; 1) Knowledge in Computer Science, Geography or Engineering with an analytics \/ GIS specialisation.; 2) At least 2 years of experience in data engineering \/ analytics or GIS or other related work preferred. Those with good experience can be considered for senior positions; 3) You are familiar with relational databases and have some hands-on experience coding in SQL or you are familiar with ArcGIS geodatabases and functions.; 4) You are proficient in one or more of the programming languages such as Python or R; 5) Ability to work independently and collaboratively in a team environment; As part of the shortlisting process for the role, you may be required to complete a medical declaration and \/ or undergo further assessment.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85381719","Role":"Data Platform Engineer - RCIC","Company":"Beyondsoft International (Singapore) Pte Ltd","Location":"Singapore","Publish_Time":"2025-07-02 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85381719","job_desc":"COMPANY DESCRIPTION; Beyondsoft International (Singapore) Pte. Ltd. was set up in 2007 and established as the regional headquarters for the Southeast Asia (SEA) and European markets in September 2015. Based on our vision of \"Using technology to promote social progress, economic development and become a global customer preferred partner\" and our concept of \"Beyond your expectations\", Beyondsoft is committed to provide our customers in countries along the \"Belt and Road\" with comprehensive solutions and products and creating commercial value for customers to realizing continuous businesses development.; Our core business includes:; IT development services providing customers with IT consulting, software research and development, software and hardware testing, system integration and operation and maintenance, data analysis and other services;; New retail solutions and products through intelligent products, helping small and medium-sized enterprises (SMEs) realize the digital transformation of their daily operations;; Internet of Things (IoT) platform and solutions comprehensive use of IoT, artificial intelligence, big data, cloud computing and other technologies to provide IoT solutions for intelligent upgrades in cities, parks, buildings and industries, to create a smart future.; For more information, please visit www.beyondsoft.com.; RESPONSIBILITIES; Design, implement, and maintain platform infrastructure to support data products and automation workflows.; Automate operational workflows to improve system efficiency and reduce manual tasks.; Collaborate with cross-functional teams to support the Tableau upgrade and Glean-related initiatives.; Support Tableau server upgrades and assist in user migration, dashboard performance tuning, and configuration.; Develop and maintain scripts\/tools using Python and SQL for automation and data operations.; Build and enhance CI\/CD pipelines to streamline deployment processes.; Ensure operational stability and performance of data platforms through proactive monitoring and continuous improvement.; Document processes, contribute to platform standards, and support audit\/compliance requirements; QUALIFICATIONS; Bachelor\u2019s degree in computer science, Engineering, or a related field.; Minimum 5 years of experience with platform setup and automation of operational workflows.; Strong proficiency in Python, SQL, and experience with CI\/CD pipelines.; Strong hands-on experience with Tableau; Excellent analytical and problem-solving skills.; Beyondsoft Technology (Singapore) Pte. Ltd is committed to being an equal opportunity employer and provides equal employment opportunities to all employees and applicants. We strive to cultivate a workplace that celebrates diversity and inclusion, where individuals of all backgrounds\u2014regardless of nationality, ethnicity, religion, age, gender identity, sexual orientation, or any other distinguishing trait\u2014can succeed and thrive. We prohibit discrimination and harassment of any type with regard to race, color, religion, age, national origin, disability status, genetics, sexual orientation, gender identity, or expression. This policy applies to all terms and conditions of employment, including recruiting, hiring, and the entire employee lifecycle. We are focused on creating an environment where everyone can reach their full potential.;  Employment offers from Beyondsoft Technology (Singapore) Pte. Ltd. are contingent upon the successful completion of any required pre-employment processes, in line with applicable laws and regulations. Beyondsoft Technology (Singapore) Pte. Ltd. does not ask for any recruitment fees, nor does it request any unauthorized payments from candidates as part of the hiring process.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85442130","Role":"Associate, AI & Data Engineer","Company":"Jurong Port Pte Ltd","Location":"Singapore","Publish_Time":"2025-07-03 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85442130","job_desc":"Jurong Port\u2019s multipurpose port operating expertise includes efficient handling of general, bulk and containerized cargo, management and operations of the Tuas Offshore Marine Centre, and Lighter Terminals in Penjuru and Marina South. Besides Singapore, Jurong Port is involved in overseas joint ventures in China and Indonesia. In 2016, Jurong Port\u2019s local and overseas terminals handled close to 35 million tons of general and bulk cargo, and 560,000 TEUs of container cargo.; Overview: ; As an Associate AI & Data Engineer in Jurong Port, you\u2019ll contribute to solutioning and integration of cutting-edge AI technologies into our daily operations across diverse business units. You will be a key driver in the organisation\u2019s AI & Data Management roadmap, fostering a culture of innovation and continuous adaptation. This is an exciting opportunity to apply generative AI and agentic AI techniques in addition to Data engineering techniques to build agents that learn, adapt, and act in realistic, human-like ways. ;   Key Responsibilities: ; Vendor management and assessment of AI, Data (Big Data included) models and architectural design solutions based on business requirements, optimizing applications in areas such as NLP, computer vision, chatbots and Large Language Models (LLM).  ; Optimise data pipeline for performance, scalability, and real-time processing. ; Define clear objectives, model evaluation metrics and deployment strategies for AI solutions & Data Products and services for integration into business processes.  ; Experiment and evaluate state-of-the-art and in-house generative AI models and AI agents, analyze performance, and iterate based on findings. ; Collect feedback from users to continuously refine AI tool deployment and adoption strategies. ; Collaborate closely with product, frontend, backend, and data teams to efficiently implement AI-powered features. ; Conduct data and AI workshops\/ trainings to cultivate JP staffs in using analytical and AI tools for operations and planning. ; Participate in AI security and compliance assessments to ensure compliance with data privacy regulations and industry standards. ; Communicate technical AI concepts clearly to cross-functional teams and stakeholders.  ; Research new AI technologies and tools, driving the innovative application of AI in the product. ; Generate and implement relevant codes for data transformation (ETL), normalisation and aggregation tasks with relevant Data quality monitoring recommendations.  ; Key Deliverables ; Implement automated testing and monitoring techniques to ensure the accuracy and reliability of AI systems. ; Develop and execute comprehensive AI deployment and maintenance strategies and plans for AI tool adoption, tailored to the unique needs of different business units. ; Develop, document and deploy relevant data models for the purpose of data management, analysis and visualisation in line with business needs. ; Qualifications: ; Bachelor degree in Applied AI\/ML, Data Science, Computer Science or a related field. ; Strong foundation in Python, with experience using AI frameworks such as TensorFlow, PyTorch, and Hugging Face Transformers. ; Strong foundation in programming languages such as Python, SQL or Java. ; Experience with LLMs, generative AI (OpenAI, LangChain), or deep learning will be a plus. ; Relevant certifications in AI\/ML fundamentals or cloud platforms (e.g.,AWS Certified Machine Learning, Google Cloud AI Engineer). ; Strong analytical mindset with problem-solving skills, adaptable, eager to learn, and enthusiastic about new technologies.  ; Benefits: ; Hands-on experience working with state-of-the-art AI tools that are critical for organization's success in 5-years AI roadmap.  ; Mentorship and guidance from experienced AI & data management professionals.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"80701492","Role":"Data Engineer (Credit Assessment) (In Partnership with IMDA)","Company":"Phillip Securities Pte Ltd","Location":"Singapore","Publish_Time":"2025-06-17 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/80701492","job_desc":"Responsibilities:; Design and implement data pipelines to collect, store, and process data required.; Ensure data quality and integrity.; Develop APIs for integrating the credit assessment system with other applications.; Work with platform developers and product owners for the integration.; Requirements:; Degree in Computer Science, Data Science, Mathematics or a related IT field; Excellent time management, prioritization, and multitasking skills.; Strong interpersonal and communication skills.; Team-oriented, self-motivated and adaptable.; If you are looking for an environment of growth and opportunities, please write in with a detailed resume stating the position applied and expected salaries to the HR department via recruitment@phillip.com.sg.; We regret that only shortlisted candidates will be notified.; Brought to you by Phillip Securities Pte Ltd (A member of PhillipCapital)","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85190498","Role":"Data Quality Assurance Engineer - Data Platform 2025 Start","Company":"BYTEDANCE PTE. LTD.","Location":"Singapore","Publish_Time":"2025-06-26 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85190498","job_desc":"Data Quality Assurance Engineer - Data Platform 2025 Start; Singapore Regular R&D - Testing Job ID: A184326; Responsibilities; About ByteDance Founded in 2012, ByteDance's mission is to inspire creativity and enrich life. With a suite of more than a dozen products, including TikTok as well as platforms specific to the China market, including Toutiao, Douyin, and Xigua, ByteDance has made it easier and more fun for people to connect with, consume, and create content. Why Join; Us Creation is the core of ByteDance's purpose. Our products are built to help imaginations thrive. This is doubly true of the teams that make our innovations possible.; Together, we inspire creativity and enrich life; a mission we aim towards achieving every day.; To us, every challenge, no matter how ambiguous, is an opportunity; to learn, to innovate, and to grow as one team. Status quo? Never.; Courage? Always. At ByteDance, we create together and grow together.; That's how we drive impact-for ourselves, our company, and the users we serve. Join us. About the team; The mission of the Data Business Partner (BP) Quality Assurance Team is to build a highly robust(standardized\/professional\/efficient) data quality system, ensuring the better delivery of data. Our goal is to safeguard the quality of data throughout the entire lifecycle in terms of timeliness, accuracy, and stability, empowering the enterprise's decision-making capabilities, market competitiveness, and operational efficiency. Team members possess rich backgrounds in big data technology and quality experience, along with keen data insight and a forward-looking technological perspective.; We look forward to your contribution and joining our team What you will be doing:; Responsible for quality assurance work in offline\/real-time data warehouses and data engineering, building a quality assurance system.; Develop comprehensive test plans and testing strategies based on actual business requirements, ensuring data accuracy and maintaining data engineering quality.; Quickly develop testing tools or platforms based on testing needs to improve business delivery efficiency.; Able to identify data system risks comprehensively, ensuring data system stability through means such as performance testing and online monitoring.; In-depth understanding of the business, communicate and collaborate with various roles in data business, create value for the business, and work without boundaries.; Update software, enhances existing software capabilities and develops and direct software testing and validation procedures.; Qualifications; Minimum Qualifications:; Bachelor's degree or above in computer science, mathematics, statistics, or related fields.; At least 3 years of experience in testing development\/development work, with a preference for experience in big data testing and data engineering service testing.; 3 years experience in using one programming language (JAVA\/Go\/python) for tool development, skilled in using SQL, and preferably with independent experience in platform tool development.; Familiarity with common big data technologies such as Yarn, Spark, HDFS, understanding of data ETL processes, good literacy and foundation in mathematical analysis; Strong initiative, sense of responsibility, and ability to work under pressure are required.; Preferred Qualifications:; 5 years of experience in testing development\/development work, with a preference for experience in big data testing and data engineering service testing.; 5+ years of experience working in a complex, matrixed organization involving cross-functional, and\/or cross-business projects; Experiences in data platform related product development or big data technologies (such as Hadoop, Clickhouse, Flink etc.); Sensitivity to data, excellent logical thinking, analytical skills, business understanding, communication skills, and presentation abilities.; Solid background in statistics, data mining, and modeling are preferred.; ByteDance is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives. Our platform connects people from across the globe and so does our workplace. At ByteDance, our mission is to inspire creativity and enrich life. To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach. We are passionate about this and hope you are too.; Apply; Share to","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85549955","Role":"AD, Data & AI Platform Enablement","Company":"Singapore Telecommunications Limited","Location":"Singapore","Publish_Time":"2025-07-07 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85549955","job_desc":"An empowering career at Singtel begins with a Hello. Our purpose, to Empower Every Generation, connects people to the possibilities they need to excel. Every \"hello\" at Singtel opens doors to new initiatives, growth, and BIG possibilities that takes your career to new heights. So, when you say hello to us, you are really empowered to say\u2026\u201cHello BIG Possibilities\u201d.; Be a Part of Something BIG! ; This position plays a pivotal technical role in AI & Data Analytics (AIDA) who is responsible for leading a cross-functional team of platform, data, and operations engineers to deliver a secure, scalable, and cost-efficient hybrid cloud AI infrastructure.; The role will have to work closely with architects and AI engineering teams to build a robust platform that supports a wide range of AI use cases across multiple business units.; You will collaborate with data source and platform teams from both Networks and IT to design and implement integrated data solutions that empower users to efficiently develop and scale AI applications.; Make An Impact By; Lead, manage and grow a cross functional team consisting of platform engineers, data engineers, DataOps, DevSecOps engineers, FinOps, operations and delivery managers, ensuring the successful delivery and sustainable AI platform.; Foster innovative, agile and user focused team culture to accelerate delivery of AI and advance analytics use cases; Coordinate workload within team and integration efforts with various departments across Singtel SG; Evaluate Singtel existing and latest tools available in market to build best-in-class telco data and AI tech stack; Plan unified data layer to enable availability of cross-domain data sources in single location for AI use; Design integration architecture to existing and upcoming systems across Singtel Singapore; Deliver and implement hybrid cloud AIDA platform; Monitor platform performance, availability and utilization to ensure stability; Operate and maintain data and AI platform to maximize availability and platform           ; Manage implementation timeline to meet use case delivery targets; Integrate data sources across Singtel SG efficiently to allow effective cross-domain AI and data use cases; Responsible for data quality monitoring, cataloguing, CII & PII data and sensitivity classification for AIDA; Set-up and operate AIDA\u2019s DevSecOps platform for efficient, traceable development and deployment; Constantly update and upgrade pipelines and security policies to align with latest libraries and cybersecurity recommendations; Implement solutions to continuously optimize compute resource and data transfer cost; Manage platform and cloud cost (FinOps) for cost effective AI operations; Skills for Success:; Bachelor\u2019s degree in computing, engineering or relevant fields; 5+ years experience in design, implementation and operation of hybrid cloud data engineering platform; 1-2 years of team leadership or delivery management experience; Capable of designing hybrid cloud data and AI system; Experience in operation of hybrid cloud data and AI systems; Proficient in data ingestion and integration development (Spark, Kafka, Hadoop, Azure Storage, etc.); Experience in workload optimization and FinOps to minimize computing cost; Good knowledge of DevSecOps processes and tools; Demonstrated good project management skills in implementation of data\/system projects; Rewards that Go Beyond; Full suite of health and wellness benefits ; Ongoing training and development programs ; Internal mobility opportunities; Are you ready to say hello to BIG Possibilities?; Take the leap with Singtel to unlock new opportunities and accelerate your growth. Apply now and start your empowering career!","salary":"","work_type":"","country":"singapore"}
{"Job_ID":"85690813","Role":"Backend Software Engineer (SRE) - Cloud Infrastructure","Company":"BYTEDANCE PTE. LTD.","Location":"Singapore","Publish_Time":"2025-07-12 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85690813","job_desc":"Backend Software Engineer (SRE) - Cloud Infrastructure; Singapore Regular R&D Job ID: E5747; Responsibilities; The team is responsible for infrastructure systems, including Storage\/Computing\/DB. We aim to be the leading SRE team across the industry. In the SRE team, you will have the opportunity to manage the complex challenges of scale, while using expertise in coding, algorithms, complexity analysis, and large-scale system design. We embrace a culture of diversity, intellectual curiosity, openness, and problem-solving. We also encourage ownership, self-governance and independence to work on various projects, and an environment that provides the support and mentorship needed to learn and grow as an engineer. What you will be doing: 1. Reliability: Ensuring the reliability and efficiency of our core infrastructure, focusing on system capacity and stability; setting up reliability standards and recovery SOP. 2. Reliability: Troubleshooting and locating the technical issues, bottleneck analysis, managing system high availability architecture transformation and upgrading. 3. Efficiency: Building automated operation solutions for large-scale systems; partnering with system development teams for system iteration. 4. Efficiency: Designing and implementing software platforms and monitoring frameworks for efficient, automated, and intelligent service-oriented architecture (SOA) governance. 5. Cost: There are millions of CPUs. We should build delivery standards, and monitor and budget systems to optimize the cost of the company. 6. Compliance: Designing and setting up new IDC; designing and implementing data protection plan to meet the standard requirement.; Qualifications; Minimum Qualifications: - Bachelor's \/ Master's Degree in Computer Science or related major, with at least 5 years of relevant experience; - Solid basic knowledge of computer software, understanding of Linux operating system, storage, network IO and other related principles. - Familiar with one or more programming languages, such as Python, Go, and Java. Knowledge of design patterns and coding principles is necessary. Preferred Qualifications: 1. Experience with storage, and relevant system experience with the following: KV, Table, Graph, Redis, MySQL, MongoDB, MQ, and Kafka. 2. Experience with computing & big data, and system experience with the following: Kubernetes, Docker\/Containers, AIops, Spark, Flink, Function as a service, RPC Framework, and Service Mesh.; Job Information; Inspiring creativity is at the core of ByteDance's mission. Our innovative products are built to help people authentically express themselves, discover and connect \u2013 and our global, diverse teams make that possible. Together, we create value for our communities, inspire creativity and enrich life - a mission we work towards every day.; As ByteDancers, we strive to do great things with great people. We lead with curiosity, humility, and a desire to make impact in a rapidly growing tech company. By constantly iterating and fostering an \"Always Day 1\" mindset, we achieve meaningful breakthroughs for ourselves, our Company, and our users. When we create and grow together, the possibilities are limitless. Join us.; Diversity & Inclusion; ByteDance is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives. Our platform connects people from across the globe and so does our workplace. At ByteDance, our mission is to inspire creativity and enrich life. To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach. We are passionate about this and hope you are too.; Apply; Share to","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"80654795","Role":"Data Platform Engineering Lead","Company":"National University of Singapore","Location":"Singapore","Publish_Time":"2025-06-17 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/80654795","job_desc":"Apply now","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85578458","Role":"Senior Engineer, Data Management","Company":"MODEC Offshore Production Systems (S) Pte Ltd","Location":"One North","Publish_Time":"2025-07-07 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85578458","job_desc":"Job Description; \u2022 Manage the Information and Data hand-over to the client of project information and documentation in accordance with the agreed procedures; \u2022 Monitor all Information and Data interfaces with Client or other third parties to ensure requirements are addressed, and coordinate with adequate stakeholders if necessary; Coordination with stakeholders; \u2022 Coordinate\/Advise all the project teams (e.g. Engineering, Supply Chain, Completion & Commissioning, and subcontractors) to ensure information and data requirements are understood and complied with; o Coordinate with Engineers or each discipline members to ensure the information\/data quality within the schedule; o MODEC started to focus on complying with Information and Data Management requirements, and understanding from different departments\/stakeholders are crucial, so we need someone who can really involve departments and work closely to build not only protocols but also an organizational understanding and culture towards Information and Data Management; \u2022 Coordinate with Digital\/IT team to ensure that the Project DB and Tools are properly setup as per project requirement; o Enterprise Data Platform\/Data Management Program and Information and Data Management related tools are under development from scratch in MODEC, hence great collaborator to provide necessary input\/feedback to Digital\/IT team is required (need wholistic and long-term view to contribute to the enterprise level architecture, rather than solely standing on the project requirements view); Quality Assurance \/ Reporting; \u2022 Identify issues related to data quality, and coordinate with\/support stakeholders populating the data to eliminate the issues; \u2022 Monitor data quality KPIs to reflect the Project Information and Data Management status; Job Requirements; \u2022 8-10 years experiences or Information Management position in Oil & Gas filed EPC contractor; \u2022 Background of either Mechanical, Piping, Electrical or Instrument Engineer experience in EPC contractor would be a big plus; \u2022 Background of Mechanical Completion & Commissioning Coordinator, CMMS Engineer or Application Engineer (AVEVA); \u2022 Engineering\/Science bachelor or master\u2019s degree; \u2022 Good understanding of systems and software related EPCI and O&M (e.g. AVEVA, SAP, EDMS, Completion, CMMS); \u2022 Communicative\/ great team player; \u2022 Stakeholder management skills; \u2022 Familiar with Digital transformation","salary":"","work_type":"Kontrak\/Temporer, Full time","country":"singapore"}
{"Job_ID":"85702508","Role":"Senior AI Platform Engineer","Company":"Singapore Telecommunications Limited","Location":"Singapore","Publish_Time":"2025-07-09 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85702508","job_desc":"An empowering career at Singtel begins with a Hello. Our purpose, to Empower Every Generation, connects people to the possibilities they need to excel. Every \"hello\" at Singtel opens doors to new initiatives, growth, and BIG possibilities that takes your career to new heights. So, when you say hello to us, you are really empowered to say\u2026\u201cHello BIG Possibilities\u201d.; Be a Part of Something BIG!; Lead design, implementation and initial operation of data and AI platform for AIDA; Build hybrid architecture on Microsoft Azure with necessary integration with on-premise systems; Make decisions in adopting existing tools vs introducing new elements to realize best-in-class telco AI tech stack; Perform hands-on technical implementation and operations support during the platform\u2019s early phase; Lay groundwork for future-ready, scalable platform and grow high-performing technical team; Collaborate with IT, Networks, AIDA developers to align platform capabilities with their needs.; Make An Impact B; Jointly design with AI architecture team and implement overall architecture and technology stack for AIDA platform; Evaluate existing on-premise and cloud components for reuse or replacement; Select appropriate Azure services and open-source\/third party elements to meet scalability, cost, performance and timeline needs; Ensure seamless integration between AIDA\u2019s Azure platform with other Azure and on-premise systems; Lead hands-on implementation of core platform capabilities including storage, analytics, ML and GenAI elements; Establish Infrastructure-as-Code (IaC) practices for platform deployment and automation; Manage platform delivery roadmap and ensure timely execution of deliverables; Oversee platform health, security, access controls, and cost optimization; Act as the first-line platform operations lead during early-stage rollout; Work closely with AI architecture, data engineering, developers, and business teams to align platform capabilities with their needs.; Skills to Succeed; Bachelor\u2019s or Master\u2019s degree in Computer Science, Engineering, or a related field; 5 years of experience in data engineering, platform architecture, or infrastructure roles; 1 year in a senior technical role delivering cloud-based data platforms; Deep hands-on knowledge of Azure data services and Databricks; Solid understanding of on-premise infrastructure integration (e.g., network, identity, hybrid storage, firewall, Azure Arc); Proven experience with IaC (e.g., Terraform, Bicep, ARM), for data platforms.; Strategic thinker with the ability to balance technical depth and business impact; Strong communicator who can collaborate across functions and with vendors; Comfortable working in ambiguity and driving clarity in a new setup; Strong grasp of data policies and cost management in cloud environments.; Rewards that Go Beyond; Full suite of health and wellness benefits  ; Ongoing training and development programs  ; Internal mobility opportunities; Your Career Growth Starts Here. Apply Now!; We are committed to a safe and healthy environment for our employees & customers and will require all prospective employees to be fully vaccinated.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85074630","Role":"Senior Director, Data Platform Mgmt","Company":"Singapore Telecommunications Limited","Location":"Singapore","Publish_Time":"2025-06-17 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85074630","job_desc":"An empowering career at Singtel begins with a Hello. Our purpose, to Empower Every Generation, connects people to the possibilities they need to excel. Every \"hello\" at Singtel opens doors to new initiatives, growth, and BIG possibilities that takes your career to new heights. So, when you say hello to us, you are really empowered to say\u2026\u201cHello BIG Possibilities\u201d.; Be a Part of Something BIG!  ; Data has become the strategic enabler for digital transformation initiatives undertaken by Singtel. Business units are actively using data and insights to make strategic decisions and drive Go-To-Market use cases.; This role will be responsible to lead and enable business units and corporate functions the right use of data through effective data lifecycle management and efficient use of data through data engineering and innovation. The role will be partner with stakeholders across the organization to understand their business priorities and deliver data use cases across Singtel. This role will also lead, consult and deliver Singtel data architecture, technology strategy, roadmap and products usage. Provide thought leadership on data democratization and data exchange within Singtel and our business partners. Act as trusted advisors to senior management (C levels, MDs and VPs) on the emerging technologies and digital trends that are most relevant to the company's goals and evolving needs.; It comprises of variety of data functions with focus on plan, design, build and run in these areas:; Data Architecture; Data Security; Data Management; Data Integration and Engineering; Data Modeling; Data Warehousing and Reporting; Data Visualization; DataOps (including DataOps); Data Innovations; Consent Management; This role shall also be the main POC with Pune ODC, coordinating resource allocation, delivery strategies, and project management, while nurturing a culture of excellence, collaboration and continuous improvement.; Team Leadership; Project Management; Performance Management; Stakeholder Engagement; Process Improvement; Talent Development; Make An Impact By; Develop, define and govern Data & Platform Management Strategy, Architecture, Platforms, Roadmaps to support business data strategies and initiatives; Drive innovation initiatives using data and analytics; Lead and drive alignment of architecture and technology across Singtel SG; Build up new capabilities for Data & Platform Management to support business initiatives; Drive strategic data-driven transformation program in Singtel group with partnership with Data Governance and Group Data Office where we can promote the right use of data and analytics.; Collaborate and partner with business units to identify opportunities where we can drive data to business value; Proactively advise the senior management team on the emerging data technologies and digital trends that are most relevant to the company's goals and evolving needs; Transform the organization from project-driven to platform-driven data products; Build and retain core data capabilities and skillsets needed to build and run solutions in the areas of Data Management, Data Solutions, Data Engineering and Data Services; Responsible to deliver and manage multiple programs and projects comprise of internal IT professionals and IT service providers to ensure that programs \/ projects are delivered within the agreed scope, budget and schedule; Manage and govern data operations to ensure compliance to regulatory, compliance and information security requirements including Personal Data Protection Act (PDPA), General Data Protection Regulation (GDPR) and Spam Control Act; Plan and move build and run functions into DevOps practices to drive automation; Represent Singtel SG IT at assigned project and program steering committees.  Present and pitch at relevant senior leadership levels and executive steering committees; Build strong relationship and influence with business leaders, other Singtel SG IT domains and IT service providers to deliver value via data; Partner with key stakeholders in providing right expertise and advisory on data management, data engineering, reporting, analytics and digital marketing; Develop annual budgets, capital expenditure plans, system development plans and monitor performance against plans; Develop multi-year IT investment roadmaps that link to the IT strategy; Build and develop high-performance data teams; Provide strong leadership and guidance to Pune ODC team; Oversee project planning and resource allocation; Support the growth and development of team members through training and career progression opportunities; Skills to Succeed; Bachelor's degree in business management, IT, Computer Science, Computer Engineering or equivalent.; At least 15 years of working experience, preferably in Telco industry in the following areas:; Experience in developing data strategy, architecture and technology roadmap; Leading a data delivery and operations team comprises of project managers, subject matter experts, data architects, solution designers, data engineers and service ops team. Mix of permanent staff, contract staff and developers from IT service providers of not less than 100 headcounts; Managing multiple outsourced vendors in supporting various applications such as Enterprise Data Warehouse, Big Data, Data Integration Platform, Enterprise Reporting Platform, Data Analytics and Visualization Platform and Campaign Solutions to support a wide array of business functions that include sales, products, finance, operations, marketing and analytics; Successfully managed and implemented large-scale solutions in the areas of data warehouse, big data, master data management system, reporting and analytics, marketing campaign; Ability to review and evaluate emerging and latest data technologies for potential adoption into the organization.; In-depth understanding of the following data technologies:; Big data solutions such as Cloudera, AWS, Microsoft, GCP; Data warehouse such as Teradata, Oracle Exadata, Amazon Redshift, Google BiqQuery for Analytics and Microsoft SQL Server; NoSQL\/In Memory database such as MongoDB, Couchbase and Cassandra; Business intelligence and analytics solutions such as Tableau, PowerBI, Alteryx, Trifacta and Cognos; Data integration solutions for both batch and real-time; Campaign platform such as Marketing Cloud; A good understanding of the enterprise data model for Telco such as aLDMs, Teradata Communications Logical Data Model (CLDM) and Oracle Communications Data Model.; A good understanding of the data management framework including master data management, data quality management, data security management and metadata management.; Rewards that Go Beyond; \u2022 Full suite of health and wellness benefits  ; \u2022 Ongoing training and development programs  ; \u2022 Internal mobility opportunities; Your Career Growth Starts Here. Apply Now!; We are committed to a safe and healthy environment for our employees & customers and will require all prospective employees to be fully vaccinated.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85445233","Role":"Alibaba Cloud-Senior Solutions Architect (Seven Core Products & Services\/Full...","Company":"Alibaba Cloud","Location":"Singapore","Publish_Time":"2025-06-24 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85445233","job_desc":"1. Customer Engagement and Implementation Support; Partner closely with customers to ensure successful architecture, migration, and deployment of applications and services on Alibaba Cloud\u2014especially for full-stack scenarios.; Guide enterprises through data-driven digital transformation and large-scale site migrations (Site Migration).; ; 2. Innovation and End-to-End Solution Design; Leverage deep industry expertise to challenge existing paradigms, drive innovation, and foster divergent thinking within client accounts.; Understand customers' unique business requirements and design tailored, end-to-end cloud solutions using Alibaba Cloud's \u201cSeven Core Products\/Full Stack\u201d (Compute, Storage, Network, Security, Big Data, AI, Database).; Drive solution wins and own architecture governance to ensure high availability, high concurrency, and reliable delivery at scale.; ; 3. Strategic Planning and Analysis; Produce comprehensive market analyses and multi-cloud architecture studies to identify trends, opportunities, and pain points within segmented industries.; Gather competitive intelligence to craft differentiated multi-cloud offerings.; Lead technical strategic planning, author white papers, case studies, and deliver conference presentations to amplify Alibaba Cloud\u2019s technical influence.; ; 4. Pre-Sales Support and Business Development; Engage in pre-sales activities with key decision-makers: build trust, uncover actual needs, and develop compelling business cases.; Conduct thorough assessments of customers\u2019 existing large-scale website operations and technical architectures (including multi-cloud landscapes), document current state and transformation goals, and shape migration-to-cloud strategies.; ; 5. Solution Promotion and Risk Management; Tailor solutions to customer requirements: optimize product selection, site migration plans (Site Migration), POC strategies, RFP responses, and product configurations to secure contracts and increase Alibaba Cloud\u2019s market share.; Incubate and scale public cloud solutions in vertical or niche fields while overseeing risk management throughout implementation.; ; 6. Capability Building and Knowledge Sharing; Build and maintain a comprehensive full-stack cloud architecture knowledge base: architecture templates, best practices, migration playbooks, and case studies.; Deliver training and enablement materials to sales teams, ecosystem partners, and customer technical teams, sharing best practices for multi-cloud and AI.; ; 7. Product Development Feedback \u25cb Collect and relay product requirements, pain points, and enhancement requests from large-scale site migrations and multi-cloud scenarios to drive improvements in Alibaba Cloud\u2019s \u201cSeven Core Products\/Full Stack\u201d and AI offerings.; Education; Bachelor\u2019s degree or above in Computer Science, Software Engineering, Information Systems, or a related field with solid understanding of cloud infrastructure, data technologies, and enterprise applications.; Experience; Minimum of 8 years' experience in operating and architecting large-scale websites, with proven end-to-end site migration (Site Migration) delivery.; Extensive background in data analytics, AI technologies, and enterprise industry solutions is preferred.; Full-Stack & Cloud Platform Expertise; Full-stack capabilities: proficient in front-end, back-end, and DevOps.; Hands-on experience with AWS, Azure, GCP, and Alibaba Cloud at scale\u2014delivering and operating distributed, high-concurrency systems.; Big Data Frameworks & Technologies; Deep knowledge of mainstream big data frameworks and technologies; expertise in at least five of the following: Hadoop, Spark, Blink, Flink, Zookeeper, HBase, Hive, Flume, Kafka, Sqoop, MapReduce, HDFS, Pig, or Presto.; Experience with database and data warehousing solutions (Redis, MySQL, PostgreSQL, NoSQL) and machine learning tools.; Familiarity with Alibaba Cloud MaxCompute, DataWorks, EMR, and PAI strongly preferred.; Multi-Cloud Architecture & AI; Solid understanding of -cloud and multi-cloud architectures; able to design, deploy, and operate systems across multiple cloud providers.; Familiarity with mainstream AI frameworks and platforms (e.g., TensorFlow, PyTorch, Alibaba Cloud PAI); hands-on AI project experience is a plus.; Teamwork & Communication; Proven ability to collaborate in cross-functional, distributed teams.; Strong project management skills and execution capability under tight deadlines.; Language Proficiency; Excellent written and spoken English; Chinese language proficiency is a plus to facilitate communication with China-based teams and local customers.; Bonus Qualifications; Experience in high-traffic e-commerce, social media, or streaming platforms.; Proficiency with Kubernetes, containerization, and microservices governance.; Possession of cloud architect certifications (Alibaba Cloud, AWS, or equivalent).","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85156727","Role":"Senior Data Architect (Networks)","Company":"Singapore Telecommunications Limited","Location":"Singapore","Publish_Time":"2025-06-18 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85156727","job_desc":"At Singtel, we believe in the strength of a vibrant, diverse and inclusive workforce where backgrounds, perspectives and life experiences of our people help us innovate and create strong connections with our customers. We strive to ensure all our people practices are non-discriminatory and provide a fair, performance-based work culture that is diverse, inclusive and collaborative. Join us and experience what it\u2019s like to be with an Employer of Choice*. Together, let\u2019s create a brighter digital future for all. *Awarded at the HR Fest Awards 2020.; Singtel Networks, the most established telecommunications infrastructure provider in Singapore is transforming to enable the digital generation of tomorrow. We are introducing new capabilities in 5G, Cloud, Analytics, Digital Commerce, Software Engineering, Cyber Security to enhance our core competencies and deliver innovative and differentiated Mobile and Fixed services (Broadband, TV and Telephony) for our customers. We are committed to celebrating inclusion and diversity and is a strong believer to upskill and nurture all individuals. Come join us today as we build Singtel\u2019s Networks of tomorrow, and Empower Every Generation to live, work and play in new ways!; Make an Impact by:; Lead and manage cloud data lake or solution initiative including design and determine the SaaS or software to be used for the data processing and pipeline for ETL, Streaming, analytics, AI\/ML and APIs.; Establish and lead the Day 2 operation process, SLO\/SLA, data off-premise clearance and security governance for cloud data pipeline.; Manage Networks data governance for the department and support in Group data governance and data protection framework.; Perform as Tier 3 Systems SME\uf0a7    Accountable for overall System Performance and Design.\uf0a7    Accountable for Change Management outcomes, executing minor to major software upgrades as well as solution changes independently.\uf0a7    Responsible to manage vendors and technically debate on optimum solution performance while ensuring robust and cost-efficient architecture. ; Lead and manage Tier 2 Systems Administration & Operation Support.; Accountable for System Security\uf0a7    Management of Anti-Malware systems and perform monthly scanning for any security threats and system administration and perform monthly scanning for systems.\uf0a7    Analyse security reports from Anti-Malware scans, determining best course of action.\uf0a7    Administrator for the firewalls and perform the quarterly review for the rules to ensure the system defence-in-depth and security.; Responsible for Information Security (InfoSec) governance, serving as the Security representative for the department or division. ; Responsible for architecting network and IP address designs, as well as implementing data protection and security measures.; Lead Incident Management and provide timely update to the Management and accountable for the RCA of the managed platforms.; Conduct research and development (R&D) or proof of concept (POC) on new technologies or proposals provided by the vendor.; Ensure operational processes for the respective systems are well documented. This includes system inventories, solution doc, IP\/Network design, SOPs etc.; Lead system audits, oversee security review submissions, and manage the Business Continuity Management (BCM) for the department.;  Skills for Success:; Degree in Engineering or IT.; At least 5 years\u2019 experience in data solution and administration.; Experience in Cloud Data solution (AWS, Microsoft Azure, etc), On-premise data platform on HPE Ezmeral, Stream data platform in Kafka and Confluent, On-premise system implementation, System.; Able to handle Data Architect, mapR, Hadoop and Spark (or equivalent).; Has knowledge in Linux\/Unix, Ansible automation, Shell Scripting, Kubernetes, Docker, serverless functions, APIs and Kafka bus, Network, IP Address and Security design.; Rewards that Go Beyond:; \u2022    Hybrid work arrangements; \u2022    Full suite of health and wellness benefits ; \u2022    Ongoing training and development programs ; \u2022    Internal mobility opportunities; Are you ready to say hello to BIG Possibilities?; Take the leap with Singtel to unlock new opportunities and accelerate your growth. Apply now and start your empowering career!","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85044055","Role":"Staff SW Engineer - AIOPS developer","Company":"VISA WORLDWIDE PTE. LIMITED","Location":"Singapore","Publish_Time":"2025-06-19 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85044055","job_desc":"Company Description; Visa is a world leader in payments and technology, with over 259 billion payments transactions flowing safely between consumers, merchants, financial institutions, and government entities in more than 200 countries and territories each year. Our mission is to connect the world through the most innovative, convenient, reliable, and secure payments network, enabling individuals, businesses, and economies to thrive while driven by a common purpose \u2013 to uplift everyone, everywhere by being the best way to pay and be paid.; Make an impact with a purpose-driven industry leader. Join us today and experience Life at Visa.; Job Description; The Reliability Engineering Automation team prides itself in keeping Visa systems up and secure, catering to the 24*7 needs of the business. The team uses AI to aggregate observational data (from monitoring systems output, job logs, syslog, etc.) and data from ticketing, incident, and event management system data to produce a virtuous circle of continuous insights yielding continuous improvements and fixes The Staff Software Engineer, a highly motivated senior experienced individual contributor, responsible for innovative planning, designing and developing effective solutions in accordance with industry standards on best web development practices. As part of the team, you will be required to own key modules, perform code and design AI architectural reviews, suggest best practices and implement design and development standards. The role is a senior consultant who has the passion to solve problems, factor old codes, learn and pick up new technologies like generative AI, chatops, LLM, Onprem models. Model QA. The role will also be required to lead implementations on key modules and mentor junior team members,; ; Responsibilities:; Have the passion in developing and supporting the growth of the next generation software architectures and inspire innovation within the team; Develop scalable real-time low-latency Artificial Intelligence processing solutions in an agile delivery method; Lead and guide the team in AI design, code and implement new algorithms to solve complex problems; Develop AIOPS solutions using available tools and technologies and assist the global team in problem resolution by hands-on participation.; Engage technical workshops with cross-vertical Technology partners for cross-vertical integration in software development; This is a hybrid position. Hybrid employees can alternate time between both remote and office. Employees in hybrid roles are expected to work from the office 2-3 set days a week (determined by leadership\/site), with a general guidepost of being in the office 50% or more of the time based on business needs.; Qualifications; Basic Qualifications; \u2022 5+ years of relevant work experience with a Bachelor\u2019s Degree or at least 2 years of work experience with an Advanced degree (e.g. Masters, MBA, JD, MD) or 0 years of work experience with a PhD, OR 8+ years of relevant work experience.; Preferred Qualifications; \u2022 Bachelor\u2019s Degree or equivalent in the Computer Science, Computer Engineering, or Information Technology disciplines with strong fundamentals in software engineering.; \u2022 At least 5+ years of working experience in the IT industry with advanced web development experience with Machine Learning and Artificial Intelligence in IT operations using data analytics theory and application.; Technical Qualifications; \u2022 5+ years of hands-on experience in development of applications using Aiops technologies like generative AI, chatops AI, NLP models etc.; \u2022 Solid executable knowledge of at least one or more programming language: Java, C#, python or NodeJS, Angular, Spring; \u2022 Explore, understand, and implement most recent Machine Learning algorithms and approaches for supervised and unsupervised machine learning and deep learning.; \u2022 Handle and process multi-terabyte data sets in scale-up and scale-out environments.; \u2022 Engage in full stack development from REST service to persistence adopting latest state-of-the-art technologies.; \u2022 Ability to design, develop and debug cloud applications. Ideally have some experience developing application on modern cloud platform.; \u2022 Create excellence both in terms of results, quality and system scalability through continuous evaluation, analysis, and refinement of the application implementation.; \u2022 Use AI and big data to identify opportunities and work with partner teams to improve systems availability and automate\/optimize operations.; \u2022 Develop business cases for automation and partner with the relevant teams to implement them.; \u2022 Software engineering principles (preferably Agile\/Scrum) and their application to the creation of self-healing infrastructure and applications.; \u2022 Good understanding of micro service architecture, Git and how to configure and deploy complex containerized applications; \u2022 Good understanding of any one or more security scanning tools like Checkmarx, Clair, SonarQube, Blackduck, Appcheck.; \u2022 Good to have at least one or more Cloud Platforms (Azure, AWS, GCP) is a strong plus.; \u2022 Good to have enterprise-level client-server web application development in Java\/J2EE, C# and OOP.; \u2022 Good understanding of SSDLC, Agile methodologies. Agile development experience in a SCRUM environment is a strong plus.; \u2022 Excellent articulation, communication, interpersonal and collaboration skills are required with problem solving attitude and ready to adopt new challenges.; \u2022 Excellent time management, organization and planning skills are essential.; At least 5+ years of working experience with defining system architecture and design patterns.; At least 5+ years of designing systems for greenfield engineering projects from ground up.; Knowledge of content management concepts and systems will be advantageous.; Analytical skills to understand business requirements and to translate into technical specifications.; Problem solver with excellent skills in troubleshooting.; Ability to set priorities in a multi-tasking environment.; Ability to negotiate with both Technology and Business counterparts in terms of delivery scope and timelines that aligns with capacity of the delivery team.; Passion to learn and pick up new technologies as needs evolves.; Additional Information; Visa is an EEO Employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability or protected veteran status. Visa will also consider for employment qualified applicants with criminal histories in a manner consistent with EEOC guidelines and applicable local law.","salary":"","work_type":"","country":"singapore"}
{"Job_ID":"84387288","Role":"Senior Manager - Site Reliability Engineering (SRE -Big Data \/ Kafka)","Company":"VISA WORLDWIDE PTE. LIMITED","Location":"Singapore","Publish_Time":"2025-06-17 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/84387288","job_desc":"Company Description; Visa is a world leader in payments and technology, with over 259 billion payments transactions flowing safely between consumers, merchants, financial institutions, and government entities in more than 200 countries and territories each year. Our mission is to connect the world through the most innovative, convenient, reliable, and secure payments network, enabling individuals, businesses, and economies to thrive while driven by a common purpose \u2013 to uplift everyone, everywhere by being the best way to pay and be paid.; Make an impact with a purpose-driven industry leader. Join us today and experience Life at Visa.; Job Description; Essential Functions:; Design, build and manage Big Data and Kafka infrastructure on private Cloud  AWS, GCP and Azure.; Manage and optimize Apache Big Data and Kafka clusters for high performance, reliability, and scalability.; Develop tools and processes to monitor and analyze system performance and to identify potential issues.; Collaborate with other teams to design and implement Solutions to improve reliability and efficiency of the Big data cloud platforms.; Ensure security and compliance of the platforms within organizational guidelines.; Other responsibilities include effective root cause analysis of major production incidents and the development of learning documentation. The person will identify and implement high-availability solutions for services with a single point of failure.; The role involves planning and performing capacity expansions and upgrades in a timely manner to avoid any scaling issues and bugs. This includes automating repetitive tasks to reduce manual effort and prevent human errors.; The successful candidate will tune alerting and set up observability to proactively identify issues and performance problems. They will also work closely with Level 3 teams in reviewing new use cases and cluster hardening techniques to build robust and reliable platforms.; The role involves creating standard operating procedure documents and guidelines on effectively managing and utilizing the platforms. The person will leverage DevOps tools, disciplines (Incident, problem, and change management), and standards in day-to-day operations.; The individual will ensure that the platforms can effectively meet performance and service level agreement requirements. They will also perform security remediation, automation, and self-healing as per the requirement.; The individual will concentrate on developing automations and reports to minimize manual effort. This can be achieved through various automation tools such as Shell scripting, Ansible, or Python scripting, or by using any other programming language.; Team Leadership:; Lead and mentor a team of SRE engineers providing strategic and technical guidance and support.; Foster a culture of continuous improvement, innovation, and operational excellence.; Develop and implement professional development programs and succession planning for the team.; Technical Leadership; Provide technical leadership and oversight to engineers; Establish SRE best practices; Ensure engineering and operational excellence (quality, security, performance, scalability, availability, resilience).; Collaboration & Strategy; Collaborate with Product Office, Operations & Infrastructure, Cybersecurity, Client Support, and other Product Development teams.; Drive the coordination, organization, and execution of qualitative and quantitative decisions.; The Skills You Bring:; Energy and Experience: A growth mindset that is curious and passionate about technologies and enjoys challenging projects on a global scale; Challenge the Status Quo: Comfort in pushing the boundaries, \u2018hacking\u2019 beyond traditional solutions; Language Expertise: Expertise in one or more general development languages (e.g., Python ,Java, ); Learner: Constant drive to learn new technologies; This is a hybrid position. Expectation of days in office will be confirmed by your Hiring Manager.; Qualifications; Basic Qualifications; o 8+ years of relevant work experience and a Bachelor\u2019s degree, OR 11+ years of; relevant work experience; Preferred Qualifications; 9 or more years of relevant work experience with a Bachelor\u2019s degree or 7 or; more relevant years of experience with an Advanced Degree (e.g. Masters,; MBA, JD, MD) or 3 or more years of experience with a PhD; o Experience with managing and optimizing Big Data and Kafka clusters.; o Proficient in scripting languages (Python, Bash) and SQL.; o Familiarity with big data tools (Big Data, Spark, Kafka, etc.) and frameworks (HDFS, MapReduce, etc.).; o Strong knowledge in system architecture and design patterns for high-performance computing.; o Good understanding of data security and privacy concerns.; o Excellent problem-solving and troubleshooting skills.; o Observability: knowledge on observability tools like Grafana, opera and Splunk.; o Linux: understanding of Linux, networking, CPU, memory, and storage.; o Programming Languages: Knowledge of and ability to code or program in one of Java, python or a widely used coding language.; o Communication: Excellent interpersonal skills, along with superior verbal and written communication abilities.; o Demonstrated experience with AWS and GCP cloud platforms; o Superior verbal, written & interpersonal communication skills with both technical & non-technical audiences; o Excellent team player, with strong collaboration skills and ability to influence cross-functional team for results; Additional Information; Visa is an EEO Employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability or protected veteran status. Visa will also consider for employment qualified applicants with criminal histories in a manner consistent with EEOC guidelines and applicable local law.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85196333","Role":"Senior Java Developer with SQL","Company":"LUXOFT INFORMATION TECHNOLOGY (SINGAPORE) PTE. LTD.","Location":"Central Region","Publish_Time":"2025-06-26 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85196333","job_desc":"Project Description:; Our client is the corporate and investment banking arm of The Group, world's 10th largest bank by total assets.; Their Singapore center, Information Systems Asia Pacific (ISAP), is one of the 3 main IT hubs for worldwide business with over 1000 IT staff covering Production and Application Development activities. They work daily with international branches located in 33 countries by supporting their IT solutions and envisioning and developing the Bank's future information systems.; The department is responsible for development and maintenance of Capital Market IT applications (in-house and package) used worldwide, covering Front Office (Murex, Apex, Orchestrade...), Back Office (Summit, Calypso...), Market data referential (Asset Control..), Market Risk (in-house application on big data technologies).; The SALES IT division works across the full value chain to sustain business activity and enable its acceleration (thru integration of new financial products, optimizing and automating front to back business processes).; This role aims to develop on SALES application, in project mode as well as maintenance, and to support the \u201crun the bank\u201d function. It requires a deep understanding of the nature of business operations, timing of activities & data flowing through internal and external systems. The build and support team continuously works with business users, infrastructure engineers and system managers.; Responsibilities:; Build & change management; Able to lead in terms of technology and conceptualize, propose design and architectural evolution, along with operational model.; Contributing to development of high quality fully tested source code complying with deadlines, schedule and coding standards.; Liaise with business analysts for clarification and understanding of requirements.; Provide deployment and change management of the solution\/platform.; Develop tests strategy in coordination with project manager \/ Business Analysts.; Provide technical assistance during user acceptance test.; Incidents and Requests management (business users and other IT teams); Communicate effectively.; Analyse enhancement requests and bug-fixes.; Document deliveries.; Support applications in production - analyze and resolve issues as they arise as well as propose optimizations for future.; Monitor the results and quality of the different software solutions and projects implemented in the organization.; Change management; Apply monthly release strategy, following the team's milestones for analysis, coding and testing.; Support the release roadmap in production.; Operations management; Application support, level 3 (expert).; Troubleshoot and fix issues.; Continuous improvement; Process improvements.; Technology, tools and infrastructure upgrade; Automation of manual work; Delivery efficiency and quality improvement; Mandatory Skills Description:; 12+ years of working experience; 8+ years of experience working with Java and T-SQL (Transact-SQL); Strong knowledge on various design patterns; A strong understanding of recent Java language features, such as lambdas, streams, and futures; Good knowledge of algorithms and data structures, with strong fundamentals in complexity analysis; Strong ability to analyze code \u2013 understand execution flow & debug even without access to a debugger; Experience with Maven, Git, writing and maintaining integration tests; Strong familiarity with Linux and bash; Good knowledge of SQL or an SQL-inspired dialect such as HQL; Experience in cloud native architecture.; AWS cloud architecture; Analytical and problem solving skills, a pro-active mind-set in security, proven ability to work individually under pressure.; Team player with good interpersonal skills.; Excellent written and verbal communication skills.; Nice-to-Have Skills Description:; Additionally, knowledge of the following would be helpful although it is not required:; Web development fundamentals (HTML, JavaScript, typescript, jQuery, ReactJS, etc); Docker\/Kubernetes, Microservices; An ideal candidate will also have expertise in some or all of the following:; JupyterLab, JupyterHub, Jupter notebook, Mercury ecosystem; Gitlab, Jenkins and Ansible with exposure to devops culture; Jira or a similar issue-tracking system; Python; ElasticSearch; Understanding or interest in finance & financial markets, particularly interest rate derivatives in their many forms (Forwards, Futures, Swaps, Swaptions, etc); Willing to do L3 Support if required.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85547564","Role":"Senior Software Architect, GCP","Company":"Rakuten Asia Pte. Ltd.","Location":"Singapore","Publish_Time":"2025-07-07 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85547564","job_desc":"Job Description:; Department Overview; The Global Ad Technology Department (GATD) manages the whole of the advertising systems that power Rakuten. We are a cross functional and data-driven organization working together in a diverse team spread across five countries: Japan, Singapore, India, China and UK.; GATD Website; https:\/\/corp.rakuten.co.jp\/careers\/feature\/adtech\/en\/; Position:; Why We Hire; Our organization has a lot of services and systems which are composed of various technologies.; To ensure the high system quality of each services and to increase engineers capability we are trying to seek full-time higher level Senior Software Architect to join our architect team. In this position, the Senior Software Architect on Horizontal Organization which called Tech Management Section.; We are looking for an experienced Software Architect to make intuitive high level decisions for software development.; You will see the \u201cbig picture\u201d and create architectural approaches for software design and implementation to guide the development team under the whole GATD services group.; Position Details; - Collaborating with other professionals to determine functional and non-functional requirements for new software or applications; - Using tools and methodologies to create representations for functions and user interface of desired product; - Developing high-level product specifications with attention to system integration and feasibility; - Define all aspects of development from appropriate technology and workflow to coding standards; - Communicate successfully all concepts and guidelines to development team; - Oversee progress of development team to ensure consistency with initial design; - Provide technical guidance and coaching to developers and engineers through Architecture review and Tech support activities; - Ensure software meets all requirements of quality, security, modifiability, extensibility etc; - Approve final product before launch; - Design Hybrid cloud; Mandatory Qualifications:; - At least 10+ years of experience in software development including solution or technical architecture experience; - Experience in software development and coding in various languages (Especially Java); - Excellent knowledge of software and application design and architecture; - Excellent knowledge of UML and other modeling methods; - Excellent knowledge of API design and implementation; - Understanding of software quality assurance principles; - A technical mindset with great attention to detail; - High quality organizational and leadership skills; - Outstanding communication and presentation abilities; - Strong analytical and quantitative problem solving ability; - Proactive self-starter with self-motivated mind; - Strong get-things-done mind with a sense of ownership; -Strong knowledge in public cloud , preferably GCP.; - Experience in GCP networking and security; Desired Qualifications:; - Basic knowledge of digital marketing or advertising technologies - Ad serving, Ad tracking, DSP, SSP, DMP, Campaign Management, etc; - Experience with big data solutions and data pip-line and processing - Hadoop, Hive, Spark, Kafka, etc; - Understanding Micro service architecture concepts and containerized applications; - Familiarity with HTML\/CSS, JavaScript and UI\/UX design; -Hybrid Cloud Architecture; - Experience in GCP Managed service like DataProc, Bigquery, Data Lake, GCS, DataStream etc; Others Information:; Additional information on Location; Singapore; #engineer #applicationsengineer #globaladdiv","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85819888","Role":"Network Engineer, Mobile Data Operation (Networks)","Company":"Singapore Telecommunications Limited","Location":"Singapore","Publish_Time":"2025-07-10 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85819888","job_desc":"At Singtel, we believe in the strength of a vibrant, diverse and inclusive workforce where backgrounds, perspectives and life experiences of our people help us innovate and create strong connections with our customers. We strive to ensure all our people practices are non-discriminatory and provide a fair, performance-based work culture that is diverse, inclusive and collaborative. ; Join us and experience what it\u2019s like to be with an Employer of Choice*. Together, let\u2019s create a brighter digital future for all. *Awarded at the HR Fest Awards 2020.; Singtel Networks, the most established telecommunications infrastructure provider in Singapore is transforming to enable the digital generation of tomorrow. We are introducing new capabilities in 5G, Cloud, Analytics, Digital Commerce, Software Engineering, Cyber Security to enhance our core competencies and deliver innovative and differentiated Mobile and Fixed services (Broadband, TV and Telephony) for our customers. We are committed to celebrating inclusion and diversity and is a strong believer to upskill and nurture all individuals. Come join us today as we build Singtel\u2019s Networks of tomorrow, and Empower Every Generation to live, work and play in new ways!; Make an Impact by:; Implementation of routing, switching and security policies of IP core networks serving the mobile network and associated value added systems.  ; Install or supervision of installation of new hardware in WAN LAN infrastructure.; Create and prepare implementation scripts and MOPs (Method of Procedures) pertaining to in house implementations.; To effectively perform risk assessment and impact on services network based on proposed scope of implementation and provide risk mitigation measures or implementation alternatives to ensure successful implementation with minimum services network impact.; Perform network node administration and security audits on Cisco Juniper switches routers and Fortinet CheckPoint firewalls, inclusive of routine preventive and corrective maintenance of the network nodes, so as to implement, test and roll out new mobile related products and services.; Provide 24 x 7 3rd tier technical and operational support to the IP core networks and handle any customer fault escalation.  ; Troubleshoot and perform root causes failure analysis for software & hardware faults related to the network nodes.; Perform application and testing of hardware and software updates (e.g. Cisco IOS) for routers, switches and firewalls. This may include liaising and coordinating with vendors for delivery of new software updates and patches. Create and maintain engineering blueprint documentation for the mobile IP core networks and to update the documentation on a continual basis as the IP core networks evolve.; Assist in the creation of Engineering Change Orders (ECO) and working with various systems node owners in the timely and coordinated execution of the ECO.; Manage IP workflow day to day IP requests and user troubleshooting (non project).; Pro active health check on IP network to ensure proper functionalities and to take appropriate corrective actions to timely remedy any abnormalities detected. Backup of all network nodes\u2019 configurations for contingency purposes.; Perform and coordinate disaster recovery activity expeditiously in times of service disruptions so as to minimize service downtime and revenue loss.  ; Provide engineering support expertise to Planning and Product Marketing in technical feasibility studies of new products and services, POC (Proof of Concept) Trials, inclusive of fast IP setup for new corporate GPRS 3G,4G,5G data customers.; Review existing best practices and processes for the IP network and introduce preventive or optimized measures to improve the network quality cum resilience in a proactive manner.; Administration of training operating capital expenditures, inclusive of negotiation renewal of maintenance contracts.; Skills for Success:; Degree in Electrical , Electronicl, Computer Engineering,  Computer Science or Info Communications Technology.; Minimum 3 to 5 years of experience in configuring and administering routers and switches firewall products, and NTP servers.; Strong interest in developing IP networking.; Good working knowledge of L2 L3 switching and MPLS, OSPF and BGP routing over geographical large scale WAN LAN mobile networks. CCNA CCNP qualifications will be advantageous.; Possess good analytical and troubleshooting skills in technical fault isolation and rectification.; Are you ready to say hello to BIG Possibilities; Take the leap with Singtel to unlock new opportunities and accelerate your growth. Apply now and start your empowering career!","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85015360","Role":"Site Reliability Engineer, Traffic Platform - Traffic SRE","Company":"BYTEDANCE PTE. LTD.","Location":"Singapore","Publish_Time":"2025-06-18 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85015360","job_desc":"Site Reliability Engineer, Traffic Platform - Traffic SRE; Singapore Regular R&D Job ID: A136692; Responsibilities; The Traffic Infrastructure Global Engineering (TIGE)-Traffic Platform team at ByteDance builds and operates multi-cloud based large scale network services around the world that we use to accelerate and optimize network traffic for Tiktok and a variety of application services for ByteDance internal customers, which include but are not limited to layer 4 loadbalancing, layer 4\/7 acceleration, global ingress, CMAF, FaaS and WAF, etc. By joining us, you can work within a brilliant team and learn how to build Tiktok scale network traffic platform which serves billions of users globally. Responsibilities; Design and develop features of traffic software (DNS Server, L4 and L7 Proxy, Web Caching, and FaaS Runtime), integrate based on our traffic platform to process terabyte-scale data in real-time.; Build data pipelines and develop telemetry systems to support datadriven traffic control.; Develop API acceleration and other networking services that run on top of our multi-cloud based traffic platform.; Problem solving and performance tuning for online traffic.; Research new technologies for more efficient and scalable traffic processing.; Qualifications; Minimum Qualifications; Experience in developing network systems in Rust, C, C++, and\/or; Go, developing skills in Linux environment.; Bachelor's degree in Computer Science, Electrical Engineering, Computer Engineering or related majors.; Familiarity with network protocols such as TCP\/IP, HTTP\/HTTPs, and DNS.; Familiarity with Microservice architecture.; Familiarity with container and orchestration technologies such as Docker and Kubernetes.; Preferred Qualifications; Experience in building large scale network services on cloud (AWS, GCP, OCI).; Experience in designing and developing high performance network loadbalancer products.; Experience in developing proxy software such as Nginx and Envoy is a big plus.; Experience in System Programming using low level system calls such as epoll, io-uring, etc., is a big plus.; Experience in developing Web Caching software such as Apache Traffic Server and Varnish, etc.; Experience in Edge Computing and FaaS Runtime development.; Experience in building distributed or cloud service based management system.; Proficiency in networking newer protocols such as HTTP2, HTTP3\/QUIC, TLS1.3, etc.; Job Information; Inspiring creativity is at the core of ByteDance's mission. Our innovative products are built to help people authentically express themselves, discover and connect \u2013 and our global, diverse teams make that possible. Together, we create value for our communities, inspire creativity and enrich life - a mission we work towards every day.; As ByteDancers, we strive to do great things with great people. We lead with curiosity, humility, and a desire to make impact in a rapidly growing tech company. By constantly iterating and fostering an \"Always Day 1\" mindset, we achieve meaningful breakthroughs for ourselves, our Company, and our users. When we create and grow together, the possibilities are limitless. Join us.; Diversity & Inclusion; ByteDance is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives. Our platform connects people from across the globe and so does our workplace. At ByteDance, our mission is to inspire creativity and enrich life. To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach. We are passionate about this and hope you are too.; Apply; Share to","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"84517050","Role":"Integration, Verification and Validation Engineer","Company":"Thales","Location":"Singapore","Publish_Time":"2025-06-17 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/84517050","job_desc":"Location: Singapore, Singapore; Thales people architect solutions at the heart of the defence-security continuum. Interoperable and secure information and telecommunications systems for defence, security, and civil operators, are based upon innovative use of radiocommunications, networks, and cybersecurity. We are ground breaking new digital technologies such as 4G mobile communications, cryptography, cloud computing and big data for use in physical protection systems, and critical information systems.Thales established its presence in Singapore in 1973 to support the expansion of aerospace-related activities in the Asia-Pacific region. Throughout the last four decades, the company grew from strength to strength and is today involved in the primary businesses of Aerospace (including Air Traffic Management), Defence & Security, Ground Transportation and Digital Identity & Security. Thales today employs over 2,100 people in Singapore across all its business areas.; ROLE DESCRIPTION SUMMARY; The IVVQ Engineer is responsible for planning, participating and supervising the efficient integration, verification and validation (IVV) execution defense and supporting systems. He\/she assures that the appropriate means and practices are applied to guarantee the adherence to defined requirements and quality standards with minimal wasted effort or inefficiencies. IVVQ Engineer is responsible for the Test plan and reports, assuring that the scope of the IVV activities fully covers all aspects of the system under test, while taking all previous verification activities on comparable systems into account to optimize the test effort.; KEY ACTIVITIES AND RESPONSIBILITIES; As an IVVQ Engineer , you are accountable for:; Responsible for the Planning and Execution of IVV activities; Decides on the appropriate test strategy, modules and practices; Create and conduct the System Test plan and report; Works closely with Project Engineers (SEM) to agree on testing durations and project risks; Efficiently manages test resources (staff, software & hardware); Manages Defects \/ Engineering Changes for the customer; Contributes to the estimation, planning and execution of IVV activities; Be able to coordinate activities with a global team; Participate in Bid activities providing scope and costing; ; KEY KNOWLEDGE AND EXPERIENCE; To be successful in your role, you will have demonstrated and\/or acquired the following knowledge and experience:; Bachelor's degree in Engineering or related field with 5 years\u2019 experience,; Minimum of 5 years related experience and\/or training in Verification\/Validation experience in large integrated systems; Previous knowledge of Unix \/Linux\/ Android systems; Expertise in defect reporting, characterization, and management workflows - Use of common defect management and tools; Ability to read and interpret requirement documentation - Use of common requirements management and tools; Project Management and communication skills; Creativity in detecting and provoking possible problems; Expertise in common test practices (Functional tests, user scenario tests, automation); Previous experience writing test cases; Comfortable with handling and performing basic troubleshooting on IT software & hardware; Preferred Skills and Experience:; Minimum five (5) years of comprehensive experience in a software engineering integration, verification or validation team; Experienced in the integration of software systems, able to coordinate activities internal and external to the system; Collaborate with clients, software developers, and other stakeholders to identify system requirements and ensure that our software products meet those requirements; Working experience with open-source configuration management and deployment tools such as Puppet or Chef, Docker, Kubernetes, and Linux scripting.; Understanding of software development methods and electrical\/mechanical design; User Interfaces, Networking, wireless, and\/or communication systems test experience, software APIs; Be versatile to be able to handle job scopes such as supporting bids and Work Package Management; At Thales we provide CAREERS and not only jobs. With Thales employing 80,000 employees in 68 countries our mobility policy enables thousands of employees each year to develop their careers at home and abroad, in their existing areas of expertise or by branching out into new fields. Together we believe that embracing flexibility is a smarter way of working. Great journeys start here, apply now!","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85167393","Role":"Engineer \/ Executive Engineer\/ Senior Engineer (Systems Engineering and...","Company":"Public Service Division","Location":"Changi","Publish_Time":"2025-06-17 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85167393","job_desc":"[What the role is]; The global Unmanned Systems sector has made quantum leaps and is expected to continue growing rapidly. This growth is encouraged by the interest in developing emerging technologies (such as Unmanned Aircraft System (UAS) and electric-Vertical Take-Off and Landing (e-VTOL) aircraft) and innovative use-cases for both commercial and recreational use, arising from the benefits that Unmanned Systems bring across multiple industries.; To capture the potential and opportunities of the Unmanned Systems sector for Singapore, the Unmanned Systems Technology, Engineering and Planning (UTEP) Division develops strategies to assess, identify, and forge partnerships with the industry, other civil aviation authorities and research entities, to grow new business areas, initiate greenfield research and develop capabilities. The division also focuses on systems engineering, technology capabilities and strategic planning for the Unmanned Systems Group, in addition to integration, implementation and operational oversight of the maintenance of sensor and command and control (C2) systems.; As an officer in the Systems Engineering and Development section of the UTEP Division, you will implement, manage and maintain large-scale sensor and C2 systems such as Counter-Unmanned Aircraft System (C-UAS) and Unmanned Aircraft System (UAS) Traffic Management (UTM) Systems. At the onset, you will have opportunities to engage stakeholders to understand their needs, ideate the overall solution and architecture framework, and work with vendors to deliver solutions. With the implementation of the systems, you will manage the maintenance of the systems to ensure the highest level of serviceability throughout the life of the systems. To develop new capabilities for C-UAS and UTM, you will also have the opportunity to research, review, test and assess the latest technological advancements through study trips, engagements with technology providers and other regulators, and technical trials. If this excites you, read on for details of the job responsibilities and requirements. We look forward to your application.; [What you will be working on]; Responsibilities; Gather and analyse stakeholders\u2019 operational requirements and translating them into technical specifications.; Propose solutions and system architecture that meet requirements and ensure that solutions are implemented in accordance with requirements.; Source, assess and evaluate possible technological solutions to meet operational needs.; Work with system provider on development of system architecture, technical designs, and specifications to support operational requirements expressed by stakeholders and following existing or industry best practice system development frameworks.; Undertake project management activities including planning & budgeting, project tracking, prioritisation of features development, contract variation and change request management.; Participate in system procurement activities including preparation of tender documentation, vendor evaluation and technical reviews; Implementation and deployment of systems, including testing, commissioning, and system upgrade activities.; Oversee the cyber-security protection of the systems, ensuring compliance to prevailing industry standards and Government policies.; Oversee and manage the maintenance and support operations of the systems after commissioning.; Research, review, test and assess the latest C-UAS and UTM technologies to determine suitable improvements\/upgrades\/augmentations to existing capabilities.; [What we are looking for]; Requirements; Trained in one of the following disciplines: Software Engineering, Systems Engineering, Electrical\/Electronic Engineering, Computer Science, Information Technology or equivalent.; Possess knowledge, experience and familiarity in the following:; Project management of a large-scale real-time software-based system such as a C2 system;; System architecture design and development (including cloud, microservices and on-premise);; System administration of Linux\/VMware environments, cloud environments, database management (e.g. SQL) and network management;; Software development life-cycle and quality assurance methodologies, including safety critical systems;; Cybersecurity and Data security;; Relevant policies, guidelines and industry best practices (e.g. GovTech\u2019s Instruction Manual for Infocomm Technology and Smart Systems, Center for Internet Security Benchmarks, etc.); Knowledge and experience in disciplines such as RF transmitters, radars, electro-optics, flying platforms, Artificial Intelligence\/Machine Learning, Big Data, as well as frequency spectrum and communication theory is a plus.; Prior experience working in an Operations Centre, in aviation industry, and\/or in technical standards development (e.g. ASTM, EUROCAE, etc.)  will be an added advantage.; Ability to analyse complex technical information and write precise, detailed technical specifications and documentation.; Calm disposition to handle and resolve unforeseen circumstances.; Good analytical and problem-solving skills.; Excellent interpersonal, verbal, and written communication skills.; Good ethics and professional conduct, and with strong sense of care, responsibility and teamwork spirit.; Note: Your appointment designation will commensurate with your relevant work experience. Successful candidates will be offered a 3-year contract in the first instance, and may be considered for placement on a permanent tenure or subsequent contract renewal.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85177018","Role":"Senior Software Engineer - Traffic Infrastructure","Company":"BYTEDANCE PTE. LTD.","Location":"Singapore","Publish_Time":"2025-06-17 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85177018","job_desc":"Senior Software Engineer - Traffic Infrastructure; Singapore Regular R&D Job ID: A38414; Responsibilities; ByteDance will be prioritizing applicants who have a current right to work in Singapore, and do not require ByteDance's sponsorship of a visa. ByteDance Founded in 2012, ByteDance's mission is to inspire creativity and enrich life. With a suite of more than a dozen products, including TikTok, Helo, and Resso, as well as platforms specific to the China market, including Toutiao, Douyin, and Xigua, ByteDance has made it easier and more fun for people to connect with, consume, and create content.; Why Join Us Creation is the core of ByteDance's purpose. Our products are built to help imaginations thrive.; This is doubly true of the teams that make our innovations possible. Together, we inspire creativity and enrich life - a mission we aim towards achieving every day. To us, every challenge, no matter how ambiguous, is an opportunity; to learn, to innovate, and to grow as one team.; Status quo? Never. Courage?; Always. At ByteDance, we create together and grow together. That's how we drive impact - for ourselves, our company, and the users we serve.; Join us. Traffic Infrastructure Global Engineering Team The Traffic Infrastructure Global Engineering (TIGE) team at ByteDance operates a large network of POPs around the world that we use to accelerate site traffic and cache CDN content, and we own all layer 4 and layer 7 traffic management for Tiktok Edge.; By joining us, you can learn how to build content delivery networks and Edge Computing Platform within Tiktok's Edge. To better support Tiktok, the TIGE team is seeking experienced software engineers who can help improve our Kubernetes based Cloud Native Traffic Platform. The traffic platform balances, manages and processes Tiktok application traffic across all Tiktok's edge clusters.; Also, the traffic platform contains varied network services in order to orchestrate the delivery of bits from our servers to your phone. Responsibilities; Design and develop features of traffic software (DNS Server, L4 and L7 Proxy, Web Caching, and FaaS Runtime), integrate based on our traffic platform to process terabyte-scale data in real-time; Build data pipeline and develop telemetry systems to support datadriven traffic control; Develop API acceleration and other networking services that run on top of our traffic platform; Problem solving and performance tuning for online traffic; Research new technologies for more efficient and scalable traffic processing; Qualifications; Minimum Qualifications:; 3+ years experience in developing network systems in Rust, C, C++, and\/or; Go, strong developing skills in Linux environment.; Master\u2019s degree (or Bachelor's degree with addtional 2+ years of experience) in Computer Science, Electrical Engineering, Computer Engineering or related majors.; Familiarity with network protocols such as TCP\/IP, HTTP, and DNS.; Familiarity with Microservice architecture.; Familiarity with container and orchestration technologies such as Docker and Kubernetes; Strong understanding of software deployment fundamentals and automation.; Good understanding of concepts in operating system, remote process communication, high availability etc.; Preferred Qualifications:; Experience in System Programming using low level system calls such as epoll, io-uring, etc., is a big plus.; Experience in developing HTTP proxy such as Nginx and Envoy is a big plus.; Experience in developing Web Caching software such as Apache Traffic Server and Varnish, etc.; Experience in Edge Computing and FaaS Runtime development.; Experience in building distributed or cloud service based management system; Proficiency in networking newer protocols such as HTTP2, HTTP3\/QUIC, TLS1.3, etc.; ByteDance is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives. Our platform connects people from across the globe and so does our workplace. At ByteDance, our mission is to inspire creativity and enrich life. To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach. We are passionate about this and hope you are too.; Apply; Share to","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85044471","Role":"Infrastructure Engineer","Company":"ANTAES ASIA PTE. LTD.","Location":"Singapore","Publish_Time":"2025-06-19 16:18:32","URL":"https:\/\/id.jobstreet.com\/id\/job\/85044471","job_desc":"Job Description:; Contribute to IT projects for Antaes clients; Provide Service Operations support to internal and external customers in accordance with the terms of the customer contract and Service Level Agreements (SLAs); Ensure the correct functioning and maintenance of all internal and external systems and products serviced by Service Operations; When required, act as the customer SPOC and co-ordinate the scheduling of intervention with Customer's, internal resolver groups, and the Service Desk ensuring the highest level of customer services and communications are maintained to resolve the fault and incident within the prescribed SLA.; Carry out incident and problem management support to the highest standards and co-ordinate the resolution with the appropriate resolver groups; Ensure shortest restoral times possible, initiating the timely escalations to specialized resolver groups inside and outside Client\u2019s office, according to the customer contracts, SLAs and monitoring requirements; Manage the replacement of faulty equipment through the use of spares, and ensuring the timely replenishment the spare according to prescribed availability and sparing policy.; To ensure the Service Operations team adheres to the highest working standards for all incidents and problems by providing guidance, support and direct management.; Proactively detect problems related to service and infrastructure operations and delivery services, conduct diagnostics and provide service request ownership to ensure resolution of customer problems; Support the senior team members in the management, reporting, and co-ordination of day-day tasks; Adhere to installation guidelines and industry best practices to deliver quality service and infrastructure operations; Use the appropriate tools and equipment to perform the installation, intervention, and repairs in accordance with Service Operations and Delivery guidelines and instructions were provided; Report and escalate to the next level those problems which cannot be fixed; Carry out preventive and proactive maintenance of equipment and monitoring of systems and services in accordance with agreed schedules and customer expectations; Perform Change Management, Configurations, Design and Implementation of the supported Product & Systems; Manage local suppliers in the provision of services for the Client\u2019s operations centres and report on services provided to management.; Conducts the analysis, definition, documentation and testing of application & systems enhancements; To provide onsite support to Users during the cutover of the services; Continuously identify and document lessons learnt, known errors and operational knowledge for improved services; When\/where required, be contactable for escalations and support, on and on-call standby basis during out of office hours.; Contribute to the promotion of Antaes services on top of assistance provided to clients; Job Requirements:; Diploma\/Bachelor Degree in Computer Science, Electronic Engineering or equivalent; 3 years experience with :; Operating System: RHEL 6\/7, RHEL HA ,Windows Data center 2016 with clustering; Hardware: HP DL360 Gen 9, DELL R430XD, DELL R730XD \/ equivalent; Virtualization: ESXi, VMWare vsphere 6, VMware vCentre, VMware SRM Standard; MQ: MQ v8, MQ IPT, MQ Clustering, IBM License Manager; Web: Apache Web Server, Apache Tomcat Application Server; Monitoring tools: NagiosXI, NagiosLog, eG Monitoring suite; Firewall: Palo Alto 850 & PA-5220 (equivalent), Checkpoint Firewall (equivalent); Load Balancer: F5 BIG-IP LTM i2600; Vulnerability Manager( IBM Qradar All-in-one console, Event collector), Waterfall MQ Agent (Waterfall for IBM Websphere MQ); Other Technologies: RedHat Satellite\/ Spacewalk, DHCP, TFTP, Mail, Squid (WEB Proxy),NFS, Active Directory,DNS,NTP,Yum repo, IPA, SysLog servers; Unix \/ Linux Certification; VMWare Certification; ITIL Foundation v3 Certification","salary":"","work_type":"Full time","country":"singapore"}
