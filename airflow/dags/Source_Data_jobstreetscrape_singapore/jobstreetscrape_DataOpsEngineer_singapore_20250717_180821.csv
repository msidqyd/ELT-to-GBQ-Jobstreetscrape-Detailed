Job_ID,Role,Company,Location,Publish_Time,URL,job_desc,salary,work_type,country
85833905,Data Engineer - Cloud Operations (Engineering & Ops),Synapxe,One North,2025-07-17 06:57:21,https://id.jobstreet.com/id/job/85833905,"Company description:; ; Synapxe is the national HealthTech agency inspiring tomorrow's health. The nexus of HealthTech, we connect people and systems to power a healthier Singapore.; ; Together with partners, we create intelligent technological solutions to improve the health of millions of people every day, everywhere. Reimagine the future of health together with us at www.synapxe.sg; ; ; Job description:; ; Position Overview; This position will serve as primary support for cloud operations related to IDMC, Tableau, STATA, Sagemaker and Databricks. This role will be responsible for ensuring platforms operate reliably, securely and efficiently within the AWS environment. Responsibilities include maintaining operational excellence, monitoring and automation, managing incident response and performance optimization and ensure governance and cloud best practices.; Role & Responsibilities; Operational Architecture and Reliability; Design scalable, fault-tolerant, and high available AWS infrastructure; Define and implement operational best practices for cloud workloads (compute, storage, database); Monitoring and Logging; Build and maintain operational playbooks; Setup alerts, dashboards and logs to track health and performance of AWS workloads; Incident Management and Troubleshooting; Conduct Root Cause analysis and drive permanent fixes for recurring issues; Define and enforce incident response processes and escalation paths; Lead resolution of incidents; Requirements; Degree in Computer Science, Computer Engineering; Minimum 10-12 year working experience in system operations compliance and management areas; 5+ years of experience in cloud operations or cloud architecture; Must be cloud certified; Good in-depth understanding of data warehouse concepts, data profiling, data verification and advanced analytics techniques; Strong knowledge of monitoring, incident management, and clous cost control; Possess prior hands-on experience with technologies such as AWS, IDMC, Tableau, .NET, MS-SQL database, Oracle Database, Databricks, ML Ops, STATA, Sagemaker, Data Robot technologies.; Good interpersonal skills with the ability to work with different groups of stakeholders; Exposure to hospital information / clinical systems is an added advantage; Apply Now; NOTE: It only takes a few minutes to apply for a meaningful career in HealthTech - GO FOR IT!!; #LI-SYNX40",,Full time,singapore
85807353,Associate Data Engineer - ETL (Engineering & Ops),Synapxe,One North,2025-07-16 17:57:21,https://id.jobstreet.com/id/job/85807353,"Company description:; ; Synapxe is the national HealthTech agency inspiring tomorrow's health. The nexus of HealthTech, we connect people and systems to power a healthier Singapore.; ; Together with partners, we create intelligent technological solutions to improve the health of millions of people every day, everywhere. Reimagine the future of health together with us at www.synapxe.sg; ; ; Job description:; ; Position Overview; This role is for Engineering &Ops team, Level 2 support/BAU work for KTLO Operations to ensure smooth operation, operational efficiency and enhance user satisfaction.; Role & Responsibilities; Manage and prioritize user queries and production issues for existing applications in Engineering and Operations; Track and resolve production support incidents; Attend user meetings to document and analyze change request requirements or conduct regular workgroup meetings with stakeholders; Perform data profiling and mapping to define data requirements for new projects or change requests; Provide support for production reports, dashboards, and metadata; Collaborate with vendors and developers to design, configure, and test enhancements per Synapxe project methodologies; Translate user requirements into analytics, reporting needs, and ETL rules for new data mart applications and enhancements; Identify and document business attributes and metrics by analyzing existing data and reporting requirements; Conduct technical data mapping for potential data warehouse sources; Execute testing phases (system integration testing, user acceptance testing) before implementation; Provide 24/7 primary application maintenance support; Assist the Project Manager in assessing technical feasibility for cost evaluations; Requirements; Bachelor's degree in Computer Science, Information Technology, or a related field; At least 4 years of experience in the IT industry, including:; a. Development, implementation, and maintenance of IT systems, preferably in Data Warehousing, ETL rules, data modeling, and BI applications; b. Operations support and business analysis experience.; c. Strong MS-SQL and Oracle Database scriptin; Experience in diagnosing, troubleshooting, and performing root cause analysis; Ability to diagnose and troubleshoot problems with BI reports and ETL processes; Experience with AWS, Data Lake, Databricks, and the healthcare domain is a plus; Able to work independently and as an effective team player with a strong desire to deliver results; Adaptable, meticulous, and possess strong analytical skills; Good communication skills (both written and spoken); Strong team player; Apply Now; NOTE: It only takes a few minutes to apply for a meaningful career in HealthTech - GO FOR IT!!; #LI-SYNX40",,Full time,singapore
85838975,Data Engineer - ETL (MOH ITDG),Synapxe,One North,2025-07-17 08:57:21,https://id.jobstreet.com/id/job/85838975,"Company description:; ; Synapxe is the national HealthTech agency inspiring tomorrow's health. The nexus of HealthTech, we connect people and systems to power a healthier Singapore.; ; Together with partners, we create intelligent technological solutions to improve the health of millions of people every day, everywhere. Reimagine the future of health together with us at www.synapxe.sg; ; ; Job description:; ; Position Overview Role & Responsibilities; Develop TRUST data strategy:; Work with stakeholders to understand data analytics needs, data structure requirements (both in terms of scalability and accessibility), and translate this into a coherent near to long term data strategy for TRUST; Support translation of data business needs into technical system requirements for MCDR, in terms of collection, storage, batch -time processing, as well as analysis of information from structured and unstructured sources in a scalable, repeatable, and secure manner; Identify opportunities for improvements and optimisation e.g., Implement best practices and performance optimization on Big Data and Cloud to achieve the best data engineering outcomes; Oversee data preparation and data provisioning for TRUST:; Collaborate with data engineers to organise and prepare anonymised datasets in MCDR according to TRUST standards, and then providing the data in accordance with the approved TRUST Data Request. This involves working with the data engineers closely to ensure that the datasets meet the required standards and are made available as per the specific data request guidelines set by TRUST; Oversee implementation of common data model and data quality programme in TRUST and MCDR; Work with data analysts, data scientists, clinicians and other stakeholders to implement common data models to support analytics use cases; Design and implement tools to enhance the data strategy and enable seamless integration with the data, potentially leveraging API calls for efficient integration; Implement data management standards and practices; Requirements; Degree/master's in computer science, Information Technology, Computer Engineering or equivalent; At least ten (10) years of relevant working experience in Data management / Integration / Modelling the data warehouse or advanced analytics solutions; Demonstrate good, in-depth knowledge in relevant Extract-Transform-Load (ETL) hardware/software products, frameworks, and methodologies; Experience in designing and implementing cloud-based data solutions using cloud platforms (e.g., AWS cloud native tools); Databases (e.g., Oracle, MS SQL, MySQL, Teradata); Big data (e.g., Hadoop ecosystem); ETL development using ETL tools (e.g., Informatica, IBM DataStage, Talend); Data repository design (e.g., operational data stores, dimensional data stores, data marts); Experience in interacting with analytics stakeholders (economists, statisticians, clinicians, policy makers) on a business or domain level; Comfortable working independently to carry out data analysis, estimate data quality and sufficiency; Good interpersonal skills, a detail-oriented & flexible person who can work across different areas within the team; The following will be preferred: Some understanding of Singapore Healthcare System and healthcare data governance, management; and/or familiarity with health informatics; Apply Now; NOTE: It only takes a few minutes to apply for a meaningful career in HealthTech - GO FOR IT!!; #LI-SYNX40",,Full time,singapore
85009910,Data Engineer,Keppel Management Ltd,Central Region,2025-06-18 17:57:21,https://id.jobstreet.com/id/job/85009910,"Job Description; Develop, maintain scalable data pipelines and build out new integrations to support continuing increases in data volume and complexity; Develop and maintain scalable, optimized data pipelines leveraging Python and AWS services to support increasing data volume and complexity, while ensuring seamless integration with AI platforms like Bedrock and Google. Further enhance data accessibility and drive data-driven decision making by collaborating with analytics and business teams to refine data models for business intelligence tools; Develop, maintain, and optimize scalable data pipelines using Python and AWS services (e.g., S3, Lambda, ECS, EKS, RDS, SNS/SQS, Vector DB); Rapidly developing next-generation scalable, flexible, and high-performance data pipelines; Collaborate with analytics and business teams to create and improve data models for business intelligence; End-to-end ownership of data quality in our core datasets and data pipelines; Participate in code reviews and contribute to DevOps / DataOps / MLOps; Job Requirements; Bachelor's degree in Computer Science, Engineering, or a related field; 2-3 years of experience in data engineering or a similar role; Strong programming skills in Python, SQL, AWS and related tech stack; Experience with building scalable data pipelines with technologies such as Glue, Airflow, Kafka, Spark etc.; Experience using Snowflake, DBT, Bedrock is a plus; Good understanding of basic machine learning concepts (Sagemaker)",,Full time,singapore
85074940,Senior Application & Data Engineer,BRC GLOBAL ROLLS PTE. LTD.-,Downtown Tanjong Pagar,2025-06-21 17:57:21,https://id.jobstreet.com/id/job/85074940,"The Senior Application & Data Engineer is responsible for Identifying, designing, and implementing process improvements that include building/re-engineering data models, data architectures, pipelines, and data applications. Continuously look for data optimization processes and oversee data management, governance, security, and analysis.; Job Responsibilities:; Lead the development and optimization of our data pipelines, databases, and systems for serving data to our customers, ensuring scalability, efficiency, and reliability.; Work in close collaboration with stakeholders and analysts to design and implement robust data models.; Drive innovation by staying updated with the latest in data engineering practices, tools, and technologies, applying them to solve complex business and data challenges.; Design, construct, install, test and maintain a highly scalable data platform.; Analyze business requirements and create conceptual, logical, and physical data models.; Design database tables, columns, and relationships, and document data flow and dependencies.; Build high-performance algorithms, prototypes, models and proof of concepts.; Develop data set processes for data modeling, mining, and production.; Integrate new data management technologies and software engineering tools into existing structures.; Research opportunities for data acquisition and new uses for existing data for reporting.; Create custom software components and analytics applications.; Collaborate with IT team members on project and technology related goals.; Job Requirements:; Degree in Computer Science/Information Technology or equivalent data-related fields, such as data science, data engineering, data management, data governance, data analytics etc; Minimum 5 years of relevant experience in areas such as data management, engineering, extract, transfer and load data.; Strong SQL skills, on MS SQL server environment, for querying and managing data.; Proficiency in Python and SQL.; Strong understanding of object-oriented programming (OOP) and design patterns.; Proficiency in programming languages such as .NET and Python.; Experience with software development frameworks and libraries.; Familiarity with version control systems such as Git or Azure Devops.; Knowledge of software testing and debugging methodologies.; Ability to write clean, maintainable, and efficient code.; Experience with agile development methodologies.; Skills in systems problem-solving and conflict resolution.; Ability to work as part of a team, independently and make decisions.; Artificial Intelligence on LLM/RAG knowledge will be an advantage.; Ethical and able to organize and complete tasks to expected standards and on-time.; Trustworthy and accountable to deliver quality results.; Adaptability to changing requirements and circumstances.; Strong written and verbal communication skills; Ability to manage time effectively.; Ability to travel and take on short overseas assignments on an as needed basis.","$6,000 – $9,000 per month (SGD)",Full time,singapore
85685007,Associate Data Engineer - Datawarehouse (Engineering & Ops),Synapxe,One North,2025-07-11 17:57:21,https://id.jobstreet.com/id/job/85685007,"Position Overview; The Data Engineer supports the implementation of data structure and architecture, master/meta-data management approach and data quality programme to facilitate access to data and information. He/She support the design, implementation and maintenance of data flow channels and data processing systems that support the collection, storage, batch and real-time processing, and analysis of information from structured and unstructured sources in a scalable, repeatable and secure manner on on-premise or commercial cloud. He/She implements data management standards and practices.; Role & Responsibilities; Manage and prioritize user queries and production issues for existing applications in Engineering and Operations; Track and resolve production support incidents; Attend user meetings to document and analyze change request requirements or conduct regular workgroup meetings with stakeholders; Perform data profiling and mapping to define data requirements for new projects or change requests; Provide support for production reports, dashboards, and metadata; Collaborate with vendors and developers to design, configure, and test enhancements per Synapxe project methodologies; Translate user requirements into analytics, reporting needs, and ETL rules for new data mart applications and enhancements; Identify and document business attributes and metrics by analyzing existing data and reporting requirements; Conduct technical data mapping for potential data warehouse sources; Execute testing phases (system integration testing, user acceptance testing) before implementation; Provide 24/7 primary application maintenance support; Assist the Project Manager in assessing technical feasibility for cost evaluations; Requirements; Bachelor's degree in Computer Science, Information Technology, or a related field; At least 4 years of experience in the IT industry, including:; Development, implementation, and maintenance of IT systems, preferably in Data Warehousing, ETL rules, data modeling, and BI applications; Operations support and business analysis experience; Strong MS-SQL and Oracle Database scripting; Experience in diagnosing, troubleshooting, and performing root cause analysis; Ability to diagnose and troubleshoot problems with BI reports and ETL processes; Experience with AWS, Data Lake, Databricks, and the healthcare domain is a plus; Able to work independently and as an effective team player with a strong desire to deliver results; Adaptable, meticulous, and possess strong analytical skills; Good communication skills (both written and spoken); Strong team player; Apply Now; NOTE: It only takes a few minutes to apply for a meaningful career in HealthTech - GO FOR IT!!; #LI-SYNX40",,Full time,singapore
85843199,Backend Data Engineer,Whitehall Resources,Singapore,2025-07-17 12:57:21,https://id.jobstreet.com/id/job/85843199,"Backend Data Engineer; Backend Data Engineer required by Whitehall Resources for a 9-12 Month extendable project assignment for our client headquartered in Germany.; Please note, we are looking for candidates that can either work in CET or APAC time zones. Candidates based in offshore locations such as APAC are preferred.; Start Date: ASAP; Duration: 9-12 Months extendable; Delivery: Fully Remote; Capacity: Full Time; Overview:; We are seeking a skilled Backend Data Engineer to design, build, and maintain scalable data pipelines and infrastructure that support advanced analytics and business intelligence. You will work closely with data scientists, analysts, and platform teams to ensure efficient, secure, and reliable data delivery.; Key Responsibilities:; - Develop and maintain ETL/ELT pipelines using tools such as Apache Spark, Airflow, or dbt; - Design and optimize data warehouses and data lakes (e.g., Snowflake, BigQuery, Redshift); - Integrate data from various sources including APIs, databases, and streaming platforms; - Ensure data quality, consistency, and security through validation and monitoring; - Collaborate with DevOps to automate deployments and CI/CD workflows; Key Skills & Technologies:; - Proficient in Python, SQL, and working with relational & NoSQL databases; - Experience with cloud platforms (AWS, GCP, or Azure); - Knowledge of data modeling, partitioning, and performance tuning; - Familiarity with Kafka, Spark, or other big data technologies; - Understanding of data governance, security, and compliance (GDPR, etc.); Start Date is ASAP!; Please Apply Now!; All of our opportunities require that applicants are eligible to work in the specified country/location, unless otherwise stated in the job description.; Whitehall Resources are an equal opportunities employer who value a diverse and inclusive working environment. All qualified applicants will receive consideration for employment without regard to race, religion, gender identity or expression, sexual orientation, national origin, pregnancy, disability, age, veteran status, or other characteristics.",,Kontrak/Temporer,singapore
85795244,Data Engineer,SMRT Corporation Ltd,Bishan,2025-07-16 17:57:21,https://id.jobstreet.com/id/job/85795244,"Company description:; ; SMRT Corporation Ltd is a public transport service provider. Our primary business is to manage and operate train services on the North-South Line, East-West Line, the Circle Line, the Thomson-East Coast Line and Bukit Panjang Light Rail Transit. This is complemented by our bus, taxi and private hire vehicle services.; ; Our core values are Respect, Integrity, Safety and Service, and Excellence. We are committed to provide safe, reliable and comfortable service for all our commuters.; ; ; Job description:; ; Job Purpose; The Data Engineer will be part of the team to develop operation & maintenance decision-support tools to enhance train reliability and maintenance efficiency. This position involves designing, developing, and maintaining data pipelines, APIs, and cloud infrastructure for various rail-oriented applications. The ideal candidate will have expertise in data analysis, transformation, ingestion, database design, API development, and preferably, cloud infrastructure setup. Collaborating closely with software engineers, data scientists, and frontend developers, the Data Engineer will contribute to building efficient, scalable, and reliable systems.; ; Responsibilities; The duties and responsibilities for Data Engineer, are as listed below. The list is not comprehensive and related duties and responsibilities may be assigned from time to time.; Data Engineering & Processing:; Develop and maintain data pipelines for efficient data ingestion and transformation.; Work with structured and unstructured data to ensure optimal storage and retrieval.; Perform data analysis and report on results.; Database Design & Management:; Design and implement relational and NoSQL database schemas for scalability.; Optimize database performance through indexing, partitioning, and query tuning.; Implement data security and compliance best practices.; API Development & Backend Engineering:; Design and develop APIs for data access and application integration.; Implement authentication, authorization, and API security best practices.; Cloud Infrastructure & Deployment (Supporting Role):; Assist in design Azure cloud architectures; Work with IT infrastructure team to set up cloud infrastructure for application hosting, data storage and processing.; Collaboration & Best Practices:; Collaborate with internal stakeholders to understand their business needs.; Work with software engineers, data scientist, frontend developer to understand the data requirement and design architecture of the data platform.; Implement CI/CD pipelines for automated testing, deployment and monitoring.; Write testable and maintainable code and documentation to deploy to production.; Engage continuously with end-user for feedback and improvements.; Qualifications & Work Experience; Degree in Science, Technology, Engineering or Mathematics (STEM); Previous experience as a data engineer or in a similar role; Data engineering certification is a plus; Knowledge of security best practices in cloud and database management is a plus; Skills; Technical skills include:; Programming and Data processing: MATLAB, Python, SQL, or similar languages.; Databases: My SQL, SQL Server, MongoDB or similar.; Cloud Platforms: Azure; DevOps & CI/CD: Git Lab CI/CD, Docker; Generic skills include:; Strong inclination and eager for continual learning and development; Strong team player; Critical thinking and problem-solving skills; Ability to understand and explain complex data and effective interactions with the stakeholders; Ability to think independently and actively propose solutions to the team.",,Full time,singapore
85113297,Data Engineer (Cloudera),NCS Pte Ltd,Ang Mo Kio Town Centre,2025-06-23 17:57:21,https://id.jobstreet.com/id/job/85113297,"Company Description; NCS is a leading technology services firm that operates across the Asia Pacific region in over 20 cities, providing consulting, digital services, technology solutions, and more. We believe in harnessing the power of technology to achieve extraordinary things, creating lasting value and impact for our communities, partners, and people. Our diverse workforce of 13,000 has delivered large-scale, mission-critical, and multi-platform projects for governments and enterprises in Singapore and the APAC region.; Job Description; Responsibilities:; 1. Cloudera Private Cloud Deployment & Management:; Install, configure, and maintain Cloudera Private Cloud (CDP) clusters.; Manage cluster services (HDFS, YARN, Spark, Hive, Impala, Kafka, etc.).; Monitor cluster health, performance, and capacity planning.; 2. Security & Access Control:; Configure and manage Kerberos, LDAP/Active Directory integration, and TLS/SSL encryption. Implement Role-Based Access Control (RBAC) for users and services.; Ensure compliance with security best practices and audit logging.; 3. Performance Tuning & Troubleshooting:; Optimize cluster performance by tuning configurations (YARN, HDFS, Spark).; Diagnose and resolve issues related to storage, compute, and networking.; Analyze logs and metrics using Cloudera Manager and other monitoring tools.; 4. Data Processing & Optimization:; Develop, optimize, and maintain Spark, Hive, Impala, Nifi, and Kafka applications.; Write efficient SQL queries and optimize Hive/Impala performance.; Build data pipelines for batch and real-time processing; Requirements:; Must-Have Skills:; Linux & Command Line Proficiency: o Strong experience with Linux commands (ls, grep, awk, sed, chmod, ssh).; Managing file permissions, ownership, and shell scripting (Bash).; Programming Languages:; Python (for scripting and automation).; SQL (for database queries and management).; Good-to-Have Skills:; Programming Languages:; Java/Scala (nice to have for application support).; Networking & Security:; Understanding of TLS/SSL, certificates, and data-in-transit encryption.; Knowledge of networking protocols (HTTPS, DNS, NTP) and port management.; Familiarity with Microsoft Active Directory and LDAP authentication.; Scripting & Automation:; Experience with Ansible/Chef/Puppet for infrastructure automation. o Knowledge of GitLab/Bamboo for DevOps and CI/CD pipelines.; Academic Qualifications:; 1. Bachelor’s Degree in:; Computer Science / Information Technology; Software Engineering; Data Analytics / Big Data Technologies.; OR; Diploma in:; Computer Science / Information Technology; Software Engineering o Data Analytics / Big Data Technologies; We are driven by our AEIOU beliefs—Adventure, Excellence, Integrity, Ownership, and Unity—and we seek individuals who embody these values in both their professional and personal lives. We are committed to our Impact: Valuing our clients, Growing our people, and Creating our future.",,Full time,singapore
85009869,Data Architect,Keppel Management Ltd,Central Region,2025-06-18 17:57:21,https://id.jobstreet.com/id/job/85009869,"Job Description; Provide strategic recommendations on leveraging data lakes, data meshes, and serverless architectures to optimize data processing and storage; Assess and recommend AI/ML-enabled data architectures that facilitate scalable feature engineering and model training pipelines; Define and advise on the design of modern, future-ready data architectures that align with business goals and support AI, analytics, and automation; Provide thought leadership on best practices for data modeling, data warehousing, and lakehouse architectures to ensure scalability and performance; Develop data models, schemas, and standards that ensure data integrity, quality, and accessibility; Design & Build solutions with AI Services like Bedrock, Google etc.; Mentor and guide team members in best practices for data architecture and management; Participate in code reviews and contribute to the improvement of data engineering best practices; Job Requirements; Bachelor's degree in Computer Science, Engineering, or a related field; 4-5 years of experience in designing and building high-performance cloud-based data architectures; Experience in Data Engagements, especially leading data strategy, roadmap and implementation; Tech Stack – Python, SQL, AWS, Snowflake, DBT, Airflow, Bedrock, NoSQL; Knowledge of container-based big data architectures and Kubernetes; Familiarity with DevOps / DataOps / MLOps concepts",,Full time,singapore
85009893,Senior Data Engineer,Keppel Management Ltd,Central Region,2025-06-18 17:57:21,https://id.jobstreet.com/id/job/85009893,"Job Description; Develop, maintain scalable data pipelines and build out new integrations to support continuing increases in data volume and complexity; Develop and maintain scalable, optimized data pipelines leveraging Python and AWS services to support increasing data volume and complexity, while ensuring seamless integration with AI platforms like Bedrock and Google; Further enhance data accessibility and drive data-driven decision making by collaborating with analytics and business teams to refine data models for business intelligence tools; Develop data models, schemas, and standards that ensure data integrity, quality, and accessibility; Develop, maintain, and optimize scalable data pipelines using Python and AWS services (e.g., S3, Lambda, ECS, EKS, RDS, SNS/SQS, Vector DB); Build solutions with AI Services like Bedrock, Google etc.; Rapidly developing next-generation scalable, flexible, and high-performance data pipelines; Collaborate with analytics and business teams to create and improve data models for business intelligence; End-to-end ownership of data quality in our core datasets and data pipelines; Participate in code reviews and contribute to DevOps / DataOps / MLOps; Job Requirements:; Bachelor's degree in Computer Science, Engineering, or a related field; 5-6 years of experience in data engineering or a similar role; Strong programming skills in Python, SQL, AWS and related tech stack; Experience with building scalable data pipelines with technologies such as Glue, Airflow, Kafka, Spark etc.; Experience using Snowflake, DBT, Bedrock is a plus; Good understanding of basic machine learning concepts (Sagemaker)",,Full time,singapore
85481336,Data Engineer - Financial Services,Bounteousxaccolite Singapore Pte. Ltd.,Central Region,2025-07-04 17:57:21,https://id.jobstreet.com/id/job/85481336,"Bounteous is a premier end-to-end digital transformation consultancy dedicated to partnering with ambitious brands to create digital solutions for today’s complex challenges and tomorrow’s opportunities. With uncompromising standards for technical and domain expertise, we deliver innovative and strategic solutions in Strategy, Analytics, Digital Engineering, Cloud, Data & AI, Experience Design, and Marketing.; One of our key Financial Services clients is looking for a Lead Data Engineer for a senior position based in Singapore.; ; Job Description; We seek individuals with highly developed conceptual, strategic, and analytical skills, capable of striking a balance between visionary thinking and practical solutions. The ability to comprehend, inspire, and mobilize others is crucial. A business-oriented mindset coupled with effective storytelling will drive your success. We are looking for self-starters ready to take on responsibilities with enthusiasm.; Your Role;  As a Lead Data Engineer, you will play a leading role in designing, building, and optimizing our data infrastructure, ensuring that it supports the advanced analytics need of the bank. You will oversee a team of data engineers, working closely with data analysts, DevOps team, infrastructure engineers, and other stakeholders to deliver high-quality data solution. ; Your main responsibilities will include:; Design, develop, and implement Spark Scala applications and data processing pipelines to process large volumes of structured and unstructured data.; Integrate Elasticsearch with Spark to enable efficient indexing, querying, and retrieval of data.; Optimize and tune Spark jobs for performance and scalability, ensuring efficient data processing and indexing in Elasticsearch.; Collaborate with data engineers, data scientists, and other stakeholders to understand requirements and translate them into technical specifications and solutions.; Design and deploy data engineering solutions on OpenShift Container Platform (OCP) using containerization and orchestration techniques.; Optimize data engineering workflows for containerized deployment and efficient resource utilization.; Collaborate with DevOps teams to streamline deployment processes, implement CI/CD pipelines, and ensure platform stability.; Monitor and optimize data pipeline performance, troubleshoot issues, and implement necessary enhancements.; Implement monitoring and logging mechanisms to ensure the health, availability, and performance of the data infrastructure.; Document data engineering processes, workflows, and infrastructure configurations for knowledge sharing and reference.; Stay updated with emerging technologies, industry trends, and best practices in data engineering and DevOps.; Provide technical leadership, mentorship, and guidance to junior team members to foster a culture of continuous learning and innovation to the continuous improvement of the analytics capabilities within the bank.; Requirements; Bachelor’s degree in computer science, Data Engineering, Information Technology, or a related field.; At least 10 years of experience as a Data Engineer, working with Hadoop, Spark, and data processing technologies in large-scale environments.; Strong expertise in designing and developing data infrastructure using Hadoop, Spark, and related tools (HDFS, Hive, Ranger, etc); Experience with containerization platforms such as OpenShift Container Platform (OCP) and container orchestration using Kubernetes.; Proficiency in programming languages commonly used in data engineering, such as Spark, Python, Scala, or Java.; Knowledge of DevOps practices, CI/CD pipelines, and infrastructure automation tools (e.g., Docker, Jenkins, Ansible, BitBucket); Experience with Grafana, Prometheus, Splunk will be an added benefit; Strong problem-solving and troubleshooting skills with a proactive approach to resolving technical challenges.; Excellent collaboration and communication skills to work effectively with cross-functional teams.; Ability to manage multiple priorities, meet deadlines, and deliver high-quality results in a fast-paced environment.; Experience with cloud platforms (e.g., AWS, Azure, GCP) and their data services is a plus.;  What's on Offer; Bounteous brings together 5000+ employees spanning North America, APAC, and EMEA, and partnerships with leading technology providers. Through advanced digital engineering, technology solutions, and data-driven digital experiences, we create exceptional and efficient business impact and help our clients win.; Bounteous is focused on promoting an inclusive environment and is proud to be an equal opportunity employer. We celebrate the different viewpoints and experiences our diverse group of team members bring to Bounteous. Bounteous does not discriminate on the basis of race, religion, colour, gender identity, sexual orientation, age, physical or mental disability, national origin, or any other status protected under state, or local law.",,Full time,singapore
85817969,Data Engineer,TECHNOPALS CONSULTANTS PTE. LTD.,West Region,2025-07-16 17:57:21,https://id.jobstreet.com/id/job/85817969,"Key Responsibilities:; Design, build, and optimize scalable data pipelines for data extraction, transformation, and loading (ETL/ELT).; Develop and maintain data architectures, databases, and data warehouses.; Collaborate with data scientists and analysts to understand data needs and deliver clean, structured datasets.; Monitor and improve the performance of data systems.; Implement and enforce data quality, security, and governance practices.; Work with cloud platforms (e.g., AWS, Azure, GCP) for data storage and processing.; Automate data processes and workflows using orchestration tools like Airflow or similar.; Maintain documentation of data models, schemas, and systems.; Required Skills and Qualifications:; Bachelor's degree in Computer Science, Engineering, Information Technology, or related field.; Proven experience as a Data Engineer or in a similar role.; Proficient in programming languages such as Python, Scala, or Java.; Strong SQL skills and experience with relational and NoSQL databases (e.g., PostgreSQL, MySQL, MongoDB).; Hands-on experience with ETL tools and data pipeline frameworks (e.g., Apache Spark, Kafka, Airflow).; Experience with cloud data platforms like AWS (Redshift, Glue, S3), Azure (Data Factory, Synapse), or GCP (BigQuery, Dataflow).; Familiarity with data modeling, data warehousing concepts, and data lakes.; Strong problem-solving skills and attention to detail.; Preferred Qualifications:; Experience with big data technologies like Hadoop, Hive, or Presto.; Knowledge of CI/CD and DevOps practices in data engineering.; Experience with version control tools (e.g., Git).; Understanding of data privacy and compliance standards",,Kontrak/Temporer,singapore
85818029,Data Engineer,Nearsource,West Region,2025-07-16 17:57:21,https://id.jobstreet.com/id/job/85818029,"Key Responsibilities:; Design, build, and optimize scalable data pipelines for data extraction, transformation, and loading (ETL/ELT).; Develop and maintain data architectures, databases, and data warehouses.; Collaborate with data scientists and analysts to understand data needs and deliver clean, structured datasets.; Monitor and improve the performance of data systems.; Implement and enforce data quality, security, and governance practices.; Work with cloud platforms (e.g., AWS, Azure, GCP) for data storage and processing.; Automate data processes and workflows using orchestration tools like Airflow or similar.; Maintain documentation of data models, schemas, and systems.; Required Skills and Qualifications:; Bachelor's degree in Computer Science, Engineering, Information Technology, or related field.; Proven experience as a Data Engineer or in a similar role.; Proficient in programming languages such as Python, Scala, or Java.; Strong SQL skills and experience with relational and NoSQL databases (e.g., PostgreSQL, MySQL, MongoDB).; Hands-on experience with ETL tools and data pipeline frameworks (e.g., Apache Spark, Kafka, Airflow).; Experience with cloud data platforms like AWS (Redshift, Glue, S3), Azure (Data Factory, Synapse), or GCP (BigQuery, Dataflow).; Familiarity with data modeling, data warehousing concepts, and data lakes.; Strong problem-solving skills and attention to detail.; Preferred Qualifications:; Experience with big data technologies like Hadoop, Hive, or Presto.; Knowledge of CI/CD and DevOps practices in data engineering.; Experience with version control tools (e.g., Git).; Understanding of data privacy and compliance standards (e.g., GDPR, HIPAA).",,Kontrak/Temporer,singapore
85220289,AVP - Data Platform Operations,The Great Eastern Life Assurance Company Limited,Central Region,2025-06-27 17:57:21,https://id.jobstreet.com/id/job/85220289,"We’re looking for a motivated Platform Engineer to help design, build, and maintain scalable data platform infrastructure that supports smooth operations across our hybrid cloud environment. The role involves close collaboration with database, network, infrastructure, and business teams.; Data & AI Platform Operation and Support ; Configure and administer data platforms including security, performance, and scalability ; Create and manage user accounts and permissions, and manage data access ; Support the administration and daily operations of the data and AI platform, including issue resolution as needed ; Plan and execute upgrades, patches, and maintenance ; Develop and implement backup and recovery strategies ; Collaborate with IT teams to ensure integration with other systems ; Create and maintain SOP documentation ; Align with systems engineering to deploy/ expand environments ; Collaborate with application teams for OS installations and updates ; Platform Optimization, Automation & Developer Enablement ; Drive continuous improvement in automation, optimization, and developer experience ; Explore and implement new data platform capabilities aligned with business needs ; Optimize platform performance and cost ; Champion “code base first” DevOps/ DataOps/ MLOps practices and build CI/CD pipelines for reliable Data & AI workflows ; Observability and Reliability ; Monitor platform performance and troubleshoot issues ; Responsible for high availability and performance of applications on platforms ; Capacity planning and scaling ; 24x7 monitoring of server performance, connectivity, and security ; File system management and monitoring ; Teaming with infrastructure, network, and BI teams to ensure data quality and availability ; Governance and Compliance ; Ensure smooth operations and support for the Data & AI platform ; Drive platform optimization, automation, and developer enablement ; Strengthen observability and system reliability ; Bachelor’s Degree in Computer Science, Information Systems, or a related field ; At least 10 years of experience in platform implementation and administration with mandatory 2 years of hands-on experience in Tableau, Hadoop/Cloudera, Informatica, Dataiku and AWS or equivalent technologies. ; Practical experience with DevOps practices including automation, CI/CD, infrastructure as code, and code-first approaches for data workflows and deployments; Strong expertise in implementing comprehensive monitoring solutions using tools like Prometheus, Grafana, Dynatrace and Datadog, combined with deep experience in SRE practices including SLI/SLO definition, incident response automation, chaos engineering, and distributed tracing across microservices architectures; Skills in driving continuous improvement through automation tools, optimizing platform performance and costs, and enhancing developer experience; Good understanding of OS concepts, process management and resource scheduling; Basics of networking, CPU, memory and storage; Good hold of shell scripting",,Full time,singapore
85763140,Data Engineer - Contract,Manpower Staffing Services (S) Pte Ltd - Head Office,Central Region,2025-07-15 17:57:21,https://id.jobstreet.com/id/job/85763140,"About the Role; As a Senior Data Engineer, you will focus on designing, building, and maintaining scalable data solutions that support business needs and AI applications. You will play a key role in:; Data Pipeline & Architecture Development; Build and optimize automated data pipelines for ingesting, transforming, and processing large datasets.; Design efficient data architectures that support analytics, machine learning, and real-time applications.; Cloud Migration & AI Enablement; Support cloud migration efforts, transitioning on-premises data workflows to cloud-based platforms like Databricks.; Collaborate with data scientists to improve feature selection, feature engineering, and enable end-to-end AI workflows from model training to deployment and monitoring.; CI/CD & Automation; Develop CI/CD pipelines to streamline data pipeline deployments and ensure stable, automated workflows.; Improve monitoring and observability to maintain system reliability.; Collaboration & Business Impact; Work with data scientists, product teams, and platform engineers to align data solutions with business objectives.; Ensure data quality, security, and compliance with industry standards.; Contribute to best practices in data governance, documentation, and automation.; Qualifications & Skills; Degree holder of Information Technology, Mathematics or Statistics with least 3 years of experience in data engineering.; Expert in Python, Java, SQL, Linux Shell.; Experience in UNIX environment, Git Flow, CI/CD automation, Jenkins, Bitbucket.; Hands-on experience with Spark, Hadoop platforms & tools (Hive, Impala, Airflow, NiFi), Spring Boot, etc. to build big data products & platforms.; Proficiency with a modern cloud or hybrid-cloud stack (AWS, Databricks, Cloudera, etc).; Experience in building and deploying production-level data-driven applications and data processing workflows or pipelines.; Interested candidates may send in their resume and cover letter directly to gem.cabria@manpower.com.sg (R1434374), stating the position as the subject title in the email.; Jireli Gem Mejia Cabria EA License No.: 02C3423 Personnel Registration No.: R1434374; Please note that your response to this advertisement and communications with us pursuant to this advertisement will constitute informed consent to the collection, use and/or disclosure of personal data by ManpowerGroup Singapore for the purpose of carrying out its business, in compliance with the relevant provisions of the Personal Data Protection Act 2012. To learn more about ManpowerGroup's Global Privacy Policy, please visit https://www.manpower.com.sg/privacy-policy",$8k - $10k p.m. (SGD),Kontrak/Temporer,singapore
85784716,Data Engineer,Helius Technologies Pte Ltd,Central Region,2025-07-15 17:57:21,https://id.jobstreet.com/id/job/85784716,"🔍 Key Responsibilities:; 6+ Years in Design, develop, and maintain end-to-end data lakes and data warehouse solutions; Work with Microsoft Fabric, Azure Data Factory (ADF), or Informatica IDMC for data integration and orchestration; Build scalable data pipelines using Python/PySpark; Develop and maintain data models for both transactional and analytical systems; Tune and optimize complex queries and analyze performance for large-scale datasets; Collaborate in master data management (MDM) initiatives, preferably in customer domains; (Optional) Contribute to Big Data and DataSecOps initiatives for enterprise-grade solutions; ✅ Key Requirements:; Strong hands-on experience in Azure Data Services, ideally Microsoft Fabric; Proficiency in Python / PySpark scripting; Solid background in data modeling, database design, and SQL performance tuning; Experience with large datasets and real-time data environments; Exposure to MDM tools/processes, particularly in customer data domains; Advantageous to have familiarity with Big Data tools and DataSecOps practices; Thanks, and Best Regards; Karanam Vijaya Kiran; (EA Registration no: R1443178); HP: +65 92333815; Email: vijay@helius-tech.com; Recruitment Manager; Helius Technologies Pte Ltd (EA Licence No: 11C3373)","$7,500 – $8,500 per month (SGD)",Full time,singapore
85603062,"AVP, Data Engineer, Group Asset Management - Business Technology",United Overseas Bank Limited (UOB),Singapore,2025-07-09 17:57:21,https://id.jobstreet.com/id/job/85603062,"United Overseas Bank Limited (UOB) is a leading bank in Asia with a global network of more than 500 branches and offices in 19 countries and territories in Asia Pacific, Europe and North America. In Asia, we operate through our head office in Singapore and banking subsidiaries in China, Indonesia, Malaysia and Thailand, as well as branches and offices.; Our history spans more than 80 years. Over this time, we have been guided by our values – Honorable, Enterprising, United and Committed. This means we always strive to do what is right, build for the future, work as one team and pursue long-term success. It is how we work, consistently, be it towards the company, our colleagues or our customers.; Established in 1986, UOB Asset Management (UOBAM) is a wholly owned subsidiary of United Overseas Bank. Headquartered in Singapore, UOBAM has grown extensively across Asia with local presence in Brunei, Indonesia, Japan, Malaysia, Taiwan, Thailand and Vietnam. Our network includes UOB Islamic Asset Management in Malaysia and a joint venture with China’s Ping An Trust to form Ping An Fund Management Company. We have also forged a strategic alliance with Wellington Management Singapore.; Our experienced team of more than 90 investment professionals conduct rigorous fundamental research within a proven investment framework to provide our clients with innovative investment solutions. The strength of our team lies in our commitment to investment excellence. Our performance has been recognised by the industry and we have garnered over 340 awards regionally since 1986.; Through our regional network, we offer global investment management expertise to individuals, institutions and corporations. Our comprehensive suite of products ranges from retail unit trusts and exchange-traded funds to customised portfolio management services for institutional clients. A leader in innovation, UOBAM offers a digital option to manage investments with UOBAM Invest robo-adviser, making investing simpler, smarter and safer.;   UOBAM Technology provides software and system development, as well as information technology support services and banking operations.;   We have centralized and standardized the technology components into Singapore, creating a global footprint which can be utilized for supporting our regional subsidiaries and the branches around the world. We operate and support 8 countries with this architecture to provide a secure and flexible Asset Management infrastructure.; Develop and maintain infrastructure for enterprise data platforms and machine learning.; Collaborate with business stakeholders to gather requirements and translate them into effective data visualizations and reporting solutions.; Design, build, and maintain scalable and interactive dashboards using BI tools such as Power BI, Tableau, or Looker.; Implement and manage data governance frameworks, including data cataloging, lineage, quality, and access controls using cloud-native tools (e.g., AWS Glue, Azure Purview, Google Cloud Data Catalog).; Data Platform Development & Management:; Design, develop, and maintain data platforms that support large-scale data ingestion, storage, and processing using cloud-based data infrastructure.; Implement and manage data warehousing and centralized data solutions tailored for asset management.; Evaluate and integrate new data technologies and tools to enhance data platform capabilities.; Data Pipeline Development:; Build and maintain robust and efficient data pipelines for data ingestion, processing, and transformation.; Develop and implement data quality checks and validation processes to ensure data accuracy, timeliness, and consistency.; Utilize ETL/ELT tools and techniques to transform and load data into target systems.; Employ exceptional problem-solving skills, with the ability to see and solve issues before they snowball into problems.; Learn and share knowledge and experience in a multi-disciplinary team.; Bachelor's or Master's degree in Computer Science, Data Science, Engineering, or a related field.; At least 5 years of experience in data engineering, business intelligence, machine learning engineering, or a related role in a production environment.; Familiarity with data governance frameworks (e.g., DAMA-DMBOK) and regulatory compliance (e.g., GDPR, CCPA).; Hands-on experience in Python and SQL. Experience with other programming languages (e.g., Java, Scala, C++) is a plus.; Experience in best practices such as DataOps and MLOps; Experience with big data technologies and cloud platforms such as BigQuery, Kafka, GCP, AWS and their data engineering and machine learning products and services.; Strong understanding of software development best practices, including version control (Git), testing, and CI/CD.; Excellent communication and organizational skills, and the ability to stay focused on completing tasks and meeting goals within a busy workspace.; Skilled at working in tandem with a team of engineers, or alone as required.; Strong troubleshooting and analytical skills.; Cloud and data certifications are a plus.; UOB is an equal opportunity employer. UOB does not discriminate on the basis of a candidate's age, race, gender, color, religion, sexual orientation, physical or mental disability, or other non-merit factors. All employment decisions at UOB are based on business needs, job requirements and qualifications. If you require any assistance or accommodations to be made for the recruitment process, please inform us when you submit your online application.; ; Apply now and make a difference.",,Full time,singapore
85150830,Data Engineer,NEO GARDEN CATERING PTE. LTD.,Boon Lay,2025-06-25 17:57:21,https://id.jobstreet.com/id/job/85150830,"We’re looking for a hands-on Data Engineer to lead the transformation of our data landscape. In this role, you’ll drive the migration to Microsoft Fabric, improve data quality in our ERP system, and build robust data architecture to support advanced analytics and future AI initiatives. You’ll work closely with cross-functional teams to enhance reporting, optimize sales operations, and shape our data-driven strategy.; ; Role Summary; Serve as primary data professional responsible for transforming existing data landscape into robust, scalable architecture; Address critical data quality challenges within current ERP system; Lead Microsoft Fabric migration initiative and establish foundation for AI applications; Build comprehensive data solutions while maintaining operational flexibility for sales optimization; Key Responsibilities; Data Infrastructure and Quality Management; Lead data quality remediation for account and items data within ERP system; Develop comprehensive data validation processes and governance standards; Design and implement robust ETL processes for in-house ERP system integration; Establish data integrity frameworks while preserving operational flexibility required for sales growth; Microsoft Fabric Migration and Architecture; Spearhead migration of company data to Microsoft Fabric platform; Design target architecture consolidating data from mixed POS systems and BCMS; Establish unified data models supporting current reporting and future AI applications; Advanced Analytics and AI Foundation; Prepare data infrastructure for AI solutions; Integrate user behavioral data from web analytics tools; Create customer segmentation and personalization data frameworks; Business Intelligence and Reporting Enhancement; Expand existing Power BI dashboard capabilities for comprehensive business insights; Work directly with stakeholders understanding reporting requirements across organizational levels; Establish data-driven decision-making frameworks; Required Qualifications; Educational Background; Bachelor's degree in Computer Science, Information Systems, Data Science, or related technical field; 3–5 years progressive experience in data engineering roles; Background working in small to medium enterprise environments with diverse responsibilities; Technical Expertise; Advanced proficiency in Microsoft ecosystem technologies including SQL Server, Azure data services, and Power BI; Preparation knowledge for Microsoft Fabric implementation and migration; Expertise in data modeling, ETL development, and database optimization; Experience with data quality assessment and remediation methodologies; Proficiency in Python or R for data processing and analysis; Experience integrating diverse data sources including ERP systems and web analytics platforms; Essential Competencies; Strategic and Technical Skills; Ability to design data architecture supporting immediate operational needs and long-term strategic objectives; Strong analytical thinking with proven problem-solving capabilities in complex data environments; Experience leading data transformation projects and managing competing priorities; Communication and Leadership; Excellent communication skills translating technical concepts into business value propositions; Ability to collaborate effectively with non-technical stakeholders across departments; Experience presenting data insights to management and training team members on data tools; Confidence making technical recommendations shaping organizational data strategy; Independent Work and Project Management; Strong project management capabilities with ability to prioritize multiple initiatives simultaneously; Proven ability to work autonomously while making sound technical decisions; Experience establishing data standards and implementing solutions independently; Track record of successful data transformation project leadership; Application Requirements; Resume demonstrating relevant data engineering experience and technical qualifications; Cover letter detailing experience with data quality remediation projects and Microsoft ecosystem implementations","$4,000 – $6,000 per month (SGD)",Full time,singapore
84707702,Senior Data Engineer (2 year contract),StarHub Ltd,Singapore,2025-06-17 17:57:21,https://id.jobstreet.com/id/job/84707702,"Senior Data Engineer; You will design, develop, and deploy AI-powered, cloud-based products. As a Data Engineer, you’ll work with large-scale, heterogeneous datasets and hybrid cloud architectures to support analytics and AI solutions. Collaborate with data scientists, infra engineers, sales specialists, and stakeholders to ensure data quality, build scalable pipelines, and optimize performance. Your work will integrate telco data with other verticals (retail, healthcare), automate DataOps/MLOps/LLMOps workflows, and deliver production-grade systems.; As a Data Engineer, you will:; · Ensure Data Quality & Consistency • Validate, clean, and standardize data (e.g., geolocation attributes) to maintain integrity.; • Define and implement data quality metrics (completeness, uniqueness, accuracy) with automated checks and reporting.; · Build & Maintain Data Pipelines • Develop ETL/ELT workflows (PySpark, Airflow) to ingest, transform, and load data into warehouses (S3, Postgres, Redshift, MongoDB).; • Automate DataOps/MLOps/LLMOps pipelines with CI/CD (Airflow, GitLab CI/CD, Jenkins), including model training, deployment, and monitoring.; · Design Data Models & Schemas • Translate requirements into normalized/denormalized structures, star/snowflake schemas, or data vaults.; • Optimize storage (tables, indexes, partitions, materialized views, columnar encodings) and tune queries (sort/distribution keys, vacuum).; · Integrate & Enrich Telco Data • Map 4G/5G infrastructure metadata to geospatial context, augment 5G metrics with legacy 4G, and create unified time-series datasets.; • Consume analytics/ML endpoints and real-time streams (Kafka, Kinesis), designing aggregated-data APIs with proper versioning (Swagger/OpenAPI).; · Manage Cloud Infrastructure • Provision and configure resources (AWS S3, EMR, Redshift, RDS) using IaC (Terraform, CloudFormation), ensuring security (IAM, VPC, encryption).; • Monitor performance (CloudWatch, Prometheus, Grafana), define SLAs for data freshness and system uptime, and automate backups/DR processes.; · Collaborate Cross-Functionally & Document; • Clarify objectives with data owners, data scientists, and stakeholders; partner with infra and security teams to maintain compliance (PDPA, GDPR).; • Document schemas, ETL procedures, and runbooks; enforce version control and mentor junior engineers on best practices.; Qualifications; · Bachelor’s or Master’s in Computer Science, Software Engineering, Data Science, or equivalent experience; · 4+ years in data engineering, analytics, or related AI/ML role; · Proficient in Python for ETL/data engineering and Spark (PySpark) for large-scale pipelines; · Experience with Big Data frameworks and SQL engines (Spark SQL, Redshift, PostgreSQL) for data marts and analytics; · Hands-on with Airflow (or equivalent) to orchestrate ETL workflows and GitLab CI/CD or Jenkins for pipeline automation; · Familiar with relational (PostgreSQL, Redshift) and NoSQL (MongoDB) stores: data modeling, indexing, partitioning, and schema evolution; · Proven ability to implement scalable storage solutions: tables, indexes, partitions, materialized views, columnar encodings; · Skilled in query optimization: execution plans, sort/distribution keys, vacuum maintenance, and cost-optimization strategies (cluster resizing, Spectrum); · Experience with cloud platforms (AWS): S3/EMR/Glue, Redshift and containerization (Docker, Kubernetes); · Infrastructure as Code using Terraform or CloudFormation for provisioning and drift detection; · Knowledge of MLOps/LLMOps: auto-scaling ML systems, model registry management, and CI/CD for model deployment; · Strong problem-solving, attention to detail, and the ability to collaborate with cross-functional teams; Nice to Have; · Exposure to serverless architectures (AWS Lambda) for event-driven pipelines; · Familiarity with vector databases, data mesh, or lakehouse architectures; · Experience using BI/visualization tools (Tableau, QuickSight, Grafana) for data quality dashboards; · Hands-on with data quality frameworks (Deequ) or LLM-based data applications (NL-->SQL generation); · Participation in GenAI POCs (RAG pipelines, Agentic AI demos, geomobility analytics); · Client-facing or stakeholder-management experience in data-driven/AI projects",,Kontrak/Temporer,singapore
85789645,DATA Engineer,KRISE SOLUTIONS PTE. LTD.,Central Region,2025-07-15 17:57:21,https://id.jobstreet.com/id/job/85789645,"Job Summary:; We are looking for a skilled and detail-oriented Data Engineer to join our team. The ideal candidate will be responsible for designing, building, and maintaining scalable data pipelines and infrastructure. You will work closely with data scientists, analysts, and business teams to ensure the availability, reliability, and accuracy of data.; Key Responsibilities:; Design, build, and optimize scalable data pipelines for data extraction, transformation, and loading (ETL/ELT).; Develop and maintain data architectures, databases, and data warehouses.; Collaborate with data scientists and analysts to understand data needs and deliver clean, structured datasets.; Monitor and improve the performance of data systems.; Implement and enforce data quality, security, and governance practices.; Work with cloud platforms (e.g., AWS, Azure, GCP) for data storage and processing.; Automate data processes and workflows using orchestration tools like Airflow or similar.; Maintain documentation of data models, schemas, and systems.; Required Skills and Qualifications:; Bachelor's degree in Computer Science, Engineering, Information Technology, or related field.; Proven experience as a Data Engineer or in a similar role.; Proficient in programming languages such as Python, Scala, Java.; Strong SQL skills and experience with relational and NoSQL databases (e.g., PostgreSQL, MySQL, MongoDB).; Hands-on experience with ETL tools and data pipeline frameworks (e.g., Apache Spark, Kafka, Airflow).; Experience with cloud data platforms like AWS (Redshift, Glue, S3), Azure (Data Factory, Synapse), or GCP (BigQuery, Dataflow).; Familiarity with data modeling, data warehousing concepts, and data lakes.; Strong problem-solving skills and attention to detail.; Experience with big data technologies like Hadoop, Hive, or Presto.; Knowledge of CI/CD and DevOps practices in data engineering.; Experience with version control tools (e.g., Git).; Understanding of data privacy and compliance standards (e.g., GDPR, HIPAA).",,"Kontrak/Temporer, Full time",singapore
85685073,Assistant Lead Engineer - Data (Engineering & Ops),Synapxe,One North,2025-07-11 17:57:21,https://id.jobstreet.com/id/job/85685073,"Position Overview; The data engineer will be attached to the L3 Operations team lead to oversee/manage the following areas:; Administration and operations processes in data virtualization and visualization applications and components, including Tableau, OAS, Sagemaker, Appsstream; Technical refresh projects for application components near EOS/EOL; Security compliance and deviations; Facilitate the operations transition after migration to HEALIX; Role & Responsibilities; Manage systems administration function and lead the organisation's system projects and environments for sustainable operations; Plan and oversee systems upgrades and migrations, ensuring systems are up-to-date with the latest patches and coherent across the organisation; Address multi-faceted issues effectively and collaboratively across departments; Integrate diverse needs and perspectives from internal and external clients; Develop new and innovative ideas and solutions; Influence key stakeholders and clients to adopt a data-driven approach to resolve business issues; Oversee budgeting and planning; Set data standards, governance, and best practices; Develop standards, policies, and controls to ensure system security and compliance; Execute standard operating procedures to ensure system security and compliance; Maintain up-to-date inventory of existing system assets and architecture artifacts with the latest technologies; Provide guidance on best practices related to data governance and security compliance processes; Continually educate staff on the latest changes in standard operations practices and policies; Performance Optimization and Continuous Improvement:; Recommend process, product, or service improvements, resource optimization, and cost savings; Oversee hardware and software upgrades; Review resource utilization to identify areas for improvement or resolve bottlenecks; Recommend new technologies, methodologies, systems, or opportunities for cost savings, security, and service quality improvement; Optimize and automate processes to achieve higher efficiency and sustainability; Team Management:; Oversee team management, including budgets, forecasting, work allocations, and staffing; Develop staff through ongoing coaching, mentoring, and career discussions; Define common goals, direction, and accountability among staff; Drive effective performance management practices within the department in accordance with company policies and procedures; Requirements; Degree in Computer Science, Computer Engineering; Minimum 12-15 year working experience in system operations compliance and management areas; Highly-motivated self-starter who will undertake all activities to the highest professional standards; Must be cloud certified; Good in-depth understanding of data warehouse concepts, data profiling, data verification and advanced analytics techniques; Experience in managing a team in project implementation or support is a must; Certification in IT operation management is preferred; Possess prior hands-on experience and technical expertise such as Databricks, Informatica, Tableau, OBIEE, SQL databases & AWS cloud technologies; Good interpersonal skills with the ability to work with different groups of stakeholders; Exposure to hospital information / clinical systems is an added advantage; Apply Now; NOTE: It only takes a few minutes to apply for a meaningful career in HealthTech - GO FOR IT!!; #LI-SYNX34",,Full time,singapore
85570215,Contract - Data Engineer [AI Data Pipeline] (1 year),Infineon Technologies,Kallang,2025-07-08 17:57:21,https://id.jobstreet.com/id/job/85570215,"#WeAreIn for driving decarbonization and digitalization.; As a global leader in semiconductor solutions in power systems and IoT, Infineon enables game-changing solutions for green and efficient energy, clean and safe mobility, as well as smart and secure IoT. Together, we drive innovation and customer success, while caring for our people and empowering them to reach ambitious goals. Be a part of making life easier, safer and greener.; Are you in?; ; We are on a journey to create the best Infineon for everyone.; This means we embrace diversity and inclusion and welcome everyone for who they are. At Infineon, we offer a working environment characterized by trust, openness, respect and tolerance and are committed to give all applicants and employees equal opportunities. We base our recruiting decisions on the applicant´s experience and skills.; Please let your recruiter know if they need to pay special attention to something in order to enable your participation in the interview process.; Click here for more information about Diversity & Inclusion at Infineon.; The Data Engineer will serve as a technical expert in the fields of design and develop AI data pipelines to manage both large unstructured and structured datasets, with a particular focus on GenAI RAG/Agent solutions.; ; In your new role you will:; Working closely with data scientists and domain experts to design and develop AI data pipelines using agile development process.; Developing pipelines for ingesting and processing large unstructured and structured datasets from a variety of sources, ensure efficient and effective data processing.; Development of BIA solution using defined framework for Data Modelling; Data Profiling; Data Extraction, Transformation & Loading; Design and provide data/information in form of reports, dashboards, scorecards and data storytelling using Visualization Tools such as Business Objects & Tableau.; Work with cloud technologies such as AWS to design and implement scalable data architectures; Supporting the operation of the data pipelines involves troubleshooting and bug fixing, as well as implementing change requests to ensure that the data pipelines continue to meet user requirements.; You are best equipped for this task if you have:; Master's or Bachelor's Degree in Computer Science/Mathematics/ Statistics or equivalent.; Minimum of 3 years of relevant work experience in data engineering, including in-depth technical knowledge of databases, BI tools, SQL, OLAP, ETL, RAG / Agentic Data pipeline.; Proficient in RDBMS: Oracle/PL SQL; Extensive hands-on experience in conceptualising, designing, and implementing data pipelines. Proficiency in handling unstructured data formats (e.g., PPT, PDF, Docx), databases (RDMS, NoSQL such as Elasticsearch, MongoDB, Neo4j, CEPH) and familiarity with big data platforms (HDFS, Spark, Impala).; Experience in working with AWS technologies focusing on building scalable data pipelines.; Front-end Reporting & Dashboard and Data Exploration tools -Tableau; Strong background in Software Engineering & Development cycles (CI/CD) with proficiency in scripting languages, particularly Python.; Good understanding and experience with Kubernetes / Openshift Platform.; Other Skills / Attributes:; Good understanding of data management, data governance, and data security practices.; Highly motivated, structured and methodical with high degree of self-initiative; Team player with good cross-cultural skills to work in an international team; Customer and result-oriented; This is a 12 months contract under 3rd party payroll partner and entitled to benefits according to partner company",,Kontrak/Temporer,singapore
85576661,Senior Lead Engineer - Data (Engineering & Ops),Synapxe,One North,2025-07-08 17:57:21,https://id.jobstreet.com/id/job/85576661,"Position Overview; The data engineer will be attached to the L3 Operations team lead to oversee/manage the following areas:; Administration and operations processes in data virtualization and visualization applications and components, including Tableau, OAS, Sagemaker, Appsstream; Technical refresh projects for application components near EOS/EOL; Security compliance and deviations; Facilitate the operations transition after migration to HEALIX; Role & Responsibilities; Manage systems administration function and lead the organisation's system projects and environments for sustainable operations; Plan and oversee systems upgrades and migrations, ensuring systems are up-to-date with the latest patches and coherent across the organisation; Address multi-faceted issues effectively and collaboratively across departments; Integrate diverse needs and perspectives from internal and external clients; Develop new and innovative ideas and solutions; Influence key stakeholders and clients to adopt a data-driven approach to resolve business issues; Oversee budgeting and planning; Set data standards, governance, and best practices; Develop standards, policies, and controls to ensure system security and compliance; Execute standard operating procedures to ensure system security and compliance; Maintain up-to-date inventory of existing system assets and architecture artifacts with the latest technologies; Provide guidance on best practices related to data governance and security compliance processes; Continually educate staff on the latest changes in standard operations practices and policies; Performance Optimization and Continuous Improvement:; Recommend process, product, or service improvements, resource optimization, and cost savings; Oversee hardware and software upgrades; Review resource utilization to identify areas for improvement or resolve bottlenecks; Recommend new technologies, methodologies, systems, or opportunities for cost savings, security, and service quality improvement; Optimize and automate processes to achieve higher efficiency and sustainability; Team Management:; Oversee team management, including budgets, forecasting, work allocations, and staffing; Develop staff through ongoing coaching, mentoring, and career discussions; Define common goals, direction, and accountability among staff; Drive effective performance management practices within the department in accordance with company policies and procedures; Requirements; Degree in Computer Science, Computer Engineering; Minimum 12-15 year working experience in system operations compliance and management areas; Highly-motivated self-starter who will undertake all activities to the highest professional standards; Must be cloud certified; Good in-depth understanding of data warehouse concepts, data profiling, data verification and advanced analytics techniques; Experience in managing a team in project implementation or support is a must; Certification in IT operation management is preferred; Possess prior hands-on experience and technical expertise such as Databricks, Informatica, Tableau, OBIEE, SQL databases & AWS cloud technologies; Good interpersonal skills with the ability to work with different groups of stakeholders; Exposure to hospital information / clinical systems is an added advantage; Apply Now; NOTE: It only takes a few minutes to apply for a meaningful career in HealthTech - GO FOR IT!!; #LI-SYNX34",,Full time,singapore
85371151,"Data Engineer (Azure, PySpark)",Sembcorp Industries Ltd,Central Region,2025-07-02 17:57:21,https://id.jobstreet.com/id/job/85371151,"About Sembcorp; Sembcorp is a leading energy and urban solutions provider headquartered in Singapore. Led by its purpose to drive energy transition, Sembcorp delivers sustainable energy solutions and urban developments by leveraging its sector expertise and global track record.; Purpose & Scope; We are seeking a highly skilled and self-driven Azure Data Engineer with expertise in PySpark, Python, and modern Azure data services including Synapse Analytics and Azure Data Explorer. The ideal candidate will design, develop, and maintain scalable data pipelines and architectures, enabling effective data management, analytics, and governance.; Key Roles and Responsibilities; Design, develop, and maintain scalable and efficient data pipelines (both batch and real-time streaming) using modern data engineering tools.; Build and manage data lakes, data warehouses, and data marts using Azure Data Services.; Integrate data from various sources including APIs, structured/unstructured files, IoT devices, and real-time streams.; Develop and optimize ETL/ELT workflows using tools such as Azure Data Factory, Databricks, and Apache Spark.; Implement real-time data ingestion and processing using Azure Stream Analytics, Event Hubs, or Kafka.; Ensure data quality, availability, and security across the entire data lifecycle.; Collaborate with analysts, data scientists, and engineering teams to deliver business-aligned data solutions.; Contribute to data governance efforts and ensure compliance with data privacy standards.; Establish and manage source system connectivity (on-prem, APIs, sensors, etc.).; Handle deployment and migration of data pipeline artifacts between environments using Azure DevOps.; Design, develop, and troubleshoot PySpark scripts and orchestration pipelines.; Perform data integration using database joins and other transformations aligned with project requirements.; Qualifications, Skills & Experience; Bachelor’s Degree in Computer Science, Engineering, or related field; 3–5 years of experience in Azure-based data engineering, PySpark, and Big Data technologies; Strong hands-on experience with Azure Synapse Analytics for pipeline orchestration and data handling; Expertise in SQL, data warehousing, data marts, and ingestion using PySpark and Python; Solid experience building and maintaining cloud-based ETL/ELT pipelines, especially with Azure Data Factory or Synapse; Familiarity with cloud data environments such as Azure and optionally AWS; Experience with Azure DevOps for CI/CD and artifact deployment; Excellent communication, problem-solving, and interpersonal skills; 1–2 years of experience working with Azure Data Explorer (including row-level security and access controls).; Experience with Azure Purview for metadata management, data lineage, governance, and discovery; Ability to work independently and take full ownership of assignments; Proactive in identifying and resolving blockers and escalating when needed; Exposure to real-time processing with tools like Azure Stream Analytics or Kafka; Our Culture at Sembcorp; At Sembcorp, our culture is shaped by a strong set of shared behaviours that guide the way we work and uphold our commitment to driving the energy transition.; We foster an institution-first mindset, where the success of Sembcorp takes precedence over individual interests. Collaboration is at the heart of what we do, as we work seamlessly across markets, businesses, and functions to achieve our goals together. Accountability is a core principle, ensuring that we take ownership of our commitments and deliver on them with integrity and excellence. These values define who we are and create a workplace where our people can thrive while making a meaningful impact on driving energy transition.; Join us in making a real impact!",,Full time,singapore
85072379,Data engineer - Azure,Flintex Consulting Pte Ltd,City Hall,2025-06-21 17:57:21,https://id.jobstreet.com/id/job/85072379,"Benefits: 13th Month Salary; Data engineer (Azure) – Synapse and Pyspark, Python,  Datawarehouse and Power BI , Azure Devops; Skills & Experience; Bachelor’s Degree in Computer Science or Engineering with 3-5 years of experience in Azure Data engineering, Python, Pyspark or Big Data development; Sound Knowledge of Azure Synapse analytics for pipelines, orchestration, set up; 1-2  experience in Visualization design and development with Power BI. Knowledge on row-level security, access control; Sound experience in SQL, Datawarehouse, data marts, data ingestion with Pyspark and Python; Expertise in developing and maintaining ETL processing pipelines in cloud-based platforms such as AWS, Azure, etc. (Azure Synapse or data factory preferred); Team player with good interpersonal, communication, and problem-solving skills.; Job Scope; Design, review and development of Pyspark scripts. Testing, troubleshooting of data pipelines, orchestration; Designing and developing reports and dashboards in Power BI, setting up access control with row-level security, DAX query experience; Establishing connections to source data systems, including internal systems e.g. SAP, Historians, Data Lake, etc. as well as external systems such as Web APIs, etc; Managing the collected data in appropriate storage/data-base solutions e.g. file systems, SQL servers, Big Data platforms such as Hadoop, HANA, etc. as required by the specific project requirements; Design, development of data marts and relevant data pipelines using pyspark, data copy activities for batch ingestion; Deployment of pipeline artifacts from one environment to the other using Azure Devops; Performing data integration e.g. using database table joins, or other mechanisms at an appropriate level as required by the analysis requirements of the project.; Good to have; Data catalog with Purview  enabling effective metadata management, lineage tracking, and data discovery; Candidates should demonstrate the ability to leverage Purview to ensure data governance, compliance, and efficient data exploration within Azure environments.; Others; Able to work independently on assignment according to agreed schedule without much supervision; Own assignment and take initiative to resolve issues hinder completion of assignment Proactively reach out for help/guidance whenever required.","$7,000 – $10,000 per month (SGD)",,singapore
85550111,"Data Engineer (ETL & Data Integration) / $5,500",APBA TG Human Resource Pte Ltd,Jurong East,2025-07-07 17:57:21,https://id.jobstreet.com/id/job/85550111,"Key Responsibilities:; Design, develop, and maintain scalable ETL pipelines for production-grade datasets.; Partner with business units to model data across multiple domains.; Ensure the timely and reliable delivery of high-quality datasets for analytics, reporting, and application consumption.; Curate large and complex datasets to support data science and BI initiatives.; Implement and maintain data virtualization views for seamless data access.; Collaborate with infrastructure teams on secure, efficient deployment practices.; Monitor and optimize data workflows for performance and reliability.; Ensure compliance with data governance, access control, and security policies.; Automate and schedule data jobs using enterprise-grade schedulers.; Participate in continuous improvement of data architecture and engineering standards.; Technical Skill Set:; ETL & Data Streaming: Talend, Qlik Replicate (Attunity), Apache Kafka, REST APIs, JSON/XML; Databases & Querying: Proficient in SQL; experience with MS SQL Server and MongoDB; Data Virtualization: Experience with Denodo (VQL, data services, scheduling); Scheduling Tools: Talend Scheduler, Denodo Scheduler, Unix/Linux cron jobs; Programming & Scripting: SQL, Python, Shell scripting; Data Modeling: Familiarity with ER modeling and dimensional modeling (e.g., Kimball methodology); Infrastructure & Deployment: Linux/Unix, Kubernetes (on Nutanix), Jenkins CI/CD, Nexus Repository; Security & Access Control: Knowledge of LDAP, SAML, Azure Entra ID; To Apply, please kindly email your updated resume to weizhe.teoh@tg-hr.com; Regret to inform that only shortlisted candidates will be notified.; CEI: R25127749; EA License: 14C7275","$4,500 – $5,500 per month (SGD)",Kontrak/Temporer,singapore
85038918,Contract - Data Engineer [AI Data Pipeline Dvt & Mgt] (1 yr),Infineon Technologies,Kallang,2025-06-19 17:57:21,https://id.jobstreet.com/id/job/85038918,"#WeAreIn for driving decarbonization and digitalization.; As a global leader in semiconductor solutions in power systems and IoT, Infineon enables game-changing solutions for green and efficient energy, clean and safe mobility, as well as smart and secure IoT. Together, we drive innovation and customer success, while caring for our people and empowering them to reach ambitious goals. Be a part of making life easier, safer and greener.; Are you in?; ; We are on a journey to create the best Infineon for everyone.; This means we embrace diversity and inclusion and welcome everyone for who they are. At Infineon, we offer a working environment characterized by trust, openness, respect and tolerance and are committed to give all applicants and employees equal opportunities. We base our recruiting decisions on the applicant´s experience and skills.; Please let your recruiter know if they need to pay special attention to something in order to enable your participation in the interview process.; Click here for more information about Diversity & Inclusion at Infineon.; The Data Engineer will serve as a technical expert in the fields of design and develop AI data pipelines to manage both large unstructured and structured datasets, with a particular focus on GenAI RAG/Agent solutions.; ; In your new role you will:; Working closely with data scientists and domain experts to design and develop AI data pipelines using agile development process.; Developing pipelines for ingesting and processing large unstructured and structured datasets from a variety of sources, ensure efficient and effective data processing.; Development of BIA solution using defined framework for Data Modelling; Data Profiling; Data Extraction, Transformation & Loading; Design and provide data/information in form of reports, dashboards, scorecards and data storytelling using Visualization Tools such as Business Objects & Tableau.; Work with cloud technologies such as AWS to design and implement scalable data architectures; Supporting the operation of the data pipelines involves troubleshooting and bug fixing, as well as implementing change requests to ensure that the data pipelines continue to meet user requirements.; You are best equipped for this task if you have:; Master's or Bachelor's Degree in Computer Science/Mathematics/ Statistics or equivalent.; Minimum of 3 years of relevant work experience in data engineering, including in-depth technical knowledge of databases, BI tools, SQL, OLAP, ETL, RAG / Agentic Data pipeline.; Proficient in RDBMS: Oracle/PL SQL; Extensive hands-on experience in conceptualising, designing, and implementing data pipelines. Proficiency in handling unstructured data formats (e.g., PPT, PDF, Docx), databases (RDMS, NoSQL such as Elasticsearch, MongoDB, Neo4j, CEPH) and familiarity with big data platforms (HDFS, Spark, Impala).; Experience in working with AWS technologies focusing on building scalable data pipelines.; Front-end Reporting & Dashboard and Data Exploration tools -Tableau; Strong background in Software Engineering & Development cycles (CI/CD) with proficiency in scripting languages, particularly Python.; Good understanding and experience with Kubernetes / Openshift Platform.; Other Skills / Attributes:; Good understanding of data management, data governance, and data security practices.; Highly motivated, structured and methodical with high degree of self-initiative; Team player with good cross-cultural skills to work in an international team; Customer and result-oriented; This is a 12 months contract under 3rd party payroll partner and entitled to benefits according to partner company",,Full time,singapore
85550127,"Data Quality Engineer ($7,000)",APBA TG Human Resource Pte Ltd,Jurong East,2025-07-07 17:57:21,https://id.jobstreet.com/id/job/85550127,"Key Responsibilities:; Define and implement data quality checks, validation rules, and exception handling workflows.; Collaborate with business stakeholders to establish metadata standards, business rules, and data lifecycle policies.; Support and promote adoption of enterprise data governance frameworks.; Monitor, report, and proactively remediate data quality issues.; Maintain and enhance documentation, including data dictionaries, glossaries, and lineage.; Assist with data classification and access control aligned with regulatory and security policies.; Contribute to auditing and compliance efforts related to data usage and governance.; Use data profiling and cleansing tools to assess data health and readiness.; Coordinate with technical teams to embed quality controls into data pipelines.; Drive continuous improvement in data stewardship practices.; Technical Skill Set:; Solid grasp of data quality dimensions (accuracy, completeness, consistency, timeliness, validity, uniqueness); Familiarity with tools like Talend, Dataiku, Denodo for profiling, cleansing, and validation; Experience with structured and semi-structured data (SQL Server, MongoDB, APIs); Hands-on experience with metadata management and lineage tracking tools; Working knowledge of data governance frameworks (e.g., DAMA-DMBOK), business glossaries, and cataloging systems; Proficient in SQL, Python, and Shell scripting for automated data quality checks; Comfortable in Linux environments; Understanding of data classification, access control, and security policies; Experience documenting data standards, definitions, and stewardship ownership; Strong communication and stakeholder engagement skills; To Apply, please kindly email your updated resume to weizhe.teoh@tg-hr.com; Regret to inform that only shortlisted candidates will be notified.; CEI: R25127749; EA License: 14C7275","$5,000 – $7,000 per month (SGD)",,singapore
85746300,Senior Data Engineer,Adecco Personnel Pte Ltd.,Central Region,2025-07-14 17:57:21,https://id.jobstreet.com/id/job/85746300,"A regional tech services firm was formed through a major collaboration between a global hardware leader and a long-standing IT solutions provider in Asia. This partnership brought together thousands of professionals to deliver a full range of enterprise services, from system integration and software development to infrastructure support and managed operations. With strong capabilities across both global delivery and local implementation, the company plays a key role in driving digital transformation initiatives across the Asia-Pacific region. One party holds a significant majority stake in the venture, reinforcing its long-term commitment to expanding service offerings beyond hardware.; Join our client's presales team to design and recommend cutting-edge data and AI/ML solutions for government and enterprise clients. You'll drive the technical vision, build POCs, and collaborate across teams to deliver scalable, secure, and high-impact data strategies.; Key Responsibilities:; Design and optimize data pipelines and architectures for analytics and ML use cases; Evaluate and recommend modern data/AI/ML technologies; Handle diverse data types (video, audio, tabular, text); Build POCs and demos to showcase solution feasibility; Communicate technical solutions and ROI to varied stakeholders; Support AI solution development to promote adoption internally and externally; Requirements:; Degree in Computer Science, Engineering, or related field; 3-5 years in data engineering or presales/solution engineering; Certified in Azure/AWS/GCP Data Engineering (e.g., DP-203, Databricks, Snowflake); Experience in Spark, Kafka, Airflow, MLOps (CI/CD, model deployment); Strong with cloud platforms (AWS, Azure, GCP), Linux, Docker/Kubernetes, Terraform; Familiar with government standards (e.g., IM8, GCC); Preferred:; TOGAF or architecture certification; Presales experience; Hands-on with Azure Data Factory, AWS Glue, or Google DataFlow; ; Wilson Tay; Direct Line: 6697 7866; EA License No: 91C2918; Personnel Registration Number: R2091205",$10k - $12k p.m. + Bonus (SGD),Full time,singapore
85703055,Data Engineer,ONE NORTH AI PTE. LTD.,Singapore,2025-07-13 17:57:21,https://id.jobstreet.com/id/job/85703055,"One North, a Singapore based firm specializing in providing Technology Solutions is currently hiring Data Engineers with about 5~10 years of experience especially in Databricks as per details given below.; Job Description & Requirements: -; As Data Engineer, you will support Data Engineering team in setting up the Data Lake on Cloud and the implementation of standardized Data Model, single view of customer.; You will develop data pipelines for new sources, data transformations within the Data Lake , implementing GRAPHQL, work on no sql database, CI/CD and data delivery as per the business requirements.; Job Description:; Build pipelines to bring in a wide variety of data from multiple sources within the organization as well as from social media and public data sources.; Collaborate with cross functional teams to source data and make it available for downstream consumption.; Work with the team to provide an effective solution design to meet business needs.; Ensure regular communication with key stakeholders, understand any key concerns in how the initiative is being delivered or any risks/issues that have either not yet been identified or are not being progressed.; Ensure dependencies and challenges (risks) are escalated and managed. Escalate critical issues to the Sponsor and/or Head of Data Engineering team.; Ensure timelines (milestones, decisions and delivery) are managed and achieved, without compromising quality and within budget.; Ensure an appropriate and coordinated communications plan is in place for initiative execution and delivery, both internal and external.; Ensure final handover of initiative to business-as-usual processes, carry out a post implementation review (as necessary) to ensure initiative objectives have been delivered, and any lessons learnt are included in future processes.; Who we are looking for:; Competencies & Personal Traits:-; Expertise in Databricks; Experience with at least one Cloud Infra provider (Azure/AWS); Experience in building data pipelines using batch processing with Apache Spark (Spark SQL, Dataframe API) or Hive query language (HQL); Experience in building streaming data pipeline using Apache Spark Structured Streaming or Apache Flink on Kafka & Data Lake; Knowledge of NOSQL databases.; Expertise in Cosmos DB, Restful APIs and GraphQL; Knowledge of Big data ETL processing tools, Data modelling and Data mapping.; Experience with Hive and Hadoop file formats (Avro / Parquet / ORC); Basic knowledge of scripting (shell / bash); Experience of working with multiple data sources including relational databases (SQL Server / Oracle / DB2 / Netezza), NoSQL / document databases, flat files; Experience with CI CD tools such as Jenkins, JIRA, Bitbucket, Artifactory, Bamboo and Azure Dev-ops.; Basic understanding of DevOps practices using Git version control; Ability to debug, fine tune and optimize large scale data processing jobs; Excellent problem analysis skills; Working Experience; 7+ years (no upper limit) of experience working with Enterprise IT applications in cloud platform and big data environments.; Professional Qualifications; Certifications related to Data and Analytics would be an added advantage",,Full time,singapore
85572857,Snowflake Data Engineer (6 Months Extendable),Robert Half International Pte Ltd,Central Region,2025-07-08 17:57:21,https://id.jobstreet.com/id/job/85572857,"The Company; ; We are seeking an experienced Snowflake Data Engineer to support our client within the Manufacturing industry on a mission-critical project for 6 months with potential of extension. This role is critical in supporting enterprise-wide data integration, transformation, and analytics initiatives by designing robust ETL/ELT solutions and optimizing data workflows across platforms.; ; ; The Role; Design, develop, and optimize complex SQL-based transformations on the Snowflake platform.; Build and maintain reliable ETL/ELT pipelines using Snowflake and Fivetran to seamlessly ingest data from diverse sources such as SharePoint and Oracle ERP.; Collaborate closely with analytics teams to create and optimize Power BI data models and reports that empower business decision-making.; Partner with the Data Architect Manager to drive best practices in data architecture, modeling, and governance across the organization.; Lead efforts in ensuring data quality and integrity by implementing validation checks and quickly resolving inconsistencies.; Monitor and optimize pipeline performance and Snowflake warehouse usage to maximize efficiency and control costs.; Provide ongoing technical support and knowledge transfer to internal teams and end users across APAC.; Document data flows, transformation logic, and system configurations clearly to maintain transparency and support ongoing development.; Use GitLab to manage code, collaborate with peers through version control, and automate deployment with CI/CD pipelines.; Participate in Agile ceremonies: sprint planning, stand-ups, retrospectives, and peer reviews.; Your Profile; Degree in Computer Science, Information Technology, Data Science, Software Engineering, or a related field.; 8+ years' experience in data engineering or enterprise data warehousing; Expert in Snowflake, advanced SQL, data modeling (star/snowflake schemas), performance tuning, and security.; Proven skills building and managing ETL/ELT pipelines, preferably using Fivetran.; Hands-on experience with Power BI for data modeling, report building, and DAX.; Comfortable with GitLab for version control, CI/CD, and code collaboration.; Experience with cloud platforms (AWS, Azure, or GCP) and orchestration tools like dbt, Airflow, or NiFi.; Strong focus on data quality, governance, and lineage best practices.; Excellent problem-solving, communication, and teamwork skills.; ; Apply Today; ; Please send your resume, in WORD format only and quote reference number GO13257376, by clicking the apply button. Please note that only short-listed candidates will be contacted.; ; ; Robert Half International Pte Ltd. Co. Registration no.: 200612189E | EA Licence No.: 07C5595 | Gabriela De Brito Lopes Prestes Oxby EA Registration no.: 1989404; By clicking 'apply', you give your express consent that Robert Half may use your personal information to process your job application and to contact you from time to time for future employment opportunities. For further information on how Robert Half processes your personal information and how to access and correct your information, please read the Robert Half privacy notice: https://www.roberthalf.com.sg/privacy-statement. Please do not submit any sensitive personal data to us in your resume (such as government ID numbers, ethnicity, gender, religion, marital status or trade union membership) as we do not collect your sensitive personal data at this time.",Competitive (SGD),Kontrak/Temporer,singapore
85576102,Data Engineer,RSK Group,Singapore,2025-07-08 17:57:21,https://id.jobstreet.com/id/job/85576102,"We are seeking a Data Engineer with experience or interest in IoT technologies and cloud-based data engineering to join our team. This role blends the management of high-volume data flows and IoT-specific analytics with general data engineering practices to deliver robust and scalable data solutions. The ideal candidate will balance technical skills and domain knowledge, enabling data-driven insights for utility operations, customer services, and infrastructure optimisation initiatives.; Key Responsibilities:; Design, develop, and maintain scalable and efficient data pipelines and ETL processes for IoT and enterprise data systems.; Implement data ingestion workflows from IoT devices and integrate with enterprise platforms using Azure Data Factory or similar tools.; Ensure data quality through validation, cleansing, and monitoring processes to address issues such as missing data, duplicates, and inconsistencies.; Define data attributes and formats for IoT device and network data to support seamless integration with existing systems and standards.; Optimise data storage solutions in Azure Data Lake Storage (or equivalent) for structured and unstructured data.; Develop APIs and data interfaces for real-time or near-real-time data transfer between IoT components and enterprise platforms.; Apply advanced analytics techniques to IoT data for performance monitoring, usage profiling, and network management.; Leverage BI tools (e.g., Power BI) to enable business intelligence and operational insights.; Implement robust data security and privacy measures, ensuring compliance with relevant regulations.; Collaborate with cross-functional teams to gather requirements and deliver high-quality, documented solutions.; Required Skills & Qualifications:; Bachelor’s degree in Computer Science, Information Technology, or a related field. Advanced degrees or certifications are advantageous.; Academic or industrial experience in data engineering, including SQL Databases and cloud platforms (Azure, AWS, or GCP).; Experience or interest in IoT technologies and data systems.; Proficiency in data ingestion tools such as Azure Data Factory and ETL processes.; Strong programming skills in Python, SQL, and one or more of Java or Scala.; Familiarity with big data technologies (e.g., Hadoop, Kafka) and enterprise service buses (ESB).; Knowledge of data management systems and integration with enterprise applications.; Understanding of operational data flows, business cycles, and regulatory requirements.; Preferred Skills & Qualifications:; Familiarity with additional Azure ecosystem tools (e.g., Synapse Analytics, Event Hub, Stream Analytics).; Experience with APIs, data interfaces, and integrating IoT systems with enterprise data platforms.; Relevant certifications in Microsoft Azure or other recognised credentials.; Knowledge of DevOps practices and CI/CD pipelines (e.g., Azure DevOps).; Familiarity with containerisation technologies such as Docker.; Background in implementing Change Data Capture (CDC) designs and scalable data architectures.; Personal Attributes:; Excellent communication and collaboration skills.; Strong analytical and problem-solving mindset.; Proactive approach to continuous professional development.; Ability to work effectively in dynamic, fast-paced environments.; This position offers an opportunity to work at the intersection of IoT data systems and modern cloud engineering, supporting innovation and operational excellence across industries.",,Full time,singapore
85742262,Big Data Engineer - TikTok,TikTok Pte. Ltd.,Central Region,2025-07-14 17:57:21,https://id.jobstreet.com/id/job/85742262,"About TikTok; TikTok is the leading destination for short-form mobile video. At TikTok, our mission is to inspire creativity and bring joy. TikTok's global headquarters are in Los Angeles and Singapore, and we also have offices in New York City, London, Dublin, Paris, Berlin, Dubai, Jakarta, Seoul, and Tokyo.; Why Join Us; Inspiring creativity is at the core of TikTok's mission. Our innovative product is built to help people authentically express themselves, discover and connect – and our global, diverse teams make that possible. Together, we create value for our communities, inspire creativity and bring joy - a mission we work towards every day.; We strive to do great things with great people. We lead with curiosity, humility, and a desire to make impact in a rapidly growing tech company. Every challenge is an opportunity to learn and innovate as one team. We're resilient and embrace challenges as they come. By constantly iterating and fostering an ""Always Day 1"" mindset, we achieve meaningful breakthroughs for ourselves, our company, and our users. When we create and grow together, the possibilities are limitless. Join us.; Diversity & Inclusion; TikTok is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives. Our platform connects people from across the globe and so does our workplace. At TikTok, our mission is to inspire creativity and bring joy. To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach. We are passionate about this and hope you are too.; Job highlights; Positive team atmosphere, Career growth opportunity, Paid leave, 100+ mil users, Meals provided; Responsibilities; Team Introduction; The mission of the Data Platform Singapore Business Partnering (DPSG BP) team is to empower the TikTok Business with data. Our goal is to build a Data Warehouse that can cater to batch and streaming data, Data Products that provide useful information to build efficient data metrics & dashboards which will be used to make smarter business decisions to support business growth. If you're looking for a challenging ground to push your limits, this is the team for you!; Responsibilities:; - Translate business requirements & end to end designs into technical implementations and responsible for building batch and real-time data warehouse.; - Manage data modeling design, writing, and optimizing ETL jobs.; - Collaborate with the business team to building data metrics based on data warehouse.; - Responsible for building and maintaining data products.; - Involvement in rollouts, upgrades, implementation, and release of data system changes as required for streamlining of internal practices.; - Develop and implement techniques and analytics applications to transform raw data into meaningful information using data-oriented programming languages and visualisation software.; - Apply data mining, data modelling, natural language processing, and machine learning to extract and analyse information from large structured and unstructured datasets.; - Visualise, interpret, and report data findings and may create dynamic data reports as well.; Qualifications; Minimum Qualifications:; - At least 5 years in software engineering and 2 years of relevant experience in data engineering.; - Proficient in creating and maintaining complex ETL pipelines end-to-end while maintaining high reliability and security.; - Familiar with data warehouse concept and have production experience in modeling design.; - Familiar with at least 1 distributed computing engine (e.g. Hive, Spark, Flink).; - Familiar with at least 1 NoSQL database is a plus (e.g. HBase).; Preferred Qualifications:; - Excellent interpersonal and communication skills with the ability to engage and manage internal and external stakeholders across all levels of seniority.; - Strong collaboration skills with the ability to build rapport across teams and stakeholders.",,Full time,singapore
85379862,Data Engineer (DST),MOH Office for Healthcare Transformation Pte Ltd,Singapore,2025-07-02 17:57:21,https://id.jobstreet.com/id/job/85379862,"MOHT envisions a transformed health system that is patient-centric, data-driven and digitally-enabled to better empower health, prevent disease and provide excellent value-based care. To realise this vision, MOHT’s mission is to design and implement innovative solutions essential for the desired health system transformation.; Technology plays a significant role in MOHT’s projects and experiments, and in its operating environment. MOHT adopts an agile approach using rapid and continuous build-measure-learn cycles that identify, develop, deliver and adapt technology to improve health. They work closely with IHiS and implementation partners. This allows MOHT’s teams to be responsive to real needs, as demonstrated by actual data, and nimble in evolving prototypes and scalable solutions.; The main tools being used are:; Agile development on cloud based environment; Digital solution architecture design, development and operation; Data analytics and modelling to understand healthcare flows and resource usage; Artificial Intelligence; Current Initiatives:; Digital application for mental wellness; Passive and active sensing tools for patient lifestyle understanding and improvement; Tele-health monitoring solutions for connected care; Mobile patient and clinical applications; Digital phenotyping; Data analytics, including for patient flow analysis; JOB RESPONSIBILITIES; Lead the design, development, and maintenance of scalable and secure data pipelines and architectures to support healthcare analytics and digital health solutions.; Collaborate with cross-functional teams to understand data needs and translate them into robust data engineering solutions.; Manage and optimize data ingestion, transformation, and storage processes across structured and unstructured data sources.; Ensure data quality, integrity, and governance across all data platforms.; Drive the adoption of best practices in data engineering, including CI/CD, testing, and monitoring.; Mentor and guide junior data engineers and contribute to team capability building.; Work closely with data scientists, analysts, and application developers to enable data-driven decision-making.; Document data workflows, architecture, and operational procedures for knowledge sharing and continuity.; JOB REQUIREMENTS; Required:; Bachelor’s degree (or above) in Computer Science, Data Engineering, Information Systems, or a related field.; At least 8 years of experience in data engineering, with a strong background in building and managing data pipelines and architectures.; Must have:; Hands-on experience with cloud-based data platforms (AWS preferred or Azure, or GCP).; Proficiency in data pipeline tools (e.g. Apache Airflow, Kafka, Spark).; Strong SQL skills and experience with relational and NoSQL databases.; Strong Python programming skills; Experience with data warehousing solutions (e.g., Snowflake, Redshift, BigQuery).; Familiarity with data governance, security, and compliance in healthcare or regulated environments.; Plus: Experience with healthcare data standards (e.g., FHIR, HL7) and DevOps practices.; Leadership: Ability to lead data engineering initiatives, coordinate with stakeholders, and manage project timelines and deliverables.; Good communication skills, both written and verbal.",,Full time,singapore
85272022,(Sr.) Data Engineer,BTSE,Singapore,2025-06-30 17:57:21,https://id.jobstreet.com/id/job/85272022,"About BTSE:; DGTL SG is a specialized service provider dedicated to delivering a full spectrum of front-office and back-office support solutions, each of which are tailored to the unique needs of global financial technology firms. DGTL SG is engaged by BTSE Group to offer several key positions, enabling the delivery of cutting-edge technology and tailored solutions that meet the evolving demands of the fintech industry in a competitive global market. ; BTSE Group is a leading global fintech and blockchain company that is committed to building innovative technology and infrastructure. BTSE empowers businesses and corporate clients with the advanced tools they need to excel in a rapidly evolving and competitive market. BTSE has pioneered numerous trading technologies that have been widely adopted across the industry, setting new benchmarks for innovation, performance, and security in fintech. BTSE’s diverse business lines serve both retail (B2C) customers and institutional (B2B) clients, enabling them to launch, operate, and scale fintech businesses. BTSE is seeking ambitious, motivated professionals to join our B2C and B2B teams.; About the opportunity:; The Data Architecture team is responsible for designing and implementing scalable data platforms and systems on AWS to support enterprise data warehousing and pipeline infrastructure. As a Data Engineer on this team, you will design, build, and maintain a robust data platform that enables reliable data integration, processing, and analytics across the organization.; Responsibilities:; Design, build, and maintain scalable data platforms and systems on AWS and Databricks to support analytics, reporting, and data processing workloads.; Work closely with the infrastructure team to build and operate the foundational components such as compute, storage, and networking that support data pipeline execution at scale.; Develop and maintain a reusable data job framework to enable efficient and scalable orchestration of data pipelines using PySpark and Databricks Workflow.; Optimize performance of distributed data processing systems, including Spark tuning and resource configuration, to ensure high efficiency and reliability.; Define and implement monitoring, alerting, and observability for the data platform infrastructure to maintain system health and support proactive issue resolution.; Collaborate cross-functionally with data engineers, analysts, and DevOps teams to deliver governed, high-quality data with strong platform-level reliability.; Requirements:; 3+ years of experience in data engineering or a related field, with hands-on experience in building cloud-based data systems (preferably AWS).; Strong proficiency in PySpark, SQL, and Python for large-scale data processing and performance tuning.; Hands-on experience with Databricks and orchestration tools such as Workflow or Apache Airflow, with a proven track record of designing reusable frameworks to run and manage data workflows.; Familiarity with CI/CD practices and version control systems like GitLab.; Working knowledge of AWS services commonly used in data platforms, such as Amazon Glue, PostgreSQL (RDS or Aurora), and ElastiCache for Redis.",,Full time,singapore
85222923,"Contract Data Engineer (Healthcare) at Buona Vista, up to $6500",Success Human Resource Centre Pte Ltd,One North,2025-06-27 17:57:21,https://id.jobstreet.com/id/job/85222923,"Job Responsibilities:; Support the design, build, and maintenance of data pipelines to ensure seamless integration and flow of data from multiple systems across a variety of frequencies and fidelities; Prepare, clean, and transform data for analytics and reporting purposes; Create and maintain comprehensive documentation for data processes; Identify and follow through on opportunities for improvements and optimisation; Test data system configurations to increase efficiency; Support the handling and logging of errors; Monitor data system performance; Period:; 6 months contract ; Location:; Buona Vista ; Working Hours:; Monday to Friday, 8.30am to 6pm; Salary:; up to $6500; Job Requirements:; Minimum Degree in Computer Science, Information Technology, Computer Engineering or equivalent.; 3 to 7 years’ experience in developing, implementing and maintaining data pipelines, architecture, and data sets, and deploying pipelines and codes in production.; Proficient in Python with experience in scripting.; Strong SQL skills and SQL server databases.; Experience working with specific AWS services such as S3, Athena, Lambda, IAM and CloudWatch; Ability to relate to business and technical stakeholders on aspects of health informatics and Singapore’s Healthcare Systems; Interested applicants, kindly email your detailed resume (MS Word format is preferred):; tracy@successhrc.com.sg (Reg No: R1107390); Please ensure that applications sent through email are no bigger than 1Mb.; We thank all applicants for your interest but regret to inform that only shortlisted candidates would be notified.; Success Resource Centre Pte Ltd (EA License Number: 04C3201); 160 Robinson Road, #13-07/08/09 SBF Center, Singapore 068914; T: 6337 3183 | F: 6337 0329 | W: www.successhrc.com.sg","$4,500 – $6,500 per month (SGD)",Full time,singapore
85584364,"Data Engineer, Healthcare",Menrva,Marina South,2025-07-08 17:57:21,https://id.jobstreet.com/id/job/85584364,"About the Organization; The organization plays a central role in architecting and implementing cutting-edge digital health solutions —a major preventive care strategy aimed at improving long-term population health.; Why This Role Matters; As a Data Engineer, you will be integral to building and maintaining the data infrastructure that fuels analytics, reporting, and predictive modelling. Your work will directly influence how data is accessed, secured, and leveraged to improve healthcare outcomes.; Key Responsibilities; Design, build, and maintain robust data pipelines for integrating and processing data from diverse sources and formats.; Clean, prepare, and transform data for analytics, business intelligence, and data science use cases.; Ensure comprehensive documentation of data processes and pipeline architecture.; Monitor, troubleshoot, and improve the performance of data systems and pipelines.; Identify optimisation opportunities for scalability, repeatability, and security.; Handle data system errors and contribute to testing configurations for improved efficiency.; Requirements; Must-Have Skills:; Proven experience in designing scalable ETL pipelines to support AI and data science initiatives.; Strong proficiency in SQL, NoSQL, and Python for data preparation, transformation, and automation.; Hands-on experience with data lake management and data pipeline development.; Familiarity with cloud collaboration and development tools (e.g., Office 365, Atlassian, AWS, Azure).; Nice-to-Have Skills:; Exposure to AWS services (e.g., S3, Athena, Lambda, IAM, CloudWatch).; Domain knowledge in health informatics; Experience supporting machine learning, clinical data projects, or hospital information systems.; Familiarity with modern data engineering frameworks like Airflow, Docker, Kubernetes.; Qualifications; Bachelor's degree in Computer Science, Information Technology, Computer Engineering, or a related field.; 3 to 7 years of hands-on experience in data engineering, pipeline development, and production deployment.; Strong scripting abilities, preferably in Python.; Solid SQL skills and experience with platforms like Informatica, Teradata, or SQL Server.",,Full time,singapore
85155664,Site Reliability Engineer (OLAP Database) - Data Platform,TikTok Pte. Ltd.,Singapore,2025-06-17 17:57:21,https://id.jobstreet.com/id/job/85155664,"Responsibilities; About TikTok TikTok is the leading destination for short-form mobile video. Our mission is to inspire creativity and bring joy. TikTok has global offices including Los Angeles, New York, London, Paris, Berlin, Dubai, Singapore, Jakarta, Seoul, and Tokyo. Why Join Us Creation is the core of TikTok's purpose. Our platform is built to help imaginations thrive. This is doubly true of the teams that make TikTok possible. Together, we inspire creativity and bring joy; a mission we all believe in and aim towards achieving every day.; To us, every challenge, no matter how difficult, is an opportunity; to learn, to innovate, and to grow as one team. Status quo? Never. Courage? Always. At TikTok, we create together and grow together. That's how we drive impact; for ourselves, our company, and the communities we serve.; Join us. About the team TikTok and affiliate are developing the next-generation high-performance analytical database, with a mission to enable efficient and real-time data-driven decision-making on PB-level data sets. The initial product was forked from Clickhouse, after which large re-architecture had been taken place.; The product now not only improves the efficiency of Clickhouse but also fits into the elastic cloud-native infrastructure with better scalability and resource utilization. With years of polishment in the internal EB-level scenarios, we are now ready to serve our business partners via various cloud vendors. What you will be doing:; Responsible for the SLA system, disaster recovery system, and fault self-healing of ByteDance's OLAP products to ensure continuous availability of business operations.; Responsible for all BU (Business Unit) teams using ByteDance's data warehouse products, continuously improving service quality and user experience, and working directly with product and research teams to promote the resolution of customer issues and ongoing product optimization.; Responsible for the development, automation, and continuous iteration of the SRE platform for ByteDance's big data products, guiding the operation and maintenance mode of the product towards digital and intelligent development.; Research, design, and develop computer and network software or specialised utility programs.; Analyse user needs and develop software solutions, applying principles and techniques of computer science, engineering, and mathematical analysis.; Update software, enhances existing software capabilities, and develops and direct software testing and validation procedures.; Work with computer hardware engineers to integrate hardware and software systems and develop specifications and performance requirements.; Qualifications; Minimum bachelor's degree in Computer Science or a related technical background involving software/system engineering or equivalent working experience; At least 3 year of production-level experience in either Python, Shell, Java or; Go; Familiar with Linux, network, and other system operation and maintenance skills; Familiar with open-source or commercial technologies such as ClickHouse, Hadoop, Doris, and Kubernetes, with experience in practical big data commercial application development.; Proficient in maintaining common SRE/DevOps open-source components, troubleshooting, and problem-solving skills.; Good communication, presentation, and logical thinking skills, as well as good service and collaboration awareness.; Strong sense of responsibility, strong resistance to pressure, and willingness to challenge technological limits.; TikTok is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives. Our platform connects people from across the globe and so does our workplace. At TikTok, our mission is to inspire creativity and bring joy. To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach. We are passionate about this and hope you are too.",,Full time,singapore
85688880,Data Engineer - Data Governance,TikTok Pte. Ltd.,Singapore,2025-07-11 17:57:21,https://id.jobstreet.com/id/job/85688880,"Responsibilities; About the team The success of TikTok's data business model hinges on the supply of a large volume of high quality labeled data that will grow exponentially as our business scales up. However, the current cost of data labeling is excessively high. The Data Solutions Team uses quantitative and qualitative data to guide and uncover insights, turning our findings into real products to power exponential growth. The Data Solutions Team responsibility includes infrastructure construction, recognition capabilities management, global labeling delivery management. About the role As Data Engineer, you will be working on cutting-edge challenges in the big data and Al industry which requires strong passion and capability of innovation. You will collaborate closely with cross-functional teams to understand business requirements and translate them into technical solutions. Responsibility 1. Design, build, and maintain robust, scalable, and efficient data pipelines for ingesting, processing, and transforming large volumes of data to meet both immediate and long-term business needs. 2. Define the technical strategy and roadmap for data engineering projects in alignment with business objectives, actively evaluate and bring in industry best practices and state-of-the-art technical approaches, and timely update the strategy according to the rapid change of the industry; 3. Own and drive data engineering projects by leveraging both internal and cross-functional resources, setting meaningful and challenging targets, and achieving them with innovative approaches; 4. Ensure data governance and quality, establishing processes to manage and protect the integrity of data across systems. 5. Translate business requirements into actionable data solutions, maintaining a strong understanding of business needs and using data to generate impactful insights for decision-making. 6. Support business intelligence efforts by designing and implementing data warehousing solutions and building interactive dashboards for real-time business analysis. Qualifications; Minimum Qualifications 1. Bachelor's or Master's degree in Computer Science, Engineering, or related field; 2. 3+ years of experience (or equivalent) in data engineering, with a strong track record of building and managing large-scale, production-grade data pipelines. 3. Proficiency in Python and SQL for data processing and automation; experience with Java or Scala is a plus. 4. Hands-on experience with big data frameworks such as Apache Spark, Hadoop, Kafka, or similar technologies. Experience in performance tuning and optimization of data pipelines and large-scale distributed systems. 5. Strong knowledge of database and data warehousing concepts, including star schema, dimensional modeling, ETL/ELT frameworks, and schema design. 6. Familiarity with data governance, data quality frameworks, and best practices in data validation, lineage, and metadata management. 7. Experience using data visualization tools such as Tableau, Power Bl, or internal platforms to support business insights. Preferred Qualifications 1. Familiarity with real-time data processing technologies such as Flink and others. 2. Strong communication and interpersonal skills, with the ability to work effectively with both technical and non-technical stakeholders.",,Full time,singapore
84883591,Data Engineer,ExpressVPN,Singapore,2025-06-17 17:57:21,https://id.jobstreet.com/id/job/84883591,"The Data Engineer will be responsible for designing, developing, and maintaining robust data pipelines and data warehouse architectures. The ideal candidate will have expertise in tools like AWS Redshift, Athena, Snowflake, and other leading databases, with a strong focus on building scalable and efficient data solutions.; What you’ll do; Design and implement scalable and efficient data pipelines to ingest, process, and store large datasets from multiple sources.; Develop and maintain data warehouse solutions using AWS Redshift, Athena, Snowflake, and other modern data platforms.; Optimize data models and queries to ensure high performance and cost-effectiveness.; Collaborate with data analysts, scientists, and cross-functional teams to understand data requirements and translate them into technical solutions.; Ensure data integrity, security, and privacy throughout the pipeline and storage processes.; Monitor data workflows and troubleshoot issues to ensure reliability and accuracy.; Implement best practices for data governance and ensure compliance with data policies.; Stay updated with the latest trends and advancements in data warehousing technologies.; What you’ll need to succeed; Bachelor’s or Master’s degree in Computer Science, Information Technology, or related field.; 5+ years of experience as a Data Engineer or similar role with a focus on data warehousing and pipeline development.; Strong proficiency with AWS Redshift, Athena, Snowflake, and other top-tier databases.; Experience with data pipeline tools and frameworks (e.g., Apache Airflow, Glue, or similar).; Proficiency in SQL and scripting languages such as Python or Shell.; Familiarity with cloud services (AWS preferred) and infrastructure management.; Experience with data modeling, schema design, and ETL processes.; Excellent problem-solving skills and attention to detail.; Strong communication skills and the ability to work collaboratively across teams.; Nice to Have:; Experience with BI tools like Tableau, Looker, or Power BI.; Knowledge of data security and privacy best practices.; Experience with DevOps tools and practices.; P.S. Mention Tech in Asia Jobs when you apply! Helps keep the good stuff coming 😉",,Full time,singapore
85246251,Senior/ Data Engineer,BW LPG Holding Pte Ltd,Queenstown,2025-06-29 17:57:21,https://id.jobstreet.com/id/job/85246251,"COMPANY DESCRIPTION; As the world's leading owner and operator of LPG carriers with five decades of operating experience, BW LPG promotes competitive, sustainable solutions to secure value for society and our stakeholders.; Our global footprint spans seven countries with 17 nationalities represented across our workforce. Our fleet of 39 Very Large Gas Carriers (VLGC) offer a total carrying capacity of over 3 million CBM. From spot voyages and time charters to Contracts of Affreightments (CoAs), our emphasis on flexible, reliable service has earned the trust of leading oil companies as well as trading and utility companies. More information about BW LPG can be found at www.bwlpg.com.; BW LPG is associated with BW Group, a leading global maritime company involved in shipping, floating infrastructure, deepwater oil & gas production, and new sustainable technologies. Founded in 1955 by Sir YK Pao, BW controls a fleet of over 490 vessels transporting oil, gas and dry commodities, with its 200 LNG and LPG ships constituting the largest gas fleet in the world. In the renewables space, the group has investments in solar, wind, batteries, biofuels and water treatment. BW LPG, as a member of the BW Group, is one of Forbes World's Best Employers 2023.; DESIGNATION : Senior/ Data Engineer; RESPONSIBILITIES; Role Overview; The Senior Data Engineer has a role focusing on building our modern Lakehouse including owning pipeline development, data modeling and governance within a cross-functional team responsible for the end-to-end delivery of data products and analytics solutions.; Additionally creating CI/CD pipelines for automated deployments and monitoring, and enforce data quality checks, ensuring data consistency, reliability, and compliance.; Key Responsibilities; Working with Product Owners, Business Analysts and Data Analysts to ensure business requirements are translated into data solutions, deliver accurate and reliable datasets that enable impactful analytics and dashboards.; They collaborate with System Analysts and the Business to understand the business logic and data lifecycle within our systems and inform our solution design with a goal of balancing scalability and performance.; QUALIFICATIONS; Skills; Collaboration and Communication Skills: Excels at working in a team to align data solutions with business needs, focused on creative problem-solving in a fast-paced environment.; Analytical Thinking & Communication: Strong skills in analyzing and translating data-driven insights and requirements to both technical and non-technical stakeholders.; Spark & Azure Databricks: Proficiency in designing and implementing data pipelines to support reliable data ingestion and transformation using Databricks and Spark.; Data Modeling Expertise: Advanced SQL and Python skills for complex queries and data modeling, enabling efficient data access and transformation.; CI/CD Pipeline Creation: Experience in creating CI/CD pipelines, enabling automated testing, validation, and deployment of data pipelines models using GitHub and GitHub Actions with Databricks.; Infrastructure as Code: Experience in automating cloud infrastructure management for consistent deployment and scaling using tools like Terraform.; Experiences; 5+ Years in Data Engineering: Proven experience building and maintaining data infrastructure, with a deep understanding of data engineering principles and best practices.; Agile Mindset and Experience: Thrives in Agile Environments, with a strong resilience, adaptability and responsiveness to change in fast-paced settings.; OTHER INFORMATION; Only shortlisted candidate will be notified.",,Full time,singapore
84766430,Data Engineer III/IV,Ollion,Singapore,2025-06-17 17:57:21,https://id.jobstreet.com/id/job/84766430,"OUR STORY; Let’s be honest: there are lots of people out there doing what we do. We’re just not convinced they’re doing it right. Businesses are hungry for innovation and opportunity, but not at the cost of their independence. At Ollion, we’ve connected companies and capabilities around the world to help ambitious organizations make the most of their transformation and leave the status quo in the dust.; WORKING AT OLLION; Innovation is risky. It demands bold steps and big questions, but that’s the price of making change. We’ve got our head in the cloud and two feet on the ground, channeling tech’s endless potential towards a single goal: making a world of difference. And we’re building a global team to do just that— a team capable of making game-changing breakthroughs without ever losing sight of the people it will impact. This is more than consulting. This is the change you can be.; THE OLLION DIFFERENCE; At Ollion, we’re all in on your independence. Our teams are seasoned. Our solutions are straightforward—sometimes even groundbreaking. And our engagements? Exactly as long as you want them to be. We deliver fresh thinking and hard-earned insight in a way that works for you and your customers, arming your organization with everything you need to make your transformation truly mean something.; WORKING WITH OLLION (our clients’ experiences); Progress matters more than process. Our global team of cloud-native pros is all about creating new and better ways to work—not just by solving your tech challenges, but by using technology to solve your business challenges. We keep the formulas, frameworks, and ten-point plans to a minimum, tackling your most pressing problems with a proprietary mix of good-old-fashioned ingenuity and refreshing humanity.; DIVERSITY AT OLLION ; One of our cultural keystones, ‘Find the angle’ recognizes that every individual has different aspirations, needs and brings a unique perspective. ; We value diversity, inclusion, and equity (DE&I) as core to our success. We believe that a diverse workforce brings together unique perspectives, experiences, and ideas, leading to innovation, creativity, and better outcomes for our clients and our organization. We are on a journey and are committed to building a workplace that celebrates and respects individuals from all backgrounds, including but not limited to race, ethnicity, gender, sexual orientation, age, disability, and cultural heritage.  ; As our commitment to diversity and inclusion is reflected in our: ; Awareness and sensitisation programs: to create awareness and sensitisation. We encourage open dialogue, active listening, and mutual respect, creating a safe and supportive environment for everyone to contribute their unique perspectives and ideas. ; Dedicated efforts to building diverse teams: that leverage the strength of our differences to tackle complex challenges and drive innovation. By embracing diversity, we broaden our collective knowledge, enhance problem-solving capabilities, and unlock limitless potential for our employees.",,Full time,singapore
85358937,Database Infrastructure Engineer (Banking/ East/ 12 Months Contract),PERSOLKELLY Singapore Pte Ltd (Formerly Kelly Services Singapore Pte Ltd),East Region,2025-07-02 17:57:21,https://id.jobstreet.com/id/job/85358937,"Responsibilities: ; Alert monitoring for all database flavors (MariaDB, PostgreSQL, EDB, Oracle, DB2, MSSQL, Redis, and Mongo) and take appropriate action. ; Daily health monitoring of production databases ensuring high availability, security, and performance. ; Perform daily backup monitoring and take action to have recovery solution in place. ; Manage database availability and performance, including incident and problem management. ; Incident management in coordination with the MIM (Major Incident Management) team to resolve or coordinate high critical issues. ; Carry out periodical production patch and upgrade management. ; Work with users and vendors to take timely and appropriate steps to resolve and troubleshoot issues impacting service levels. ; Identify opportunities for process improvements, automation, and efficiency gains in database operations. ; Implement best practices and innovative solutions to enhance system reliability and performance. ; Database Expertise Required: ; Required familiarity with multiple databases among MariaDB, PostgreSQL, EDB, Oracle, DB2, MSSQL, Redis, and Mongo. ; MariaDB exposure to replication concepts to resync. ; MariaDB backup and restore & perform point-in-time recovery. ; MariaDB: Ensure performance and security of databases. ; Oracle/DB2: Basic knowledge on backup and archive logging to ensure resiliency. ; Oracle/DB2/MSSQL: Familiarity to monitor and take action for LRQ and LOCKWAITS. ; PostgreSQL/EDB: Get familiarized with LRQ, LRT, LRIIT monitoring and take action accordingly. ; Redis: Basic monitoring of database, shards, and sync status. ; Operations and Tools: ; Perform database patching/upgrades both manually and via Robosys. ; Stay up-to-date with banking industry regulations, standards, and best practices related to database management, security, and compliance. ; Basic knowledge of Linux operating system (RedHat)/AIX/Windows and basic commands. ; Minimum Requirements: ; Minimum 2-8 years of experience and required familiarity with at least 2 databases among MySQL/MariaDB, PostgreSQL/EDB, Oracle, MSSQL, DB2, MongoDB, and Redis. ; Position requires the ability to work various work schedules that support a 24x7 coverage including nights, weekends, and holiday shifts. ; Kindly note that only shortlisted candidates will be contacted.;  By sending us your personal data and curriculum vitae (CV), you are deemed to consent to PERSOLKELLY Singapore Pte Ltd and its affiliates to collect, use and disclose your personal data for the purposes set out in the Privacy Policy available at https://www.persolkelly.com.sg/policies. You acknowledge that you have read, understood, and agree with the Privacy Policy. ; PERSOLKELLY Singapore Pte Ltd • RCB No. 200007268E EA License No. 01C4394 • EA Registration No. R21103542 (Ling Kai Jin)","$5,000 – $7,500 per month (SGD)",Kontrak/Temporer,singapore
84574188,Cloudera / Data Engineer,NCS Hong Kong and Singapore,Ang Mo Kio,2025-06-17 17:57:21,https://id.jobstreet.com/id/job/84574188,"Company Description; NCS is a leading technology services firm that operates across the Asia Pacific region in over 20 cities, providing consulting, digital services, technology solutions, and more. We believe in harnessing the power of technology to achieve extraordinary things, creating lasting value and impact for our communities, partners, and people. Our diverse workforce of 13,000 has delivered large-scale, mission-critical, and multi-platform projects for governments and enterprises in Singapore and the APAC region. ; Job Description; Cloudera / Data Engineer; As a Cloudera / Data Engineer, you will be responsible for designing, building, and maintaining scalable data pipelines and platforms, with a strong focus on the Cloudera Hadoop ecosystem. You will work closely with data analysts, scientists, and business stakeholders to ensure data accessibility, quality, and security.; What will you do?; Design, build, and manage the Cloudera Hadoop Distribution (CDH/CDP).; Develop and maintain ETL pipelines using tools such as Apache NiFi, Hive, Spark, and Impala.; Manage and optimize HDFS, YARN, Kafka, HBase, and Oozie workflows.; Monitor and troubleshoot cluster performance and jobs with strong problem-solving and debugging skills.; Collaborate with DevOps and Data Science teams to integrate data platforms into applications and analytics workflows.; Ensure data governance, security, and compliance using tools like Apache Ranger, Atlas, and Kerberos.; Mentor and guide a team of data engineers to deliver robust data solutions.; Qualifications; The ideal candidate should possess:; 10+ years of experience in big data engineering, preferably with Cloudera.; Strong programming skills in Python, Java, and Spark.; Experience with Apache Spark, Hive, Impala, and Kafka.; Familiarity with Linux/Unix and shell scripting.; Degree in Computer Science, Information Technology, or a related field.; Preferred Skills:; Cloudera Certified Professional (CCP) or Cloudera Data Platform certification.; Experience/Knowledge on cloud platforms (AWS, Azure, or GCP) and hybrid deployments.; Familiarity with CI/CD pipelines, Docker, or Kubernetes in a data context.; Additional Information; We are driven by our AEIOU beliefs - Adventure, Excellence, Integrity, Ownership, and Unity - and we seek individuals who embody these values in both their professional and personal lives. We are committed to our Impact: Valuing our clients, Growing our people, and Creating our future.  ; Together, we make the extraordinary happen.  ; Learn more about us at ncs.co and visit our LinkedIn career site.",,Full time,singapore
85652652,Data Engineer,Safran Helicopter Engines,North Region,2025-07-10 17:57:21,https://id.jobstreet.com/id/job/85652652,"The Data Engineer's mission is to collect data, aggregate it into analytical or BI solutions, and make it available to the systems and/or organizations that require it. The engineer also carries out developments to collect, cleanse, and structure data within our platform from internal and external information systems.; Roles and Responsibilities:; - Build and develop data models.; - Build APIs to exchange data.; - Propose and implement data retrieval tools, processing, and solutions.; - Implement and/or industrialize data entry and retrieval algorithms.; - Validate and monitor platform developments.; - Perform maintenance and upkeep of deployed solutions.; - IT Functional Maintenance (incidents and developments),; - POCs, POVs, and MVPs (implemented in QCD),; - Preliminary Projects (solution selection, definition of the provisional schedule, provisional budget, deployment, and risk identification),; - Projects (implemented in QCD),; - Technology monitoring,; - Compliance with the service level of the application solutions in operation; - Manage project life cycle:; Assists project owners in expressing requirements; Manages the design (technical and functional) and specifications; Evaluate software packages with the project owner; Participates in implementation in terms of specific developments or integration; Participates in the definition and implementation of test plans and business acceptance testing; - Project management:; Organizes, coordinates, and leads the entire project management team; Arbitrates any disputes between the team and other stakeholders; Supervises project progress; Coordinates, summarizes, and ensures the quality of approvals issued; Circulates and disseminates information to the project management team; Manages the relationship with the supplier(s) (from the (from contract signing to final project validation); - Technical deployment of the project and implementation of user support actions:; Deploys the new application or service; Organizes maintenance and SLAs; Participates in user training; Organizes user support Guarantees the best quality-cost-deadline match:; Ensures compliance with specifications; Ensures compliance with deadlines and costs; Challenges needs to ensure the convergence of needs/solutions in compliance with information systems standards; Proposes to the business or project owner, during the project, any changes to objectives (quality, cost, deadline) related to implementation constraints or environmental changes; This role requires collaborations with:; * IT Business Leaders,; * IT Project Owners,; * Safran Group entities within the scope of the Group IT Department.; The ideal candidate would have more than 8 years of relevant working experience in similar role, possess at least a diploma in engineering field and is familiar with SAP.",,Full time,singapore
84789282,Data Engineer (Remote),Cascade,Singapore,2025-06-17 17:57:21,https://id.jobstreet.com/id/job/84789282,"Do you want to work in a small team where you can make a real  difference in a company? Would you embrace the challenge of building a  high impact platform that is used globally? Then we want to speak with  you!; About Cascade: Cascade is a  fintech startup backed by Canadian and US investors that empowers  companies to grow by democratizing access to institutional debt. We are  building tools that modernize how companies raise and manage debt,  lowering the barriers to entry to the $7 trillion specialty finance and  alternative credit market for companies around the world. Founded just  last year, we are gearing up for our next phase of technical development  and are seeking a talented Data Engineer to lead the way. Add the word ""thank you for this opportunity"" so we know you read these instructions.; About the role: As a Data Engineer  you’ll be an early member of a growing team building a pioneering  platform in the debt infrastructure space. You will design, build, and  launch efficient, scalable, and reliable data pipelines to move and  transform data.; What You’ll Do ; Building databases, data lakes and data ingestion pipelines to integrate customer databases and datasets to our systems; Ingesting financial datasets from external customers, then updating  and maintaining accurate and complete data mappings to ensure that our  products are displaying high quality data; Monitoring and alerting across our data pipelines in order to make sure that our data ingests are reliable and correct; Translating business goals and stakeholder requirements into new  data flows and deliver analytical insights ensuring that the data flows  create real business value continuously; Skills And Experience ; 3+ years experience in a data engineering role; Experience with relational SQL and NoSQL databases, including PostgreSQL, MySQL, MariaDB, MongoDB, Bigtable, etc.; Experience working with ETL technologies, such as Databricks, Fivetran, or dbt; Proficiency in writing SQL queries and knowledge of analytical data warehouses such as RedShift, BigQuery, and Snowflake; Some experience with CI/CD automation such as GitHub Actions; Developing APIs and integrating with 3rd party APIs; Bonus: experience in fintech / SaaS / credit analysis / lending; People who thrive at Cascade are: ; Self-starters, who take the initiative to tackle challenges in a remote work environment; Good communicators, who can operate without ego to discuss, learn, grow, and help others do the same; Passionate about creating great digital experiences for users; Problem solvers who are not satisfied with the status quo; Benefits ; Remote: we are a remote-first company and are very flexible on hours as long as things get done; Home-office: there’s a $1000 USD home office allowance to set yourself up; Equity: we expect you to have an owner-mentality, and have the equity plan to match; Benefits: health, dental, vision, and more; Perks: we offer free lunches weekly and off-site trips; Job satisfaction: we offer autonomy, ample opportunities for  mastery, and an opportunity to make a difference for companies around  the world",,Full time,singapore
85154949,Big Data Development Engineer (Tea) - Data Platform,TikTok Pte. Ltd.,Singapore,2025-06-17 17:57:21,https://id.jobstreet.com/id/job/85154949,"Responsibilities; About the team: Tea(Toutiao Event Analyze) is an in-house analytics platform that is heavily used by more than 10,000 employees globally. This is a data middle platform product built on user behavior data, providing data analysis services for various domestic and international businesses within ByteDance/TikTok, including Douyin, Toutiao, Xigua, TikTok, and more. It possesses robust capabilities to support exabyte-level massive data, trillion-level event volume, and millisecond-level response time, offering users simple, flexible, and high-performance data analysis services.; We are the global engineering team for Tea. We are passionate about building the best data analytics platform in the world and are looking for top-notch software engineers to join the talented team. We deliver Tea products to over ten regions worldwide, optimizing product performance based on the unique data and cultural characteristics of each region.; Responsibilities:; Responsible for the deployment, configuration, monitoring, and maintenance of big data platforms.; Implement and optimize the infrastructure for big data storage, processing, and computation.; Responsible for architectural design, performance tuning, and troubleshooting of big data platforms.; Analyze and resolve complex system performance and stability issues to ensure system reliability and stability.; Write technical documentation to record system configurations and operational procedures.; Update software, enhances existing software capabilities and develops and direct software testing and validation procedures.; Qualifications; Minimum Qualifications:; Bachelor's Degree in Computer Science or related discipline with experience in software engineering, with 2 years of relevant experience.; 2 years experience in big data platform operations, familiarity with technologies such as Hadoop, Spark, Clickhouse, etc.; Solid programming skills, proficient in at least one programming language (e.g., Java, Scala, Python).; Strong foundation in computer science, familiarity with operating system principles, data structures, and algorithms.; Compliance Requirement: Familiarity with and adherence to international data protection regulations; ability to formulate and execute compliance-oriented strategies.; Global Multi-Environment Deployment and Operations: Experience in deploying and maintaining big data platforms across multiple global locations; familiarity with cross-geographical operational challenges.; Preferred Qualifications:; Excellent teamwork and communication skills, experience in project management.; Strong problem analysis and resolution skills, ability to respond quickly to emergencies.",,Full time,singapore
85845477,Data Engineer,TOPPAN Ecquaria Pte Ltd,Braddell,2025-07-17 03:57:21,https://id.jobstreet.com/id/job/85845477,"Responsibilities:; Design and implement robust, scalable data pipelines and architectures to support data ingestion, processing, and storage. Including performance optimizations for data modeling and ingestion; Develop and optimize complex SQL queries and stored procedures for data extraction, transformation, and analysis.; Model data to meet different use casesng applications and automate data workflows.; Collaborate with data scientists and analysts to understand data requirements and deliver high-quality data solutions.; Lead the integration of data from various sources into data lakes and warehouses, ensuring data quality and consistency.; Monitor and troubleshoot data pipelines and workflows to ensure optimal performance and reliability.; Communicate with and support data users; Document data processes, data models, and architectural designs to ensure knowledge sharing and compliance with best practices.; Prerequisites:; Experience: Minimum 3 years in data engineering fields with system integration, and at least 1 year in system integration and implementation in cloud/web-based environments.; Proven Solutions: Demonstrated experience in providing effective, working solutions and implementations, particularly in cloud-based environments.; Technical Skills:; Solid understanding of ETL processes, data warehousing concepts, and data modeling best practices.; Proficiency in Databricks, Azure Data lake, PowerBI, Tableau and related data processing and visualisation software.; Familiarity with Windows, Linux, AWS and/or Azure platforms.; Strong programming skills in languages such as Python and R is a must; Proficiency in other programming languages such as Java, Scala and C# will be advantages; Experience in data processing frameworks (e.g., Apache Spark, Apache Flink); Preferred Exposure:; Experience with large-data management system with visualisation tools.; Experience with Data Integration and ETL Pipelines, Data Warehousing and BI reporting projects.; Experience with Singapore Government Project will be advantages; Personal Attributes:; Excellent problem-solving skills; Ability to work independently; Collaborative in a fast-paced environment; Why Join Us?; Be part of a forward-thinking team that is transforming government digital services. If you are passionate about technology and innovation, and thrive in a dynamic environment, we want to hear from you!; If you are passionate about building partnerships and driving growth, we would love to hear from you!; TOPPAN Ecquaria is an equal opportunity employer and values diversity within our company. We welcome all interested candidates to apply for this position, however, we regret to inform that only shortlisted candidates will be contacted by us for an interview.; Find us at www.topppanecquaria.com or www.linkedin.com/company/toppan-ecquaria; For more career opportunities, please visit our career site at:- https://toppanecquaria.com/careers/job-openings?utm_source=Jobstreet&utm_medium=Page&utm_campaign=2020 (Please copy & paste the above link onto your browser)",,Full time,singapore
85815135,Cloud Engineer,Virtusa Singapore Private Limited,Downtown Tanjong Pagar,2025-07-16 17:57:21,https://id.jobstreet.com/id/job/85815135,"Roles & responsibilities:; Work as the go to person inside Data & Analytics organization in HSBC Wealth and Personal Banking for any banking data request from Data Scientist team and senior business stakeholders.; Work as DataOps transformation to enable self-serve, high performing data ETL.; Hands-on development for data ETL pipeline, data quality check and be responsible for peer coding review.; Participate in regular stands-up to share and speak up the progress and key blocker to ensure project is going smooth; Skills & Qualifications:; 2-5 years of data related working experience in Financial Institute with special preference in banking.; High proficiency in SQL, Python, PostgreSQL and Cloud native data pipeline automation tooling, e.g. BigQuery; Knowledge of SAS coding will be an advantages; Proven working experience in delivering large scale data projects/data products.; Project experience in designing & developing high-performance solution for handling extremely large volume data processing, storage and wrangling in real-time manner.; Good understanding of DevOps, DataOps or MLOps.; Experience in RPA (Robotic Process Automation) is an advantages; Experience in Business visualization tool Looker, including LookML; Cloud-native data ETL development in GCP is preferred; Technology stack provisioned in the team include:; DataProc plus Python on Google Cloud; Airflow DAG for scheduling; Multiple databases including PostgreSQL, BigQuery, graph databases, etc.",,Full time,singapore
85823545,Data Engineer,Innowave Tech Pte Ltd,Paya Lebar East,2025-07-17 01:57:21,https://id.jobstreet.com/id/job/85823545,"About Innowave Tech Singapore ; Innowave Tech is an Artificial Intelligence (AI) company offering solutions for the Semiconductor and Advanced Manufacturing industry. Utilizing deep industrial domain knowledge, proven experience, and innovation, we provide expert AI solutions and systems to address various industry pain points. ;  Roles & Responsibilities ; We are seeking Data Engineer to establish and lead our data infrastructure. The successful candidate will be responsible for building our data engineering practice from the ground up, implementing robust data systems for industrial AI applications, and establishing best practices that will power our semiconductor manufacturing AI solutions. ;  Your Role and Impact ; As our first Data Engineer, you will have a foundational role in building robust data infrastructure to handle manufacturing data and LLM applications, while establishing secure data practices that power our AI solutions for advanced manufacturing operations. ;  What You’ll Do ; Select and manage on-premises technologies suitable for secure and efficient operations. ; Build robust pipelines to collect, clean, and transform diverse datasets including process data, sensor data, image data, and human annotations. ; Ensure secure, maintainable, and scalable deployment of data infrastructure. ; Define and enforce best practices in data governance, privacy, and access control. ; Collaboration & Deployment. ;  What We’re Looking For ; Educational Background: ; Minimum Poly or Bachelor Degree in Computer Science, Engineering, or a related field. ; * We welcome applications from Singapore Citizens, Permanent Residents (PRs), Malaysians, and local graduates bonded for local employment, in accordance with MoM regulations.;  Technical Expertise: ; 3+ years of experience in data engineering roles, ideally with on-premises or hybrid infrastructure. ; Proven track record of building scalable data systems from ground up in a startup environment. ; Proficiency in Python and/or Java for data pipeline development. ; Solid experience with ETL frameworks (e.g., Apache Airflow, Dagster) and streaming systems (e.g., Kafka). ; Experience designing and maintaining SQL and NoSQL databases. ; Experience building and operating data lakes and data catalog. ; Familiarity with containerization (Docker), version control (Git), and CI/CD practices. ;  Soft Skills: ; Excellent communication skills and ability to collaborate with cross-functional technical and non-technical teams. ; Excellent problem-solving and debugging abilities. ; Ability to balance engineering tradeoffs. ;  Bonus Skills: ; Experience with manufacturing data systems, especially SPC, SCADA, and industrial sensor protocols (e.g., OPC UA, MQTT, Modbus). ; Familiarity with AI/ML pipelines and tools (e.g., MLflow). ; Knowledge in vector databases and LLM data infrastructure. ; Prior experience working in or with regulated industries (e.g., semiconductor, automotive, aerospace). ; What we Offer ; • A leading role in cutting-edge AI projects within the semiconductor industry. ; • The opportunity to work with an learn from experts in the field of AI and data science. ; • A dynamic, innovative, and supportive work environment. ; • Competitive salary and benefits package. ; • Career growth opportunities in a fast-paces technology company.","$5,333 – $8,000 per month (SGD)",Full time,singapore
85816706,Associate Data Engineer - ETL (Engineering & Ops),Synapxe,Singapore,2025-07-16 17:57:21,https://id.jobstreet.com/id/job/85816706,"Bachelor's degree in Computer Science, Information Technology, or a related field; At least 4 years of experience in the IT industry, including:; a. Development, implementation, and maintenance of IT systems, preferably in Data Warehousing, ETL rules, data  modeling, and BI applications; b. Operations support and business analysis experience.; c. Strong MS-SQL and Oracle Database scriptin; Experience in diagnosing, troubleshooting, and performing root cause analysis; Ability to diagnose and troubleshoot problems with BI reports and ETL processes; Experience with AWS, Data Lake, Databricks, and the healthcare domain is a plus; Able to work independently and as an effective team player with a strong desire to deliver results; Adaptable, meticulous, and possess strong analytical skills; Good communication skills (both written and spoken); Strong team player",,Full time,singapore
85813512,Data Engineer,Singapore Telecommunications Limited,Singapore,2025-07-16 17:57:21,https://id.jobstreet.com/id/job/85813512,"An empowering career at Singtel begins with a Hello. Our purpose, to Empower Every Generation, connects people to the possibilities they need to excel. Every ""hello"" at Singtel opens doors to new initiatives, growth, and BIG possibilities that takes your career to new heights. So, when you say hello to us, you are really empowered to say…“Hello BIG Possibilities”.; Be a Part of Something BIG!  ; Responsible for building and supporting data ingestion and transformation pipelines in a modern hybrid cloud platform; Independently develop basic batch and streaming pipelines, working with cloud tools such as Databricks and Kafka under the guidance of senior engineers; Contribute to the delivery of reliable, secure, and high-quality data for analytics, reporting, and machine learning use cases; Gain exposure to enterprise-scale data architecture, while growing into more advanced engineering responsibilities over time.; Make An Impact By; Build and maintain data ingestion pipelines for batch and streaming data sources using tools like Databricks and Kafka; Perform data transformation and cleansing using PySpark or SQL based on business and technical requirements; Monitor and troubleshoot data workflows to ensure data quality and pipeline reliability; Work closely with senior data engineers to understand platform architecture and apply best practices in pipeline design; Assist in integrating data from diverse source systems (files, APIs, databases, streaming); Help maintain metadata and pipeline documentation for transparency and traceability; Participate in integrating pipelines with tools such as Microsoft Fabric, Databricks, Delta Lake, and other platform components; Contribute to automation efforts using version control and CI/CD workflows; Apply basic data governance and access control policies during implementatio; Skills to Succeed; Bachelor’s degree in Computer Science, Engineering, or a related field; 1–3 years of experience in data engineering or data platform development; Proven ability to independently build basic batch or streaming data pipelines; Hands-on experience with Python and SQL for data transformation and validation; Familiarity with Apache Spark (especially PySpark) and large-scale data processing concepts; Self-starter with strong problem-solving skills and a keen attention to detail; Able to work independently while collaborating effectively with senior engineers and other stakeholders; Strong documentation and communication skills.; Rewards that Go Beyond; Full suite of health and wellness benefits  ; Ongoing training and development programs  ; Internal mobility opportunities; Your Career Growth Starts Here. Apply Now!; We are committed to a safe and healthy environment for our employees & customers and will require all prospective employees to be fully vaccinated.",,Full time,singapore
85808751,Big Data Engineer,Unison Consulting Pte. Ltd.,Singapore,2025-07-16 17:57:21,https://id.jobstreet.com/id/job/85808751,"We are seeking a highly skilled and experienced Big Data Engineer to join our team. The ideal candidate will have a minimum of 5 years of experience managing data engineering jobs in big data environment e.g., Cloudera Data Platform. The successful candidate will be responsible for designing, developing, and maintaining the data ingestion and processing jobs. Candidate will also be integrating data sets to provide seamless data access to users.; SKILLS SET AND TRACK RECORD; * Good understanding and completion of projects using waterfall/Agile methodology.; * Analytical, conceptualisation and problem-solving skills.; * Good understanding of analytics and data warehouse implementations; * Hands-on experience in big data engineering jobs using Python, Pyspark, Linux, and ETL tools like Informatica; * Strong SQL and data analysis skills. Hands-on experience in data virtualisation tools like Denodo will be an added advantage; * Hands-on experience in a reporting or visualization tool like SAP BO and Tableau is preferred; * Track record in implementing systems using Cloudera Data Platform will be an added advantage.; * Motivated and self-driven, with ability to learn new concepts and tools in a short period of time; * Passion for automation, standardization, and best practices; * Good presentation skills are preferred; The developer is responsible to:; * Analyse the Client data needs and document the requirements.; * Refine data collection/consumption by migrating data collection to more efficient channels; * Plan, design and implement data engineering jobs and reporting solutions to meet the analytical needs.; * Develop test plan and scripts for system testing, support user acceptance testing.; * Work with the Client technical teams to ensure smooth deployment and adoption of new solution.; * Ensure the smooth operations and service level of IT solutions.; * Support production issues",,Full time,singapore
85167337,Contract Data Engineer (2-year contract),Public Service Division,Singapore,2025-06-17 17:57:21,https://id.jobstreet.com/id/job/85167337,"[What the role is]; [What the role is]; The role of the Data Engineer is to collaborate with the existing team of data scientists, data engineers and analysts to create data tools, develop data ingestion and processing pipelines, ensuring optimized data processing, and ensuring that data systems meet STB's business requirements. The role requires working closely with the data science team to set up and deploying data pipelines to support machine learning models and analytics scripts, developing data integrations, assembling complex datasets and implementing process improvements. The Data Engineer plays a key role in enhancing data reliability and quality while ensuring scalable business processes and supporting the team's data-related initiatives.; [What you will be working on]; [What you will be working on]; 1. Project Management; a) Project manage and work closely with vendors and internal stakeholders to deliver on data engineering related implementations ensuring that deliverables and objectives are met within agreed scope and timelines.; b) Collaborate with cross-functional teams, including data scientists, data engineers, DevOps engineers, product managers, business analysts and business stakeholders, to integrate and deploy models into current analytics platforms and production systems.; c) Plan, execute and monitor project milestones and ensure timely update to management on project progress and issues.;   2. Application of Engineering Disciplines in Support of Strategic Business Objectives; a) Prepare, process, cleanse and verify the integrity of data collected for analysis.; b) Design, develop and implement self-managed data processing and compilation pipelines related to key enterprise data domains so that data compilation business logic can be managed and maintained in-house to retain agility in responding to changing operational needs.; c) To review the design and implementation of data pipelines developed by the vendor to ensure that they meet the operational requirements of STB’s business and are integrated back to the self-managed data compilation pipelines for a seamless data processing and compilation process.; d) Work closely with vendors and internal stakeholders to project manage and coordinate Data Science & Analytics's (DS&A) data ingestion and data processing pipelines across platforms which can include mobile apps, SaaS platforms, on-premise and partner systems; e) Help architect DS&A’s data integrations and data processing flows between external / 3rd party data sources, AWS Cloud datawarehouses (e.g. Redshift, RDS) and internal on-premise systems for workloads at scale; f) Provide guidance to internal teams on best practices for Cloud data integrations; g) Identify, design and implement internal process improvements: automating manual processes, optimising data delivery, re-designing infrastructure for greater scalability, etc.; h) Develop monitoring toolkits to ensure that integration is executed successfully and alerts where integrations have failed; i) Implement best practice DataOps processes to ensure continuous integration, deployment and governance of our data pipelines across the entire data lifecycle from data preparation to reporting.;   3. Data Integration and Data Management; a) Collaborate with current team to review the existing data integration processes and make improvements to the current data processing pipelines.; b) Work with data and agency partners to assemble large, complex datasets that meet functional and non-functional business requirements.; c) Provide inputs to the design and development of an integrated data model to allow analysis across multiple structured and unstructured datasets.; d) Recommend different ways to constantly improve data reliability and quality, including helping review and enhance the existing data collection procedures to include data for building analytics models relevant for industry transformation; e) Analyse and assess the effectiveness and accuracy of data sources (e.g., datasets received from stakeholders) and ensure that they meet STB's Data Quality standards.; [What we are looking for]; Strong project management, planning, time management and organisational skills.; Experience supporting and working with cross-functional teams in a dynamic environment.; Experienced data pipeline builder and data wrangler who enjoys optimising data systems and building them from ground up.; Experience in using Qlik Sense and AWS services (e.g., SageMaker, Athena, RDS, ECR, ECS, EMR, Lambda, Redis) will be advantageous.; The following certifications would be advantageous:; Certified AWS Cloud Architect / Data Engineer / DevOps Engineer; Certified Qlik Sense Data Architect; Degree from a recognised university in a quantitative or engineering discipline: Computer Science, Computer Engineering, Informatics / Information Systems, Applied Mathematics or Statistics.; At least 5 years of work experience in a related field with demonstrable skills in developing, deploying and maintaining data workflows.; Proven track record in managing internal and external stakeholders and delivering on objectives according to project timelines and successfully deploying at least 1 medium to large scale analytics system.; Good command of written and spoken English with good presentation and communication skills with ability to express complex ideas, data / concepts and outcomes of analysis clearly to business audiences.; Strong analytical skills with a good eye for detail and possess an aptitude/experience in solving engineering problems to produce quality deliverables.; Ability to integrate and synthesise research and data across multiple sources to derive meaningful conclusions.; Experience working with structured and unstructured datasets is essential.; Proficient in statistical programming tools (e.g., R, Python), and database scripting languages (e.g., SQL - DQL, DML, DDL); Experience with DataOps and deploying models and data workflows through DevOps process will be advantageous; [What we are looking for]",,Full time,singapore
85817927,Data Engineer,TechnoPals Pte Ltd,West Region,2025-07-16 17:57:21,https://id.jobstreet.com/id/job/85817927,"Key Responsibilities:; Design, build, and optimize scalable data pipelines for data extraction, transformation, and loading (ETL/ELT).; Develop and maintain data architectures, databases, and data warehouses.; Collaborate with data scientists and analysts to understand data needs and deliver clean, structured datasets.; Monitor and improve the performance of data systems.; Implement and enforce data quality, security, and governance practices.; Work with cloud platforms (e.g., AWS, Azure, GCP) for data storage and processing.; Automate data processes and workflows using orchestration tools like Airflow or similar.; Maintain documentation of data models, schemas, and systems.; Required Skills and Qualifications:; Bachelor's degree in Computer Science, Engineering, Information Technology, or related field.; Proven experience as a Data Engineer or in a similar role.; Proficient in programming languages such as Python, Scala, or Java.; Strong SQL skills and experience with relational and NoSQL databases (e.g., PostgreSQL, MySQL, MongoDB).; Hands-on experience with ETL tools and data pipeline frameworks (e.g., Apache Spark, Kafka, Airflow).; Experience with cloud data platforms like AWS (Redshift, Glue, S3), Azure (Data Factory, Synapse), or GCP (BigQuery, Dataflow).; Familiarity with data modeling, data warehousing concepts, and data lakes.; Strong problem-solving skills and attention to detail.; Preferred Qualifications:; Experience with big data technologies like Hadoop, Hive, or Presto.; Knowledge of CI/CD and DevOps practices in data engineering.; Experience with version control tools (e.g., Git).; Understanding of data privacy and compliance standards (e.g., GDPR, HIPAA).",,Kontrak/Temporer,singapore
85818027,Staff Data Engineer,Network Guard,North Region,2025-07-16 17:57:21,https://id.jobstreet.com/id/job/85818027,"We are seeking a highly skilled and hands-on Staff Data Engineer to architect and maintain modern data infrastructure and pipelines. This is an individual contributor role focused on building scalable data platforms that power analytics, insights, and data-driven decisions across the company. You will work closely with analysts, data scientists, product, and engineering teams to design end-to-end data solutions using tools like AWS Redshift, Athena, Snowflake, and other leading cloud platforms.; ; ; What You’ll Do; Design and build scalable data pipelines to ingest, process, and store large volumes of structured and unstructured data from diverse sources.; Develop and maintain robust data warehouse architectures leveraging tools such as AWS Redshift, Athena, and Snowflake.; Optimize data models, queries, and storage strategies for performance, scalability, and cost-effectiveness.; Collaborate with cross-functional stakeholders (analytics, product, ML) to gather requirements and deliver data solutions that support business goals.; Ensure data quality, security, and privacy through best practices in governance, testing, and monitoring.; Own and operate production data workflows, resolving incidents and ensuring reliability.; Stay up-to-date with the latest trends and advancements in data engineering and analytics.; ; ; Tech Stack; Languages & Tools: Python, SQL; Data Warehousing & Query Engines: Snowflake, AWS Redshift, Athena; Pipeline & Orchestration: Apache Airflow, AWS Glue, dbt; Cloud & DevOps: AWS (Lambda, S3, IAM), Docker, Terraform; Streaming : Kafka, Kinesis; BI & Analytics: Tableau, Power BI; ; ; What You’ll Need To Succeed; Bachelor's or Master’s degree in Computer Science, Engineering, or related field.; 5+ years of experience as a Data Engineer, with a strong focus on data pipeline development and data warehousing.; Deep proficiency in AWS Redshift, Athena, Snowflake, and experience working with large-scale data systems.; Strong programming and scripting skills in Python, SQL, and Shell.; Experience with pipeline orchestration tools (e.g., Airflow, Glue, dbt).; Solid understanding of data modeling, schema design, and ETL processes.; Familiarity with cloud infrastructure and DevOps practices (especially AWS).; Experience with BI platforms like Tableau or Power BI.; Excellent analytical and problem-solving skills.; Strong communication and the ability to work across functions and mentor others.; ; ; Nice to Have; Knowledge of data privacy and security best practices (e.g., GDPR, SOC2).; Exposure to MLOps and ML data pipelines.; Experience in high-growth, product-led or SaaS environments.",,Full time,singapore
85817337,Data Engineer,SMRT Trains Ltd,Katong,2025-07-16 17:57:21,https://id.jobstreet.com/id/job/85817337,"Job Purpose; The Data Engineer will be part of the team to develop operation & maintenance decision-support tools to enhance train reliability and maintenance efficiency. This position involves designing, developing, and maintaining data pipelines, APIs, and cloud infrastructure for various rail-oriented applications. The ideal candidate will have expertise in data analysis, transformation, ingestion, database design, API development, and preferably, cloud infrastructure setup. Collaborating closely with software engineers, data scientists, and frontend developers, the Data Engineer will contribute to building efficient, scalable, and reliable systems.; Responsibilities; The duties and responsibilities for Data Engineer, are as listed below. The list is not comprehensive and related duties and responsibilities may be assigned from time to time.; Data Engineering & Processing:; Develop and maintain data pipelines for efficient data ingestion and transformation.; Work with structured and unstructured data to ensure optimal storage and retrieval.; Perform data analysis and report on results.; Database Design & Management:; Design and implement relational and NoSQL database schemas for scalability.; Optimize database performance through indexing, partitioning, and query tuning.; Implement data security and compliance best practices.; API Development & Backend Engineering:; Design and develop APIs for data access and application integration.; Implement authentication, authorization, and API security best practices.; Cloud Infrastructure & Deployment (Supporting Role):; Assist in design Azure cloud architectures; Work with IT infrastructure team to set up cloud infrastructure for application hosting, data storage and processing.; Collaboration & Best Practices:; Collaborate with internal stakeholders to understand their business needs.; Work with software engineers, data scientist, frontend developer to understand the data requirement and design architecture of the data platform.; Implement CI/CD pipelines for automated testing, deployment and monitoring.; Write testable and maintainable code and documentation to deploy to production.; Engage continuously with end-user for feedback and improvements.; Qualifications & Work Experience; Degree in Science, Technology, Engineering or Mathematics (STEM); Previous experience as a data engineer or in a similar role; Data engineering certification is a plus; Knowledge of security best practices in cloud and database management is a plus; Skills; Technical skills include:; Programming and Data processing: MATLAB, Python, SQL, or similar languages.; Databases: My SQL, SQL Server, MongoDB or similar.; Cloud Platforms: Azure; DevOps & CI/CD: Git Lab CI/CD, Docker; Generic skills include:; Strong inclination and eager for continual learning and development; Strong team player; Critical thinking and problem-solving skills; Ability to understand and explain complex data and effective interactions with the stakeholders; Ability to think independently and actively propose solutions to the team.",,Full time,singapore
85813539,Data Engineer - TEKsystems (Allegis Group Singapore Pte Ltd),Allegis Group Singapore Pte Ltd,Singapore,2025-07-16 17:57:21,https://id.jobstreet.com/id/job/85813539,"Design and implement optimal data structures and algorithms; Develop, maintain, and enhance data pipelines; Integrate applications with relational databases (e.g., Snowflake, Oracle, MS-SQL); We are seeking an experienced Data Engineer with at least 5 years of professional experience in software engineering or platform engineering.; The ideal candidate will possess strong expertise in designing efficient and scalable applications, working with relational databases, and leveraging modern cloud and big data technologies.; This role requires a combination of technical proficiency, attention to detail, and excellent communication skills to collaborate with data analysts, business users, and vendors in delivering robust data solutions.; Responsibilities:; Design and implement optimal data structures and algorithms to create efficient and scalable applications using Python.; Develop, maintain, and enhance data pipelines, ensuring high levels of data quality and reliability.; Integrate applications with relational databases (e.g., Snowflake, Oracle, MS-SQL) to support data processing and analytics.; Collaborate with stakeholders, including data analysts, business users, and vendors, to design and develop solutions that meet business requirements.; Employ best practices for code versioning, testing, Continuous Integration/Continuous Deployment (CI/CD), and code documentation.; •Apply knowledge of data quality tools for profiling, cleansing, and monitoring data pipelines.; The Candidate:; A degree in Computer Science, Information Technology, or a related field.; 5+ years of experience as a software engineer.; Solid experience in Python, relational databases and SQL and Object-Oriented Programming (OOP) principles.; Strong analytical skills with a passion for solving complex problems through innovative solutions.; Excellent interpersonal and communication skills to interact effectively with diverse stakeholders.; A detail-oriented approach with a focus on operational excellence.; Preferred Qualifications; •Experience with Snowflake, Oracle, and MS-SQL; •Familiarity with cloud services such as AWS Glue, EKS, and S3; knowledge of Presto, Trino, AWS Athena, or similar tools.; •Experience of Financial Services - if no experience, at least an interest in the products.; ; We regret to inform that only shortlisted candidates will be notifieid.; EA Registration No: R1984122, HILDER SEAN KIPLING, R1110424 Allegis Group Singapore Pte Ltd, Company Reg No. 200909448N, EA License No. 10C4544; information_technology",,Full time,singapore
80701591,Data Engineer (Data Operations) (In Partnership with IMDA),Phillip Securities Pte Ltd,Singapore,2025-06-17 17:57:21,https://id.jobstreet.com/id/job/80701591,"Responsibilities:; Define the overall data architecture and governance framework.; Ensure consistency and alignment with business objectives; Design and develop data pipelines and automation workflows using DataOps tools.; Implement CI/CD pipelines for data pipelines and applications; Implement data quality checks and validation processes.; Ensure system reliability, data security and compliance.; Requirements:; Degree in Computer Science, Data Science, Mathematics or a related IT field; Excellent time management, prioritization, and multitasking skills.; Strong interpersonal and communication skills.; Team-oriented, self-motivated and adaptable.; If you are looking for an environment of growth and opportunities, please write in with a detailed resume stating the position applied and expected salaries to the HR department via recruitment@phillip.com.sg.; We regret that only shortlisted candidates will be notified.; Brought to you by Phillip Securities Pte Ltd (A member of PhillipCapital)",,Full time,singapore
85043745,Data Engineer,Tabernacle Pte Ltd,Central Region,2025-06-19 17:57:21,https://id.jobstreet.com/id/job/85043745,"We are seeking a hands-on Fabric Onsite Data Engineer to support our data modernization initiatives by leveraging Microsoft Fabric. This role is critical in transitioning legacy ADF pipelines to Fabric Pipelines and Dataflows Gen2 while ensuring high data quality, robust metadata management, and effective collaboration with business and technical stakeholders.; Key Responsibilities:; Rebuild and optimize existing Azure Data Factory (ADF) ingestion processes using Microsoft Fabric Pipelines and Dataflows Gen2.; Coordinate with SL10/EQS source system owners to align data schemas, field mappings, and ingestion specifications.; Perform onsite data validation and integrity checks across ingestion workflows to ensure accuracy and completeness.; Capture and log file-level and field-level metadata (e.g., file name, source system, ingestion timestamp) to enhance traceability and auditability.; Support User Acceptance Testing (UAT), triage user-reported issues, and provide technical support to business users during validation phases.; Train and guide end users on accessing and using Power BI reports, data glossary resources, and Baobao Q&A tools for self-service analytics.; Proactively respond to data queries from business users, ensuring data understanding and alignment with operational needs.; Qualifications:; Diploma or Degree in Information Technology, Computer Science, or related field.; Proven experience with Microsoft Fabric, including Pipelines and Dataflows Gen2.; Strong knowledge of Azure Data Factory, data ingestion strategies, and ETL/ELT best practices.; Familiarity with data modeling, schema design, and metadata logging.; Experience working with business users and system owners to align technical data structures with business requirements.; Solid understanding of data quality assurance, validation techniques, and troubleshooting in production environments.; Excellent communication skills to support user training and documentation.; Ability to work onsite and collaborate across cross-functional teams in a dynamic environment.",,Kontrak/Temporer,singapore
85784453,Data Engineer (Cloud Migration),ANTAS PTE. LTD.,Singapore,2025-07-15 17:57:21,https://id.jobstreet.com/id/job/85784453,"Role Overview:; Seeking a seasoned data engineer (7+ years) to design, build, and optimize scalable data solutions using AWS (S3, RDS, Redshift, Glue, Lambda), Databricks (Delta Lake, Spark), and Informatica IDMC.; Role Overview:; Seeking a seasoned data engineer (7+ years) to design, build, and optimize scalable data solutions using AWS (S3, RDS, Redshift, Glue, Lambda), Databricks (Delta Lake, Spark), and Informatica IDMC.; Key Responsibilities:; Architect data platforms including lakes, warehouses, and databases on AWS and Databricks; Develop and automate ETL/data pipelines with AWS Glue, Lambda, Databricks, and Informatica IDMC; Integrate and transform data from various sources ensuring quality and governance; Monitor and tune performance of data workflows and queries; Implement security, compliance, and cost optimization best practices; Maintain documentation and collaborate across teams to meet data needs; Skills Required:; 7+ years of experience in data engineering with hands-on work in AWS, Databricks, and/or Informatica IDMC; Skilled in Python, Java, or Scala; strong SQL and NoSQL knowledge; Experience in data modeling, performance tuning, and debugging; Strong problem-solving, communication, and collaboration skills; Certifications in AWS, Databricks, or Informatica are a plus; Experience with Apache Spark, Hadoop, and Informatica IDMC for data governance; Knowledge of Tableau or Power BI; Familiarity with Docker, Kubernetes, Git, CI/CD, and DevOps principles",,Kontrak/Temporer,singapore
85043774,Data Engineer,Tabernacle Pte Ltd,Central Region,2025-06-19 17:57:21,https://id.jobstreet.com/id/job/85043774,"Role Overview:; We are looking for a skilled Remote Fabric Data Engineer to support our ongoing data platform migration and transformation efforts. This role focuses on migrating existing Databricks workloads into Microsoft Fabric, implementing efficient transformation logic, and ensuring complete lineage documentation and performance optimization.; Key Responsibilities:; Migrate existing Databricks Silver/Gold layer SQL transformations into Microsoft Fabric SQL and Dataflows.; Design and implement transformation logic, including deduplication, joins, field mappings, and business rules in Fabric Dataflows.; Develop dimensional data models with surrogate key generation, row-level security (RLS) attributes, and role-based filtering.; Build and maintain metadata lineage pipelines to track data flow across the lifecycle — from source systems through transformation to target layers.; Tune performance of Fabric solutions by applying partitioning strategies, indexing, and aggregations where applicable.; Produce clear, concise documentation including:; Dataflow structures and logic; Dimensional model definitions; End-to-end lineage path summaries for audit and traceability; Qualifications:; Diploma or Degree in Information Technology, Computer Science, or related field.; Any other related certifications;  Experience and Skills:; Strong experience in Microsoft Fabric (Dataflows, Pipelines, Fabric SQL, and Lakehouses).; Prior hands-on experience migrating Databricks SQL logic or similar workloads into Microsoft environments.; Solid understanding of data modeling, data warehousing, and ETL/ELT transformations.; Proficiency in performance tuning and working with large-scale data systems using partitioning and indexing.; Experience with metadata management, data lineage, and documentation practices.; Comfortable working in a fully remote setting with distributed teams.; Excellent written communication skills to produce technical documentation and collaborate asynchronously.",,Kontrak/Temporer,singapore
85688508,Cloud Data Engineer,Unison Consulting Pte. Ltd.,Singapore,2025-07-11 17:57:21,https://id.jobstreet.com/id/job/85688508,"Design and architect data storage solutions, including databases, data lakes, and warehouses, using AWS services such as Amazon S3, Amazon RDS, Amazon Redshift, and Amazon DynamoDB, along with Databricks' Delta Lake. Integrate Informatica IDMC for metadata management and data cataloging.; Create, manage, and optimize data pipelines for ingesting, processing, and transforming data using AWS services like AWS Glue, AWS Data Pipeline, and AWS Lambda, Databricks for advanced data processing, and Informatica IDMC for data integration and quality.; Integrate data from various sources, both internal and external, into AWS and Databricks environments, ensuring data consistency and quality, while leveraging Informatica IDMC for data integration, transformation, and governance.; Develop ETL (Extract, Transform, Load) processes to cleanse, transform, and enrich data, making it suitable for analytical purposes using Databricks' Spark capabilities and Informatica IDMC for data transformation and quality.; Monitor and optimize data processing and query performance in both AWS and Databricks environments, making necessary adjustments to meet performance and scalability requirements. Utilize Informatica IDMC for optimizing data workflows.; Requirements; Good experience in data engineering, with expertise in AWS services, Databricks, and/or Informatica IDMC.; Proficiency in programming languages such as Python, Java, or Scala for building data pipelines.; Evaluate potential technical solutions and make recommendations to resolve data issues especially on performance assessment for complex data transformations and long running data processes.; Strong knowledge of SQL and NoSQL databases.; Familiarity with data modeling and schema design.; AWS certifications (e.g., AWS Certified Data Analytics - Specialty, AWS Certified Data Analytics - Specialty), Databricks certifications, and Informatica certifications are a plus.",,Full time,singapore
85673927,Data Architect - Technology (SAS VI),UNIVERSAL PROCUREMENT SYSTEMS PTE LTD,Singapore,2025-07-11 17:57:21,https://id.jobstreet.com/id/job/85673927,"We are looking for a seasoned Data Architect / Data Management Lead with extensive experience in designing data solutions for fraud analytics , investigative platforms , or intelligence and case management systems . The successful candidate will oversee the architecture, development, and implementation of integrated data pipelines and analytics frameworks within SAS-based environments , particularly SAS Viya . This is a hands-on leadership role requiring both technical depth and strong stakeholder coordination across business and technology teams.; Responsibilities:; Partner with business analysts to understand and shape data requirements for fraud or investigative systems.; Architect end-to-end data strategies, from source acquisition and cleansing to storage, preparation, and analytics delivery.; Convert functional needs into technical deliverables across ETL, engineering, and reporting layers.; Design and manage real-time and batch ingestion workflows into SAS platforms or large-scale data lakes.; Build and maintain robust data pipelines, orchestration layers, and visualization components using SAS Viya , DS2 , Python , and other tools.; Coordinate testing, integration, and support activities, including UAT and post-deployment maintenance.; Lead documentation efforts and contribute to overall data governance, metadata, and quality frameworks.; Troubleshoot complex data issues and track project progress against delivery milestones.; Required Experience & Skills:; Minimum 10 years of experience in data architecture, analytics platforms, or BI/data warehouse implementations.; Delivered at least 5 full-scale projects involving fraud detection, surveillance systems, or case/investigation platforms.; Strong hands-on capabilities in:; SAS Viya , SAS DS2 , and SQL; Python , Spark , JSON , XML; Linux , job orchestration tools , and real-time technologies like Kafka or SAS ESP; Postman , SOAPUI , REST APIs , and CI/CD frameworks ( DevOps / DataOps ); Familiar with:; Data modeling, source-to-target mapping, real-time processing design; Metadata and data lineage tools, orchestration frameworks; Experience with SAS Intelligent Decisioning, RTDM, PEGA, or similar decision management tools is advantageous.; Prior involvement in public sector, government, or regulatory environments is preferred.; Strong communication and stakeholder alignment skills; capable of leading technical teams.; Bachelor’s degree in a relevant discipline such as Computer Science, Information Technology, or Statistics",,Full time,singapore
85549955,"AD, Data & AI Platform Enablement",Singapore Telecommunications Limited,Singapore,2025-07-07 17:57:21,https://id.jobstreet.com/id/job/85549955,"An empowering career at Singtel begins with a Hello. Our purpose, to Empower Every Generation, connects people to the possibilities they need to excel. Every ""hello"" at Singtel opens doors to new initiatives, growth, and BIG possibilities that takes your career to new heights. So, when you say hello to us, you are really empowered to say…“Hello BIG Possibilities”.; Be a Part of Something BIG! ; This position plays a pivotal technical role in AI & Data Analytics (AIDA) who is responsible for leading a cross-functional team of platform, data, and operations engineers to deliver a secure, scalable, and cost-efficient hybrid cloud AI infrastructure.; The role will have to work closely with architects and AI engineering teams to build a robust platform that supports a wide range of AI use cases across multiple business units.; You will collaborate with data source and platform teams from both Networks and IT to design and implement integrated data solutions that empower users to efficiently develop and scale AI applications.; Make An Impact By; Lead, manage and grow a cross functional team consisting of platform engineers, data engineers, DataOps, DevSecOps engineers, FinOps, operations and delivery managers, ensuring the successful delivery and sustainable AI platform.; Foster innovative, agile and user focused team culture to accelerate delivery of AI and advance analytics use cases; Coordinate workload within team and integration efforts with various departments across Singtel SG; Evaluate Singtel existing and latest tools available in market to build best-in-class telco data and AI tech stack; Plan unified data layer to enable availability of cross-domain data sources in single location for AI use; Design integration architecture to existing and upcoming systems across Singtel Singapore; Deliver and implement hybrid cloud AIDA platform; Monitor platform performance, availability and utilization to ensure stability; Operate and maintain data and AI platform to maximize availability and platform           ; Manage implementation timeline to meet use case delivery targets; Integrate data sources across Singtel SG efficiently to allow effective cross-domain AI and data use cases; Responsible for data quality monitoring, cataloguing, CII & PII data and sensitivity classification for AIDA; Set-up and operate AIDA’s DevSecOps platform for efficient, traceable development and deployment; Constantly update and upgrade pipelines and security policies to align with latest libraries and cybersecurity recommendations; Implement solutions to continuously optimize compute resource and data transfer cost; Manage platform and cloud cost (FinOps) for cost effective AI operations; Skills for Success:; Bachelor’s degree in computing, engineering or relevant fields; 5+ years experience in design, implementation and operation of hybrid cloud data engineering platform; 1-2 years of team leadership or delivery management experience; Capable of designing hybrid cloud data and AI system; Experience in operation of hybrid cloud data and AI systems; Proficient in data ingestion and integration development (Spark, Kafka, Hadoop, Azure Storage, etc.); Experience in workload optimization and FinOps to minimize computing cost; Good knowledge of DevSecOps processes and tools; Demonstrated good project management skills in implementation of data/system projects; Rewards that Go Beyond; Full suite of health and wellness benefits ; Ongoing training and development programs ; Internal mobility opportunities; Are you ready to say hello to BIG Possibilities?; Take the leap with Singtel to unlock new opportunities and accelerate your growth. Apply now and start your empowering career!",,,singapore
85578382,DATA ENGINEER,Continental Technology Solutions Pte Ltd,Kampong Ubi,2025-07-07 17:57:21,https://id.jobstreet.com/id/job/85578382,"Responsibilities:; Design, implement, and maintain scalable and reliable data pipelines.; Build and optimize data architectures (data lakes, data warehouses, ETL/ELT processes).; Collaborate with data scientists, analysts, and other stakeholders to understand data needs.; Develop data models and structures for reporting and analytics.; Ensure data quality, integrity, security, and compliance with governance policies.; Monitor and troubleshoot performance issues with data systems.; Automate data workflows using modern orchestration tools.; Document processes, systems, and data flows.; Requirements:; Bachelor's degree in Computer Science, Engineering, or a related field.; 2+ years of experience in a data engineering or similar role.; Strong SQL skills and experience with relational databases (e.g., PostgreSQL, MySQL).; Proficiency in Python, Scala, or Java for data manipulation and pipeline development.; Experience with ETL/ELT tools (e.g., Apache Airflow, dbt, Talend).; Familiarity with data warehouse solutions (e.g., Snowflake, Redshift, BigQuery).; Experience with cloud platforms (AWS, GCP, or Azure) and their data services.; Understanding of data modeling, data governance, and best practices.",,Kontrak/Temporer,singapore
85607945,Data Engineer (Cloud Migration Projects),Synapxe,One North,2025-07-09 17:57:21,https://id.jobstreet.com/id/job/85607945,"Position Summary / Project Description; Data Engineer for cloud migration projects (Primarily utilizing AWS, IDMC, Databricks and Tableau); Role and Responsibilities; • Develop ETL (Extract, Transform, Load) processes to cleanse, transform, and enrich data, making it suitable for analytical purposes using Databricks' Spark capabilities and Informatica IDMC for data transformation and quality.; • Monitor and optimize data processing and query performance in both AWS and Databricks environments, making necessary adjustments to meet performance and scalability requirements. Utilize Informatica IDMC for optimizing data workflows.; • Implement security best practices and data encryption methods to protect sensitive data in both AWS and Databricks, while ensuring compliance with data privacy regulations. Employ Informatica IDMC for data governance and compliance.; • Implement automation for routine tasks, such as data ingestion, transformation, and monitoring, using AWS services like AWS Step Functions, AWS Lambda, Databricks Jobs, and Informatica IDMC for workflow automation.; • Maintain clear and comprehensive documentation of data infrastructure, pipelines, and configurations in both AWS and Databricks environments, with metadata management facilitated by Informatica IDMC.; • Collaborate with cross-functional teams, including data scientists, analysts, and software engineers, to understand data requirements and deliver appropriate solutions across AWS, Databricks, and Informatica IDMC.; • Identify and resolve data-related issues and provide support to ensure data availability and integrity in both AWS, Databricks, and Informatica IDMC environments.; • Optimize AWS, Databricks, and Informatica resource usage to control costs while meeting performance and scalability requirements.; • Stay up-to-date with AWS, Databricks, Informatica IDMC services, and data engineering best practices to recommend and implement new technologies and techniques.; Requirements / Qualifications; • Bachelor’s or master’s degree in computer science, data engineering, or a related field.; • Minimum 4 years of experience in data engineering, with expertise in AWS services, Databricks, and/or Informatica IDMC.; • Proficiency in programming languages such as Python, Java, or Scala for building data pipelines.; • Evaluate potential technical solutions and make recommendations to resolve data issues especially on performance assessment for complex data transformations and long running data processes.; • Strong knowledge of SQL and NoSQL databases.; • Familiarity with data modeling and schema design.; • Excellent problem-solving and analytical skills.; • Strong communication and collaboration skills.; • AWS certifications, Databricks certifications, and Informatica certifications are a plus.",,"Kontrak/Temporer, Full time",singapore
85702523,Senior Data Engineer,Singapore Telecommunications Limited,Singapore,2025-07-09 17:57:21,https://id.jobstreet.com/id/job/85702523,"An empowering career at Singtel begins with a Hello. Our purpose, to Empower Every Generation, connects people to the possibilities they need to excel. Every ""hello"" at Singtel opens doors to new initiatives, growth, and BIG possibilities that takes your career to new heights. So, when you say hello to us, you are really empowered to say…“Hello BIG Possibilities”.; Be a Part of Something BIG!  ; Design, build, and maintain scalable data pipelines and processing systems that power analytics and AI use cases across a hybrid data platform; Contribute to design conversation of AIDA’s new data and AI platform; Collaborate with platform, analytics, and governance teams to deliver high-quality, secure, and well-documented data assets; Lead team of data engineers to ensure timely availability of accurate, well-documented data for use in AI use case; Make An Impact By; Design and implement batch and streaming data ingestion pipelines from diverse sources (e.g., files, APIs, Kafka, databases); Develop real-time and near-real-time data workflows using tools like Apache Flink, Kafka Streams; Optimize performance for high-volume and high-velocity datasets; Implement data quality checks and automatic monitoring system to ensure consistently accurate and available data; Design and manage storage solutions such as Microsoft Fabric, Delta Lake, and Databricks; Apply best practices for schema design, partitioning, and data lifecycle management; Support data discovery and cataloguing in coordination with governance tools; Work with data scientists, analysts, and business users to understand data needs and translate them into technical solutions; Partner with DevSecOps and platform engineers to automate deployment and orchestration of data pipelines; Document data flows, transformations, and quality checks in accordance with governance standard; Skills to Succeed; Bachelor’s or Master’s degree in Computer Science, Engineering, or a related field; 5 years of experience in data engineering; 1 year in a senior technical role delivering data engineering projects; Deep expertise in Spark, Databricks, and data processing frameworks; Strong knowledge of streaming technologies such as Apache Kafka, Apache Flink, or Azure Event Hub; Experience working with data lake and/or lakehouse architectures such as Hadoop, Delta Lake, Iceberg and Microsoft OneLake; Proficient in Python and SQL; Familiar with workflow orchestration (e.g., Apache Airflow) and CI/CD principles; Analytical mindset with a focus on data quality, performance, and maintainability; Able to work independently and collaboratively in a dynamic environment; Strong communication and documentation skills to support cross-functional collaboration; Knowledge of data types across network and IT in telco environment; Rewards that Go Beyond; Full suite of health and wellness benefits  ; Ongoing training and development programs  ; Internal mobility opportunities; Your Career Growth Starts Here. Apply Now!; We are committed to a safe and healthy environment for our employees & customers and will require all prospective employees to be fully vaccinated.",,Full time,singapore
85273433,Data Engineer Lead,DCS Card Centre Pte. Ltd.,East Region,2025-06-30 17:57:21,https://id.jobstreet.com/id/job/85273433,"Key Responsibilities:; Lead the development of a financial-grade real-time data platform from 0 to 1 (covering data collection, storage, and computation layers), designing an architecture that supports daily processing of petabyte-scale transactional data.; Establish a cloud-native (AWS) data warehouse system integrated with Spark/Flink for unified stream and batch processing, enabling millisecond-level metric computation for payment data.; Interface with the company’s full-stack business (e.g., acquiring and issuing) and design data masking workflows and audit solutions that comply with PCI DSS.; Build and maintain data dashboards to support business teams in data-driven decision-making.; Integrate with payment systems to ensure seamless data flow and consolidation.; Responsible for data collection, processing, modeling, and structuring to improve data quality and usability.; Support cross-departmental data needs and promote data standardization and automation.; Work closely with teams such as risk control, compliance, and product to provide data support.; Requirements:; Over 5 years of experience in data engineering or data warehousing, with the capability to independently build data platforms.; Familiar with mainstream big data technology stacks, and proficient in performance tuning for Hadoop/Spark/Flink (e.g., shuffle optimization, checkpoint mechanism).; Skilled in data modeling (Dimensional/Star Schema), ETL processes, and data governance.; Preferred background in payments, finance, or internet industries; familiarity with payment data structures is a plus.; Strong communication, collaboration, and business understanding skills, with the ability to work effectively with product, risk, and compliance teams.; Preferred Qualifications:; Experience in data-related roles within fintech or payment companies; Hands-on experience building data platforms or dashboards from scratch.; Familiarity with compliance-related data processing policies and workflows.",,Full time,singapore
85437508,Data Engineer for URA with 4 years experience (Contract),Websparks Pte Ltd,East Region,2025-07-03 17:57:21,https://id.jobstreet.com/id/job/85437508,"About Design and Planning Lab (DPLab); Be part of URA’s DPLab, which spearheads URBEX efforts in the experimentation and development of smart planning technologies through Policy-Ops-Tech collaborations. Our teams leverage on their capabilities in data policy, governance, strategy, and engineering, product strategy, business analysis, software engineering, cloud engineering, urban design technologies, data science, AI/ML, and modelling and simulation to collaborate with planners, architects, and policymakers to create insights and digital solutions for Singapore’s urban planning challenges.; Data Engineering Work:; Support data engineering tasks, including the implementation and enhancement of data pipelines, as well as the rectification of broken pipelines.; Manage the data platform and perform regular software version upgrades across all environments, ensuring thorough testing and detailed documentation.; Support daily operational needs by handling application configuration changes, managing user access request to application, addressing general function queries, and troubleshooting on issues.; Perform source code review and configuration review periodically to ensure code quality and verify that sensitive information, such as secrets, is not hardcoded or embedded in source codes or configuration files.; Periodic patching of Azure Cloud Servers; Develop business processes by engaging stakeholders to understand use cases for building data pipelines, performing data modeling, completing data collection forms, documenting use cases, defining data attributes within various data quality zones, and establishing naming conventions for datasets.; Requirements:; Degree in Computer Science, Engineering, or related disciplines; Experienced in data engineering, including the implementation of data pipelines, development of data models and schemas, and pipeline monitoring and management.; Experienced in architecting, designing, and developing data platform.; Experience with:; ○ Python, PySpark, or similar programming languages; ○ Python packages for data manipulation and analysis (e.g. Pandas, GeoPandas, Shapely); ○ Database design and management (e.g. PostgreSQL, MS SQL, Oracle, Geodatabase); ○ SQL programming (e.g. writing complex queries, optimizing performance, data manipulation); ○ Scripting and version control (e.g. Bash, PowerShell, Git); ○ ETL (Extract, Transform, Load) processes; ○ Technologies such as JupyterHub, RStudio, PowerBI; ○ CI/CD tools such as Jenkins, GitLab, YAML; ○ GIS technology (e.g. ArcGIS Server, PostGIS); ○ API development and SFTP for secure data transfer; ○ Cloud Platforms (e.g. Microsoft Azure, AWS); ○ Cloud Technologies (e.g. Azure Data Factory, Databricks, Azure Functions, Azure Key Vault, AWS Lambda); ○ Containerization technologies (e.g. Docker, Kubernetes); Able to work well with a team and be willing to learn.; Effective presentation, communication and writing skills.; Added advantage with the following:; Infrastructure-as-code Tools – Terraform, CloudFormation; Log Management tools (e.g. Azure Monitor, AWS CloudWatch, Splunk); Agile Management Tools – Confluence, Jira, Kanban board",,Kontrak/Temporer,singapore
85575223,Data Engineer,Synpulse Singapore Pte. Ltd.,Singapore,2025-07-08 17:57:21,https://id.jobstreet.com/id/job/85575223,"We are an established, globally active management consulting company with offices in Switzerland, Germany, Austria, UK, USA, Singapore, Hong Kong, the Philippines, Australia, Indonesia and India. We are a valued partner to many of the world‘s largest international financial services and insurance firms. We support our clients at all project management stages from the development of strategies and operational frameworks to the technical implementation and handover. Our expertise in business and technology combined with our methodic approach enable us to create sustainable added value for our clients business.; About the job: ; Develop processes of the ingestion of data using various programming languages, techniques and tools from systems implemented using Oracle, Teradata, SAP, and Hadoop technology stack ; Evaluate and make decisions around dataset implementations designed and proposed by peer engineers ; Build large consumer database models for financial planning & analytics including Balance Sheet, Profit and Loss, Cost Analytics and Related Ratios ; Develop ETL, real time and batch data processes feeding into in-memory data infrastructure ; Perform and document data analysis, data validation, and data mapping/design ; Work with clients to solve business problems in fraud, compliance and financial crime and present project results ; Use emerging and open-source technologies such as Spark, Hadoop, and Scala ; Collaborate on scalability issues involving access to massive amounts of data and information ; You should be comfortable with working with high profile clients on their sites ; About you:; Requirements; Bachelor's degree in computer science, Physics, Mathematics, or similar degree or equivalent ; Experience with open source big-data tools, such as Spark, Hadoop, and specially Scala ; 2 to 6 years of experience working in the Financial Services sector on big data project implementations ; Demonstrate strong analytical and problem-solving skills and the ability to debug and solve technical challenges with sometimes unfamiliar technologies ; Client facing experience, good communication and presentation skills ; Strong technical communication skills with demonstrable experience of working in rapidly changing client environments ; Quantexa Certification preferred ; Why us:; Flexible working hours with part-time working models and hybrid options; Attractive fringe benefits and salary structures in line with the market; Modern and central office space with good public transport connections; Can-do mentality and one-spirit culture; Varied events and employee initiatives; Your documents to start the process:; Resume; Job references; Qualifications (bachelor/ master diploma, etc.) with certificate of grades; Motivation letter: Why Synpulse? Why you? Why this function?; Recommendation letters (optional); Do you approach your tasks with commitment and enjoyment and are you convinced that teamwork achieves better results than working alone? Are you proactive and willing to go the extra mile for your clients? Are you motivated not only to design solutions but also to implement them? As a flexible and goal-oriented person, you will quickly assume entrepreneurial responsibility with us.;  ; Do you appreciate the spirit of a growing international company with Swiss roots and a strong corporate culture? Then we look forward to receiving your online application at http://synpulse.com",,Full time,singapore
85019312,Senior Data Engineer,GC ASIA DENTAL PTE LTD,Tampines,2025-06-18 17:57:21,https://id.jobstreet.com/id/job/85019312,"Job Responsibilities; Microsoft Fabric Expertise: Serve as the primary subject matter expert for Microsoft Fabric, including but not limited to Lakehouse, Data Factory, Synapse Data Engineering (Spark), Data Warehousing, Real-Time Analytics, and Power BI integration.; Data Solution Design & Development: Lead the end-to-end design, development, and implementation of robust, scalable, and efficient data pipelines and solutions within the Microsoft Fabric ecosystem. This includes data ingestion, transformation, orchestration, and consumption layers.; Architectural Leadership: Contribute to and drive the architectural vision for our data platform on Microsoft Fabric, ensuring best practices for performance, security, reliability, and cost-effectiveness.; Independent Project Management: Take full ownership of data engineering projects, including planning, resource allocation (where applicable), timeline management, risk mitigation, and successful delivery.; Stakeholder Collaboration: Work closely with data analysts, business stakeholders, and other engineering teams to understand data requirements, translate them into technical specifications, and deliver impactful data solutions.; Data Governance & Quality: Implement and enforce data governance, quality, and security best practices within the Microsoft Fabric environment.; Performance Optimization & Troubleshooting: Proactively monitor, troubleshoot, and optimize data pipelines and data models for performance and efficiency.; Mentorship & Best Practices: Champion best practices in data engineering, data modeling, and Microsoft Fabric utilization. Potentially mentor junior team members and foster a culture of continuous learning.; Documentation: Create and maintain comprehensive technical documentation for data pipelines, data models, and architectural designs.; Job Requirements; Minimum 5+ years of experience in data engineering, with a strong focus on building and maintaining enterprise-grade data platforms.; Demonstrable expert-level proficiency in Microsoft Fabric, with hands-on experience across multiple components (Lakehouse, Data Factory, Synapse Data Engineering/Spark, Data Warehousing, Real-Time Analytics).; Proven track record of successful project management in data-related initiatives, including planning, execution, and delivery.; Extensive experience with Spark (PySpark/Scala) for data transformation and processing within a distributed environment.; Strong understanding of data warehousing concepts, dimensional modeling, and data lake architectures.; Proficiency in SQL for data manipulation and analysis.; Experience with Azure ecosystem components (e.g., Azure Data Lake Storage, Azure DevOps, Azure Functions) is a strong plus.; Experience with version control systems (e.g., Git).; Excellent problem-solving, analytical, and critical thinking skills.; Exceptional communication and interpersonal skills, with the ability to articulate complex technical concepts to non-technical stakeholders and work effectively in a team-oriented environment.; Highly self-motivated and able to work independently with minimal supervision, taking initiative and ownership of tasks.; Microsoft Certified: Azure Data Engineer Associate, or other relevant Microsoft Azure/Fabric certifications.; Experience with real-time data streaming technologies.; Familiarity with CI/CD practices for data pipelines.; Experience with Power BI for data visualization and reporting.",,Full time,singapore
85187447,Data Engineer,Goldtech Resources Pte Ltd,Singapore,2025-06-26 17:57:21,https://id.jobstreet.com/id/job/85187447,"Data Engineer (Full Time); Location In Singapore: Islandwide; Salary(SGD): 5000 - 7000; Apply; Job Description:; 1.Data Analysis and Insight Discovery: Conduct thorough analysis of structured and unstructured data sets to uncover trends, patterns, and valuable insights that inform business decisions and strategies.; 2.Data Visualization and Communication: Create user-friendly, visually engaging dashboards, reports, and presentations to convey complex data analyses to stakeholders, facilitating data-driven decision-making at all organizational levels.; 3.Data Modeling and Transformation: Plan, execute, and maintain data models, ETL pipelines, and data integration processes, ensuring data accuracy, integrity, and accessibility.; 4.Data Quality Assurance: Enforce rigorous data validation and cleansing procedures to guarantee the accuracy and dependability of data used for analysis and reporting.; 5.Identifying Data Requirements: Collaborate with stakeholders across different departments to comprehend their data needs and deliver insights aligned with their business objectives.; 6.Continuous Enhancement: Stay updated on industry trends and best practices in data analysis and engineering. Propose and implement process enhancements to improve data quality, efficiency, and overall performance.; 7.Data Security & Compliance: Establish, uphold, and oversee data security and privacy measures on the data platform.; 8.Cross-Functional Collaboration: Work closely with other teams to ensure the successful execution of projects and initiatives.; 9.Operational Support: Maintain and provide day-to-day operational assistance on the data platform, including:; -> Monitoring data pipelines, databases and analytical processes to ensure uninterrupted operation.; -> Promptly identify and resolve data-related issues, performance bottlenecks, and data quality concerns.; Requirements:; ·     Bachelor's degree in Computer Science, Data Science, Statistics, or a closely related field.; ·    Have a minimum of 5 years of practical experience in data platforms, data analytics, or related projects.; · Data Analysis Proficiency: Demonstrate a proven track record as a Data Analyst, showcasing the ability to translate intricate data sets into actionable insights.; · Data Visualization Skills: Exhibit strong expertise in data visualization tools like Tableau, Power BI, or similar, enabling the creation of compelling data visualizations and reports.; ·  Programming Aptitude: Display proficient programming skills in languages such as Python, SQL, SAS, or R for data analysis and data engineering tasks.; · Data Engineering Experience: Prior experience as a Data Engineer, with a demonstrated capability to design and implement data models, ETL processes, and data integration solutions.; · Technology Familiarity: Be acquainted with cloud-based data platforms like Azure Data Factory, Azure Purview, Databricks, Snowflake, Kafka, or equivalent, as well as data storage technologies such as SQL and NoSQL databases.; ·  Having prior experience with Microsoft SQL Server, Microsoft SSIS, or Ali Cloud (Dataphin) is considered a plus.; ·  Problem-Solving Acumen: Possess excellent problem-solving skills, particularly in dealing with large and complex data sets.; · Attention to Detail: Maintain a strong commitment to detail, ensuring data accuracy and reliability in all analyses.; · Effective Communication: Demonstrate effective communication skills to convey complex technical concepts to non-technical stakeholders.; ·  Collaborative Team Player: Prove the ability to work independently as well as collaboratively in a fast-paced, team-oriented environment.; ·  Certifications: Hold certifications in Tableau Data Analyst, Microsoft Power BI Data Analyst, SnowPro (Snowflake), Databricks, or their equivalents.; Apply",,Full time,singapore
85232560,Reporting and Analytics Developer/Data Engineer 0642B,USER EXPERIENCE RESEARCHERS PTE. LTD,Singapore,2025-06-27 17:57:21,https://id.jobstreet.com/id/job/85232560,"We are seeking a highly skilled and experienced Big Data Engineer to join our team. The ideal candidate will have a minimum of 4 years of experience managing data engineering jobs in big data environment e.g., Cloudera Data Platform. The successful candidate will be responsible for designing, developing, and maintaining the data ingestion and processing jobs. Candidate will also be integrating data sets to provide seamless data access to users.; Responsibilities; Analyse the Authority's data needs and document the requirements.; Refine data collection/consumption by migrating data collection to more efficient channels.; Plan, design and implement data engineering jobs and reporting solutions to meet the analytical needs.; Develop test plan and scripts for system testing, support user acceptance testing.; Build reports and dashboards according to user requirements; Work with the Authority's technical teams to ensure smooth deployment and adoption of new solution.; Ensure the smooth operations and service level of IT solutions.; Support production issues.; What we are looking for:; Good understanding and completion of projects using waterfall/Agile methodology.; Strong SQL, data modelling and data analysis skills are a must.; Hands-on experience in big data engineering jobs using Python, Pyspark, Linux, and ETL tools like Informatica.; Hands-on experience in a reporting or visualization tool like SAP BO and Tableau is must.; Hands-on experience in DevOps deployment and data virtualisation tools like Denodo will be an advantage.; Track record in implementing systems using Hive, Impala and Cloudera Data Platform will be preferred.; Good understanding of analytics and data warehouse implementations.; Ability to troubleshoot complex issues ranging from system resource to application stack traces.; Track record in implementing systems with high availability, high performance, high security hosted at various data centres or hybrid cloud environments will be an added advantage.; Passion for automation, standardization, and best practices.",,Full time,singapore
83649507,Senior Data Engineer,Mediacorp Pte Ltd,Singapore,2025-06-17 17:57:21,https://id.jobstreet.com/id/job/83649507,"Job Description; PURPOSE OF THIS ROLE; The purpose of the Senior Data Engineer role is to provide strategic leadership in data engineering, overseeing the development and maintenance of data infrastructure, and ensuring the availability, reliability, and performance of data solutions. This position plays a critical role in supporting data-driven decision-making across the organization.; Key responsibilities include:; Providing strategic direction and technical leadership in data engineering.; Leading the design, development, and deployment of ETL processes and data pipelines.; Integrating data from various sources and transforming it for analytics.; Maintaining data quality and implementing data governance best practices.; Managing and mentoring a team of data engineers for professional growth.; Collaborating with stakeholders to understand and fulfil data requirements.; Documenting data processes, architectures, and best practices.; Challenges in this role may include:; Ensuring data security and compliance in a rapidly evolving regulatory environment.; Optimizing data infrastructure for performance and scalability.; Resolving data-related issues efficiently.; Managing a dynamic team in a fast-paced data environment.; FOUNDATIONAL/LEADERSHIP COMPETENCIES; Strong leadership and team management skills.; Excellent problem-solving and communication skills.; FUNCTIONAL COMPETENCIES; Proficiency in programming languages such as Java, or Scala, SQL, Python.; Extensive experience with ETL tools, data integration, and data transformation.; In-depth knowledge of data storage technologies (relational databases, NoSQL, distributed file systems).; Expertise in data modelling, database design and data warehousing.; Familiarity with data governance, security, and compliance standards.; Experience with big data technologies (e.g., Hadoop, Spark, Hive, Azure, Databricks) is a plus.; Experience in container orchestration framework like Kubernetes is a plus.; Experience in Infrastructure as Code and DevOps, MLOps and DataOps is a plus.; Job Requirements; Bachelor's or Master's degree in Computer Science, Information Technology, or a related field.; Relevant certifications in data engineering and Cloud computing are advantageous.",,Full time,singapore
84921566,"Data Engineer/Senior Data Engineer, DXD (Digital Excellence & Products...",Public Service Division,Singapore,2025-06-17 17:57:21,https://id.jobstreet.com/id/job/84921566,"[What the role is]; The Government Technology Agency (GovTech) is the lead agency driving Singapore’s Smart Nation initiatives and public sector digital transformation. As the Centre of Excellence for Infocomm Technology and Smart Systems (ICT & SS), GovTech develops the Singapore Government’s capabilities in Data Science & Artificial Intelligence, Application Development, Smart City Technology, Digital Infrastructure, and Cybersecurity.; At GovTech, we offer you a purposeful career to make lives better. We empower our people to master their craft through continuous and robust learning and development opportunities all year round. Our GovTechies embody our Agile, Bold and Collaborative values to deliver impactful solutions.; GovTech aims to transform the delivery of Government digital services by taking an ""outside-in"" view, putting citizens and businesses at the heart of everything we do.; Play a part in Singapore’s vision to build a Smart Nation and embark on your meaningful journey to build tech for public good. Join us to advance our mission and shape your future with us today!; Learn more about GovTech at tech.gov.sg.; [What you will be working on]; ; ""To Mould the Future of Our Nation""; At MOE, we believe in enabling every learner to thrive in a rapidly changing world. As part of our mission, we are building internal AI capabilities to improve student learning outcomes, enhance educator productivity, and strengthen our ability to innovate sustainably. Through the responsible and meaningful application of AI, we aim to advance personalised learning, support teaching, and transform educational operations.; As a Data Engineer, you will play a key role in shaping MOE’s AI capabilities by leading the evaluation, optimisation, and deployment of AI models for education. You will partner closely with product managers, engineers, curriculum specialists, and policy teams to design solutions that are pedagogically relevant, technically robust, and ready for scale.; Our Team; You will be part of the Digital Excellence & Products Division (DXD), a cross-functional team driving MOE’s digital transformation across platforms, policies, and products. Our Forward-Deployed AI/Data Science Team focuses on rapidly applying AI to real-world problems in education — from personalised learning and curriculum support to school operations.; What you will be working on:; Translate data requirements from business users into technical specifications.; Collaborate with partner agency’s IT teams on technology stack, infrastructure and security alignment.; Build out data product as part of a data team:; Architect and build ingestion pipelines to collect, clean, merge, and harmonize data from different source systems.; Day-to-day monitoring of databases and ETL systems, e.g., database capacity planning and maintenance, monitoring, and performance tuning; diagnose issues and deploy measures to prevent recurrence; ensure maximum database uptime;; Construct, test, and update useful and reusable data models based on data needs of end users.; Design and build secure mechanisms for end users and systems to access data in data warehouse.; Research, propose and develop new technologies and processes to improve agency data infrastructure.; Collaborate with data stewards to establish and enforce data governance policies, best practices and procedures.; Maintain data catalogue to document data assets, metadata and lineage.; Implement data quality checks and validation processes to ensure data accuracy and consistency.; Implement and enforce data security best practices, including access control, encryption, and data masking, to safeguard sensitive data; [What we are looking for]; A Bachelor’s Degree, preferably in Computer Science, Software Engineering, Information Technology, or related disciplines. ; Deep understanding of system design, data structure and algorithms, data modelling, data access, and data storage.; Demonstrated ability in using cloud technologies such as AWS, Azure, and Google Cloud.; Experience in architecting data and IT systems.; Experience with orchestration frameworks such as Airflow, Azure Data Factory.; Experience with distributed data technologies such as Spark, Hadoop.; Proficiency in programming languages such as Python, Java, or Scala.; Proficiency in writing SQL for databases\; Familiarity with building and using CI/CD pipelines.; Familiarity with DevOps tools such as Docker, Git, Terraform.; Preferred requirements:; Experience in designing, building, and maintaining batch and real-time data pipelines. ; Experience with Databricks.; Experience with implementing technical processes to enforce data security, data quality, and data governance.; Familiarity with government systems and government's policies relating to data governance, data management, data infrastructure, and data security.; Our employee benefits are based on a total rewards approach, offering a holistic and market-competitive suite of perks. These include leave benefits to meet your work-life needs and employee wellness programmes. ; We champion flexible work arrangements (subject to your job role) and trust that you will manage your own time to deliver your best, wherever you are, and whatever works best for you. ; Learn more about life inside GovTech at go.gov.sg/GovTechCareers.; Stay connected with us on social media at go.gov.sg/ConnectWithGovTech.",,Full time,singapore
85018947,Data Engineer-Consultant,WSH Experts Pte Ltd,Singapore,2025-06-18 17:57:21,https://id.jobstreet.com/id/job/85018947,"Job Description; Create and manage a single master record for each business entity, ensuring data consistency, accuracy, and reliability.; Implement data governance processes, including data quality management, data profiling, data remediation, and automated data lineage.; Create and maintain multiple robust and high-performance data processing pipelines within Cloud, Private Data Centre, and Hybrid data ecosystems.; Assemble large, complex data sets from a wide variety of data sources.; Collaborate with Data Scientists, Machine Learning Engineers, Business Analysts, and Business users to derive actionable insights and reliable foresights into customer acquisition, operational efficiency, and other key business performance metrics.; Develop, deploy, and maintain multiple microservices, REST APIs, and reporting services.; Design and implement internal processes to automate manual workflows, optimize data delivery, and re-design infrastructure for greater scalability.; Establish expertise in designing, analyzing, and troubleshooting large-scale distributed systems.; Support and work with cross-functional teams in a dynamic environment.",,Full time,singapore
85072996,"Data Engineer (Azure Synapse Analytics, PySpark)",SembCorp Utilities Pte. Ltd.,East Region,2025-06-20 17:57:21,https://id.jobstreet.com/id/job/85072996,"About Sembcorp; Sembcorp is a leading energy and urban solutions provider headquartered in Singapore. Led by its purpose to drive energy transition, Sembcorp delivers sustainable energy solutions and urban developments by leveraging its sector expertise and global track record.; Play a role in Powering Asia’s Energy Transition; Drive Asia’s energy transition with us! Our Gas & Related Services segment is a key growth engine, delivering reliable and efficient energy to industries and communities across multiple countries. We support Asia’s growing energy needs while advancing the shift to a lower-carbon future.; Purpose and Scope; We are seeking a highly skilled and self-driven Azure Data Engineer with expertise in PySpark, Python, and modern Azure data services including Synapse Analytics and Azure Data Explorer. The ideal candidate will design, develop, and maintain scalable data pipelines and architectures, enabling effective data management, analytics, and governance.; Key Roles and Responsibilities; Design, develop, and maintain scalable and efficient data pipelines (both batch and real-time streaming) using modern data engineering tools; Build and manage data lakes, data warehouses, and data marts using Azure Data Services; Integrate data from various sources including APIs, structured/unstructured files, IoT devices, and real-time streams; Develop and optimize ETL/ELT workflows using tools such as Azure Data Factory, Databricks, and Apache Spark; Implement real-time data ingestion and processing using Azure Stream Analytics, Event Hubs, or Kafka; Ensure data quality, availability, and security across the entire data lifecycle; Collaborate with analysts, data scientists, and engineering teams to deliver business-aligned data solutions; Contribute to data governance efforts and ensure compliance with data privacy standards; Establish and manage source system connectivity (on-prem, APIs, sensors, etc.); Handle deployment and migration of data pipeline artifacts between environments using Azure DevOps; Design, develop, and troubleshoot PySpark scripts and orchestration pipelines; Perform data integration using database joins and other transformations aligned with project requirements; Any assigned ad-hoc duties.; Requirements:; Bachelor’s Degree in Computer Science, Engineering, or related field; 3–5 years of experience in Azure-based data engineering, PySpark, and Big Data technologies; Strong hands-on experience with Azure Synapse Analytics for pipeline orchestration and data handling; Expertise in SQL, data warehousing, data marts, and ingestion using PySpark and Python; Solid experience building and maintaining cloud-based ETL/ELT pipelines, especially with Azure Data Factory or Synapse; Familiarity with cloud data environments such as Azure and optionally AWS; Experience with Azure DevOps for CI/CD and artifact deployment; Excellent communication, problem-solving, and interpersonal skills; Good to Have:; 1–2 years of experience working with Azure Data Explorer (including row-level security and access controls).; Experience with Azure Purview for metadata management, data lineage, governance, and discovery; Ability to work independently and take full ownership of assignments; Proactive in identifying and resolving blockers and escalating when needed; Exposure to real-time processing with tools like Azure Stream Analytics or Kafka; Our Culture at Sembcorp; At Sembcorp, our culture is shaped by a strong set of shared behaviours that guide the way we work and uphold our commitment to driving the energy transition.; We foster an institution-first mindset, where the success of Sembcorp takes precedence over individual interests. Collaboration is at the heart of what we do, as we work seamlessly across markets, businesses, and functions to achieve our goals together. Accountability is a core principle, ensuring that we take ownership of our commitments and deliver on them with integrity and excellence. These values define who we are and create a workplace where our people can thrive while making a meaningful impact on driving energy transition.; Join us in making a real impact!",,Full time,singapore
85006790,Data Engineer-Consultant,WSH Experts Pte Ltd,Singapore,2025-06-17 17:57:21,https://id.jobstreet.com/id/job/85006790,"Job responsibilities; Create and manage a single master record for each business entity, ensuring data consistency, accuracy, and reliability.; Implement data governance processes, including data quality management, data profiling, data remediation, and automated data lineage.; Create and maintain multiple robust and high-performance data processing pipelines within Cloud, Private Data Centre, and Hybrid data ecosystems.; Assemble large, complex data sets from a wide variety of data sources.; Collaborate with Data Scientists, Machine Learning Engineers, Business Analysts, and Business users to derive actionable insights and reliable foresights into customer acquisition, operational efficiency, and other key business performance metrics.; Develop, deploy, and maintain multiple microservices, REST APIs, and reporting services.; Design and implement internal processes to automate manual workflows, optimize data delivery, and re-design infrastructure for greater scalability.; Establish expertise in designing, analyzing, and troubleshooting large-scale distributed systems.; Support and work with cross-functional teams in a dynamic environment.; Job Requirements; Experience building and operating large-scale data lakes and data warehouses.; Experience with Hadoop ecosystem and big data tools, including Spark and Kafka.; Experience with Master Data Management (MDM) tools and platforms such as Informatica MDM, Talend Data Catalog, Semarchy xDM, IBM PIM & IKC, or Profisee.; Familiarity with MDM processes such as golden record creation, survivorship,reconciliation, enrichment, and quality.; Experience in data governance, including data quality management, data profiling, data remediation, and automated data lineage.; Experience with stream-processing systems including Spark-Streaming.; Experience working with Cloud services using one or more Cloud providers such as Azure, GCP, or AWS.; Experience with Delta Lake and Databricks.; Advanced working experience with relational SQL and NoSQL databases, including Hive, HBase, and Postgres.; Deep understanding of SQL and the ability to optimize data queries.; If the requirement matches with your profile, kindly share your updated CV/resume to Aparna at aparna@wshexperts.com.sg",,Full time,singapore
84822778,G08 - Data Engineer,FPT Asia Pacific Pte Ltd,Singapore,2025-06-17 17:57:21,https://id.jobstreet.com/id/job/84822778,"We are looking for experienced data engineers to join our team who will be responsible for:; Data Engineering and Platform Integration; Design, develop, and maintain data pipelines and ETL processes using AWS services (Glue, Athena, S3, RDS); Work with data virtualisation tools like Denodo and develop VQL queries; Ingest and process data from various internal and external data sources; Perform data extraction, cleaning, transformation, and loading operations; Implement automated data collection processes including API integrations when necessary; Data Architecture; Design and implement data models (conceptual, logical, and physical) using tools like ER Studio; Develop and maintain data warehouses, data lakes, and operational data stores; Develop and maintain data blueprints; Create data marts and analytical views to support business intelligence needs using Denodo, RDS; Implement master data management practices and data governance standards; Technical Architecture and Integration; Ensure seamless integration between various data systems and applications; Implement data security and compliance requirements; Design scalable solutions for data integration and consolidation; Development and Analytics; Develop Python scripts in AWS Glue for data processing and automation; Write efficient VQL/SQL queries and stored procedures; Design and develop RESTful APIs using modern frameworks and best practices for data services; Work with AWS Sagemaker for machine learning model deployment and integration; Manage and optimise database performance, including indexing, query tuning, and maintenance; Work in an Agile environment and participate in sprint planning, daily stand-ups, and retrospectives; Implement and maintain CI/CD pipelines for automated testing and deployment; Participate in peer code reviews and pair programming sessions; Documentation and Best Practices; Create and maintain technical documentation for data models and systems; Follow industry-standard coding practices, version control, and change management procedures; Stakeholder Collaboration; Partner with cross-functional teams on data engineering initiatives; Gather requirements, conduct technical discussions, implement solutions, and perform testing; Collaborate with Product Managers, Business Analysts, Data Analysts, Solution Architects, UX Designers to build scalable, data-driven products; Provide technical guidance and support for data-related queries; Qualifications and Experience:; At least 3 years of experience in data engineering or similar role; Strong proficiency in Python, VQL, SQL; Experience with AWS services (Glue, Athena, S3, RDS, Sagemaker); Knowledge of data virtualisation concepts and tools (preferably Denodo); Experience with BI tools (preferably Tableau, Power BI); Understanding of data modelling and database design principles; Familiarity with data governance and master data management concepts; Experience with version control systems (Gitlab) and CI/CD pipelines; Experience working in Agile environments with iterative development practices; Strong problem-solving skills and attention to detail; Excellent communication skills and ability to work in a team environment; Knowledge of AI technologies (AWS Bedrock, Azure AI, LLMs) would be advantageous",,Full time,singapore
83923444,"Data Engineer, ITD (1 year contract)",Public Service Division,Singapore,2025-06-17 17:57:21,https://id.jobstreet.com/id/job/83923444,"[What the role is]; About EDB; The Singapore Economic Development Board (EDB), a government agency under the Ministry of Trade and Industry, is responsible for strategies that enhance Singapore’s position as a global centre for business, innovation, and talent. We undertake investment promotion and industry development, and work with international businesses, both foreign and local, by providing information, connection to partners and access to government incentives for their investments. Our mission is to create sustainable economic growth, with vibrant business and good job opportunities for Singapore.; For more information on EDB, please visit www.edb.gov.sg; Why join EDB?; As a Data Engineer, you will be assisting in the maintenance and support of EDB’s data products. You will be working closely with our divisions on their needs to leverage data for strategy formulation, policy implementation and decision making.; The work streams that you could be involved include:; • Data Science: You will work closely with our Data Scientist(s) to procure and prepare the data, and to convert them into operating models for businesses’ day-to-day use; • Business Intelligence: You will assist in the integration and structuring of data from various sources to create interactive, real-time dashboards for decision making.; • Data Architecture and Governance: You will assist in defining scalable data architecture for EDB’s analytics and reporting needs, building data pipelines and related elements of the architecture, and developing the governance to ensure data quality.; [What you will be working on]; Your roles and responsibilities would include the following:; Support the daily operations of the data platform (e.g. handling error notifications, job monitoring & recovery, testing data pipelines);; Investigate and troubleshoot reported incidents related to technical setup (e.g. connectivity failure, API timeout, service failure);; Document changes to existing setup (e.g. data sources, pipelines, accounts);; Research, propose and document technical requirements;; Review and implement fixes for reported security vulnerabilities;; Collaborate with the infrastructure team on the data team requests;; Support product managers in cloud transformation journey for data products;; Provide technical support and consultancy to business users;; Develop reports to monitor usage, performance and security events;; Generate mockup data and create unit tests;; Set up DevOps pipelines;; Optimise performance of data pipelines;; Review data pipelines to ensure adherence to data management standards, policies and procedures.; JOB REQUIREMENTS; To meet the challenges of this role, you must have/ be:; Minimum of Bachelor’s Degree in Computer Science, Computer Engineering, or related disciplines;; 5-8 years of work experience in Data Engineering; AWS experience in implementing or operating a data management solution for analytics; SQL scripting experience to analyze, transform and integrate data sources;; Proficient in building a data pipeline using Python and pySpark; Ability to work effectively under time constraints and potentially changing priorities, while maintaining a high level of attention to details;; Ability to work independently; and; Singaporean.; ; Good to have:; Experience in developing dashboards;; Experience IT infrastructure (server, database, network administration experience is an advantage);; Experience or certification in Tableau Server administration;; Experience or certification in Talend administration;; Experience or certification in MS SQL Server administration;; Proficient in PowerShell;; Experience in SHIP-HATS or DevOps tools;; Proficient in Terraform or IaC;; Experience in dashboard UX design;; Experience with agile or other rapid application development methodologies;; [What we are looking for]",,Full time,singapore
84701300,G07 - Data Engineer,FPT Asia Pacific Pte Ltd,Singapore,2025-06-17 17:57:21,https://id.jobstreet.com/id/job/84701300,"What you will be working on:; Architect and build ingestion pipelines to collect, clean, merge, and harmonize data from different source systems.; Translate data requirements from business users into technical specifications.; Work with Agency's project teams to develop user stories, functional/technical specifications and acceptance criteria; Design and build secure mechanisms for end users and systems to access data in data warehouse.; Research, propose and develop new technologies and processes to improve agency data infrastructure.; Implement data quality checks and validation processes to ensure data accuracy and consistency; Review data pipelines to ensure adherence to data management standards, policies and procedures.; Analyse impact of requested changes and propose improvements to continuously address changing business needs, and work with support team to understand and address technical problems (Operations & Maintenance phase); What we are looking for; Diploma/Degree in Computer Science, Infocomm Technology, Computer or Electronics Engineering or related subject area with minimum 3 years of relevant experience.; Data manipulation using scripting languages like Python or using ETL tools Visual analytics technologies like Tableau, Power BI End-to-end analytics architecture, preferably with some working knowledge of big data stack; Experience with SQL/NoSQL databases; Experience in AWS Cloud especially Glue and RedShift; Experience working with CI/CD setups; Strong knowledge of algorithms and database structures; Strong knowledge of database integration and migration strategy; Strong knowledge in designing and implementing scalable data infrastructure",,Full time,singapore
85156590,Senior Data Engineer (Networks),Singapore Telecommunications Limited,Singapore,2025-06-18 17:57:21,https://id.jobstreet.com/id/job/85156590,"At Singtel, we believe in the strength of a vibrant, diverse and inclusive workforce where backgrounds, perspectives and life experiences of our people help us innovate and create strong connections with our customers. We strive to ensure all our people practices are non-discriminatory and provide a fair, performance-based work culture that is diverse, inclusive and collaborative. ; Join us and experience what it’s like to be with an Employer of Choice*. Together, let’s create a brighter digital future for all. *Awarded at the HR Fest Awards 2020.; Make an Impact by:; Develop new and improve existing data pipeline on Big Data platform (Hadoop, mapR or equivalent).; Build new and enhance existing data application for streaming and batch datasets.; Work on Apache Airflow on data pipeline, Python, Spark, PySpark, Scala, Java, SQL, etc for data application open sources technologies.; Responsible in developing frontend dashboards and integration to back end. Using frontend and backend API Frameworks such as React, Angular Django, FastAPI, Springboot.; Perform R&D and conduct PoC (proof-of-concept) for new data solution.; Perform metadata and data management, data security.; Drive optimization, testing and tooling to improve data quality & efficiency in data lake and streaming platform.; Lead and monitor the performance of Junior Data Engineer and providing them with practical guidance, solution validation and implementation.; Lead and manage DevOps, DataOps and Streaming Operations (or equivalent) in Data Engineering.; Design high level & detailed design to ensure that the solution delivers to the business needs and align to the data & analytics architecture principles and roadmap.; Collaborate with different stakeholders from business, technical, project management and operation to design and implement the solution.; Ability to lead troubleshooting efforts for complex design and eliminate application issue faced by the project and operation team.; Skills for Success:; Bachelor degree in Computer Science, Math, Data Analytics or related field with at least 5 years of hands-on development experiences.; Knowledge in Big Data Platform such as Hadoop, mapR, Cloudera, HPE, etc.; Hands-on experience in Programming language including Python, PysSpark, SQL, noSQL, kSQL, prefer to have experience in Big Data application and Linux shell scripting.; Experience in Data API exposure (Spring Boot, Flask or equivalent) and web development.; Experience in Open Source like Apache Airflow, Zeppelin Notebook for Data Exploratory Analysis, et; Rewards that Go Beyond:; Hybrid work arrangements; Fulll suite of health and wellness benefits ; Ongoing training and development programs; Internal mobility opportunities;  Are you ready to say hello to BIG Possibilities; Take the leap with Singtel to unlock new opportunities and accelerate your growth. Apply now and start your empowering career!",,Full time,singapore
83482854,Data Engineer,SMRT Corporation Ltd,Singapore,2025-06-17 17:57:21,https://id.jobstreet.com/id/job/83482854,"The duties and responsibilities for Data Engineer, are as listed below. The list is not comprehensive and related duties and responsibilities may be assigned from time to time.; Data Engineering & Processing:; Develop and maintain data pipelines for efficient data ingestion and transformation.; Work with structured and unstructured data to ensure optimal storage and retrieval.; Perform data analysis and report on results.; Database Design & Management:; Design and implement relational and NoSQL database schemas for scalability.; Optimize database performance through indexing, partitioning, and query tuning.; Implement data security and compliance best practices.; API Development & Backend Engineering:; Design and develop APIs for data access and application integration.; Implement authentication, authorization, and API security best practices.; Cloud Infrastructure & Deployment (Supporting Role):; Assist in design Azure cloud architectures; Work with IT infrastructure team to set up cloud infrastructure for application hosting, data storage and processing. ; Collaboration & Best Practices:; Collaborate with internal stakeholders to understand their business needs.; Work with software engineers, data scientist, frontend developer to understand the data requirement and design architecture of the data platform.; Implement CI/CD pipelines for automated testing, deployment and monitoring.; Write testable and maintainable code and documentation to deploy to production.; Engage continuously with end-user for feedback and improvements.",,Full time,singapore
83729756,Senior Data Engineer,WSH Experts Pte Ltd,Singapore,2025-06-17 17:57:21,https://id.jobstreet.com/id/job/83729756,"Job Description; Create and manage a single master record for each business entity, ensuring data consistency, accuracy, and reliability.; Implement data governance processes, including data quality management, data profiling, data remediation, and automated data lineage.; Create and maintain multiple robust and high-performance data processing pipelines within Cloud, Private Data Centre, and Hybrid data ecosystems.; Assemble large, complex data sets from a wide variety of data sources.; Collaborate with Data Scientists, Machine Learning Engineers, Business Analysts, and Business users to derive actionable insights and reliable foresights into customer acquisition, operational efficiency, and other key business performance metrics.; Develop, deploy, and maintain multiple microservices, REST APIs, and reporting services.; Design and implement internal processes to automate manual workflows, optimize data delivery, and re-design infrastructure for greater scalability.; Establish expertise in designing, analyzing, and troubleshooting large-scale distributed systems.; Support and work with cross-functional teams in a dynamic environment.; Job Requirement; Experience building and operating large-scale data lakes and data warehouses.; Experience with Hadoop ecosystem and big data tools, including Spark and Kafka.; Experience with Master Data Management (MDM) tools and platforms such as Informatica MDM, Talend Data Catalog, Semarchy xDM, IBM PIM & IKC, or Profisee.; Familiarity with MDM processes such as golden record creation, survivorship,reconciliation, enrichment, and quality.; Experience in data governance, including data quality management, data profiling, data remediation, and automated data lineage.; Experience with stream-processing systems including Spark-Streaming.; Experience working with Cloud services using one or more Cloud providers such as Azure, GCP, or AWS.; Experience with Delta Lake and Databricks.; Advanced working experience with relational SQL and NoSQL databases, including Hive, HBase, and Postgres.; Deep understanding of SQL and the ability to optimize data queries.",,Full time,singapore
85054557,Data Engineer,Assurity Trusted Solutions Pte Ltd,Singapore,2025-06-17 17:57:21,https://id.jobstreet.com/id/job/85054557,"Assurity Trusted Solutions (ATS) is a wholly owned subsidiary of the Government Technology Agency (GovTech). As a Trusted Partner over the last decade, ATS offers a comprehensive suite of products and services ranging from infrastructure and operational services, authentication services, governance and assurance services as well as managed processes. In a dynamic digital and cyber landscape, where trust & collaboration are key, ATS continues to drive mutually beneficial business outcomes through collaboration with GovTech, government agencies and commercial partners to mitigate cyber risks and bolster security postures.; We are seeking a Data Engineer to join our dynamic team to develop and strengthen our data ecosystem for the maritime sector. This role is pivotal in bridging the gap between business/regulatory needs and our maritime data ecosystem, so that we enhance Singapore’s position as a leading global maritime hub. The ideal candidate will leverage stakeholder insights and agile methodologies to develop data ecosystem products that will be relevant for a range of stakeholders in our maritime ecosystem, which would include our port operators, international shipping lines, technology companies, among others.; The maritime sector in Singapore faces significant challenges, as well as opportunities. It is critical for MPA to position Singapore to capture benefits from growing maritime trade, while addressing various challenges including our manpower constraints and global pressures for maritime decarbonisation. This position offers a unique opportunity to enhance Singapore’s global positioning as a leading maritime hub by advancing new value proposition that can be unlocked via data for our maritime ecosystem.; Responsibilities:; Design, Develop, Test, Deploy and Maintain data pipelines (ETL) on the Enterprise Data Warehouse and Big Data Platform; Design and Develop the API /Web Services framework for curation of new datasets whether internal or external (Internet), and to interface with other systems (both internal and external); Explore and source new data sets to address emerging business use case needs; Support stakeholder engagement, development, implementation and maintenance of systems for data collection, storage, access, and analytics at scale.; Develop and manage continuous improvement of data architecture and ensure alignment with business requirements, data management and governance policies.; Design, develop, and maintain interactive dashboards that provide insights and data to support business decision-making.; Support the design and definition of the data architecture framework, standards, and principles, including modelling, metadata, privacy, security, reference data and master data; Requirements; 3+ years of related work experience as a Data Engineer; Good grasp of Software Engineering principles such as Requirements Gathering (both functional and non-functional), Modular & Re-usable Design.; Proficient in ETL using programming language /tools such as Python and/or SSIS and/or Informatica Power Centre; Able to develop data applications including integration with ICT systems, build APIs and web applications via .Java and/or Python; Familiarity with MS SQL, PostgreSQL or Oracle is preferred.; Proficient in Data Modelling and Data Mining.; Experience in designing and building scalable database schema for applications.; Understanding of Object-Oriented Design.; Knowledge of or prior work experience on Big Data platforms such as Hadoop or using Spark.; Experience in the cloud environment setup; Excellent organizational, analytical, and problem-solving skills.; Experience collaborating with business and product teams.; Join us and discover a meaningful and exciting career with Assurity Trusted Solutions!; The remuneration package will commensurate with your qualifications and experience. Interested applicants, please click ""Apply Now"".; We thank you for your interest and please note that only shortlisted candidates will be notified.; By submitting your application, you agree that your personal data may be collected, used and disclosed by Assurity Trusted Solutions Pte. Ltd. (ATS), GovTech and their service providers and agents in accordance with ATS’s privacy statement which can be found at: https://www.assurity.sg/privacy.html or such other successor site.; Benefits; A wholly-owned subsidiary of GovTech.; We promote a learning culture and encourage you to grow and learn.",,Full time,singapore
84453257,Data Engineer,LYNX ANALYTICS PTE. LTD.,Singapore,2025-06-17 17:57:21,https://id.jobstreet.com/id/job/84453257,"The Data Engineer works within a cross-functional project team and is responsible for automating and productizing advanced analytics pipelines. S/he will work with a variety of technologies, including Generative AI, and our proprietary big graph analysis framework.; WHAT YOU’LL DO; A Data Engineer’s responsibility is to implement and deploy data analysis pipelines at various clients of Lynx Analytics. This includes participating in:; Understanding deeply the business problem that we are trying to solve by our analytical solutions; Through continuous consultations with employees of our client, discover the client’s existing data sources that are relevant to the problem we try to solve. This includes discussions with client’s IT teams, data owners, future business owners etc.; Working together with the client’s IT teams to define the technical architecture for the analytical solution that we are to deploy; Implement the data ingestion subsystem: This is the system responsible for moving all the necessary data sources to a single location where the actual analysis will happen; Implement the data analysis pipelines; Integrate the results into business UIs developed by Lynx or pre-existing client software systems; SKILLS AND EXPERIENCE; Skills You Should Have:; Python (pytest, pre-commit, venv, etc.); Airflow, dbt, or other orchestration tool; Apache Spark, PostgreSQL, BigQuery, or other SQL/NoSQL engines; Docker, Docker Compose, Terraform or other IaC tools; Linux OS; One of major cloud platforms like GCP, AWS, or Azure; What You Might Also Work With:; CI/CD tools like GitHub Actions; FastAPI or Flask for service endpoints; Network setup (DNS, TLS, SSH, IP routes, etc.); You Might Be a Fit If You:; Are a generalist who thrives in ambiguity and loves figuring things out.; Enjoy breaking down big, vague, and unfamiliar problems into concrete actions.; Value pragmatism over perfection: you look for MVPs, iterative delivery, and quick feedback loops.; View that DevOps isn’t someone else’s job—you’re comfortable setting up pipelines, fixing flaky builds, or tweaking a reverse proxy config.; Are comfortable switching hats quickly—from setting up data pipelines and crawling websites to tuning SQL queries or debugging backend servers.; Are passionate about learning and applying unfamiliar technologies and tools.; Have some prior experience in Generative AI (ideally RAG) and data science / analytics.; Get excited about opportunities to travel and work abroad!; Why You’ll Love It Here:; High ownership, zero micromanagement; Rapid learning opportunities and diverse challenges; Flexible work hours, remote-friendly setup; A collaborative, culture that values real outcomes; Flat organisational hierarchy with high visibility and accessibility to our leaders; P.S. Mention Tech in Asia Jobs when you apply! Helps keep the good stuff coming 😉",,Full time,singapore
85430135,Data Architect – SAS VI,THAKRAL ONE PTE LTD,Singapore,2025-07-03 17:57:21,https://id.jobstreet.com/id/job/85430135,"Role; Data Architect / Data Management Lead – SAS (Fraud & Investigation Focus); Job Description; We are seeking a highly experienced Data Architect / Data Management Lead with strong domain knowledge in fraud analytics, case management, and investigation/intelligence systems. The ideal candidate will lead the design, development, and deployment of data integration and analytics components within SAS environments, with hands-on exposure to SAS Viya, data pipelines, and real-time streaming tools. This role is ideal for someone who thrives at the intersection of data strategy, architecture, and stakeholder collaboration.; Key Responsibilities; Collaborate with business analysts to gather and interpret data-related requirements for fraud/case management/investigation solutions.; Define and implement a comprehensive data architecture, including data sourcing, integration, cleansing, storage, and provisioning strategies.; Translate business requirements into technical work products across the ETL, data engineering, and reporting layers.; Design and lead batch and real-time data ingestion processes from various sources into SAS or big data platforms.; Develop data pipelines, orchestration frameworks, and visualization/reporting structures using tools such as SAS Viya, DS2, and Python.; Lead integration and testing of developed components, and support UAT activities.; Oversee documentation, project tracking, and technical troubleshooting.; Ensure compliance with data governance, metadata, and quality management standards.; Experience and Skills Requirements; 10+ years of relevant experience in Business Analytics, Data Architecture, or Data Warehousing.; Proven end-to-end delivery of at least 5 enterprise projects, preferably involving SAS solutions in fraud, surveillance, or case management domains.; Strong hands-on experience in:; SAS Viya, SAS DS2, SQL Programming; Python, Spark, JSON, XML; Job scheduling, Linux commands, data streaming tools (Kafka/SAS ESP); Postman/SOAPUI, REST APIs, DevOps/DataOps; Working knowledge of:; Data model design, source-to-target mapping; Real-time event processing, metadata management, and orchestration; Background in fraud analytics, investigative systems, or intelligence/case management frameworks is essential.; Familiarity with tools such as SAS Intelligent Decisioning, RTDM, PEGA, or similar is a strong plus.; Experience working in government, public sector, or regulated environments is highly preferred.; Strong communication, stakeholder management, and leadership skills.; Degree in Computer Science, IT, Statistics, or related field.; Number of Vacancies; 1; Philippines",,Full time,singapore
85074630,"Senior Director, Data Platform Mgmt",Singapore Telecommunications Limited,Singapore,2025-06-17 17:57:21,https://id.jobstreet.com/id/job/85074630,"An empowering career at Singtel begins with a Hello. Our purpose, to Empower Every Generation, connects people to the possibilities they need to excel. Every ""hello"" at Singtel opens doors to new initiatives, growth, and BIG possibilities that takes your career to new heights. So, when you say hello to us, you are really empowered to say…“Hello BIG Possibilities”.; Be a Part of Something BIG!  ; Data has become the strategic enabler for digital transformation initiatives undertaken by Singtel. Business units are actively using data and insights to make strategic decisions and drive Go-To-Market use cases.; This role will be responsible to lead and enable business units and corporate functions the right use of data through effective data lifecycle management and efficient use of data through data engineering and innovation. The role will be partner with stakeholders across the organization to understand their business priorities and deliver data use cases across Singtel. This role will also lead, consult and deliver Singtel data architecture, technology strategy, roadmap and products usage. Provide thought leadership on data democratization and data exchange within Singtel and our business partners. Act as trusted advisors to senior management (C levels, MDs and VPs) on the emerging technologies and digital trends that are most relevant to the company's goals and evolving needs.; It comprises of variety of data functions with focus on plan, design, build and run in these areas:; Data Architecture; Data Security; Data Management; Data Integration and Engineering; Data Modeling; Data Warehousing and Reporting; Data Visualization; DataOps (including DataOps); Data Innovations; Consent Management; This role shall also be the main POC with Pune ODC, coordinating resource allocation, delivery strategies, and project management, while nurturing a culture of excellence, collaboration and continuous improvement.; Team Leadership; Project Management; Performance Management; Stakeholder Engagement; Process Improvement; Talent Development; Make An Impact By; Develop, define and govern Data & Platform Management Strategy, Architecture, Platforms, Roadmaps to support business data strategies and initiatives; Drive innovation initiatives using data and analytics; Lead and drive alignment of architecture and technology across Singtel SG; Build up new capabilities for Data & Platform Management to support business initiatives; Drive strategic data-driven transformation program in Singtel group with partnership with Data Governance and Group Data Office where we can promote the right use of data and analytics.; Collaborate and partner with business units to identify opportunities where we can drive data to business value; Proactively advise the senior management team on the emerging data technologies and digital trends that are most relevant to the company's goals and evolving needs; Transform the organization from project-driven to platform-driven data products; Build and retain core data capabilities and skillsets needed to build and run solutions in the areas of Data Management, Data Solutions, Data Engineering and Data Services; Responsible to deliver and manage multiple programs and projects comprise of internal IT professionals and IT service providers to ensure that programs / projects are delivered within the agreed scope, budget and schedule; Manage and govern data operations to ensure compliance to regulatory, compliance and information security requirements including Personal Data Protection Act (PDPA), General Data Protection Regulation (GDPR) and Spam Control Act; Plan and move build and run functions into DevOps practices to drive automation; Represent Singtel SG IT at assigned project and program steering committees.  Present and pitch at relevant senior leadership levels and executive steering committees; Build strong relationship and influence with business leaders, other Singtel SG IT domains and IT service providers to deliver value via data; Partner with key stakeholders in providing right expertise and advisory on data management, data engineering, reporting, analytics and digital marketing; Develop annual budgets, capital expenditure plans, system development plans and monitor performance against plans; Develop multi-year IT investment roadmaps that link to the IT strategy; Build and develop high-performance data teams; Provide strong leadership and guidance to Pune ODC team; Oversee project planning and resource allocation; Support the growth and development of team members through training and career progression opportunities; Skills to Succeed; Bachelor's degree in business management, IT, Computer Science, Computer Engineering or equivalent.; At least 15 years of working experience, preferably in Telco industry in the following areas:; Experience in developing data strategy, architecture and technology roadmap; Leading a data delivery and operations team comprises of project managers, subject matter experts, data architects, solution designers, data engineers and service ops team. Mix of permanent staff, contract staff and developers from IT service providers of not less than 100 headcounts; Managing multiple outsourced vendors in supporting various applications such as Enterprise Data Warehouse, Big Data, Data Integration Platform, Enterprise Reporting Platform, Data Analytics and Visualization Platform and Campaign Solutions to support a wide array of business functions that include sales, products, finance, operations, marketing and analytics; Successfully managed and implemented large-scale solutions in the areas of data warehouse, big data, master data management system, reporting and analytics, marketing campaign; Ability to review and evaluate emerging and latest data technologies for potential adoption into the organization.; In-depth understanding of the following data technologies:; Big data solutions such as Cloudera, AWS, Microsoft, GCP; Data warehouse such as Teradata, Oracle Exadata, Amazon Redshift, Google BiqQuery for Analytics and Microsoft SQL Server; NoSQL/In Memory database such as MongoDB, Couchbase and Cassandra; Business intelligence and analytics solutions such as Tableau, PowerBI, Alteryx, Trifacta and Cognos; Data integration solutions for both batch and real-time; Campaign platform such as Marketing Cloud; A good understanding of the enterprise data model for Telco such as aLDMs, Teradata Communications Logical Data Model (CLDM) and Oracle Communications Data Model.; A good understanding of the data management framework including master data management, data quality management, data security management and metadata management.; Rewards that Go Beyond; • Full suite of health and wellness benefits  ; • Ongoing training and development programs  ; • Internal mobility opportunities; Your Career Growth Starts Here. Apply Now!; We are committed to a safe and healthy environment for our employees & customers and will require all prospective employees to be fully vaccinated.",,Full time,singapore
