{"Job_ID":"85782724","Role":"Informatica & ETL Developer","Company":"NEPTUNEZ SINGAPORE PTE. LTD.","Location":"Singapore","Publish_Time":"2025-07-15 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/85782724","job_desc":"About the role; We are seeking a skilled Informatica & ETL Developer to design, develop, and maintain scalable data integration solutions using Informatica PowerCenter and Informatica Intelligent Cloud Services (IICS). The ideal candidate will have strong experience in data warehousing, performance tuning, and building robust ETL workflows to support enterprise data needs.; Responsibilities:; Design and develop ETL processes using Informatica PowerCenter and Informatica Cloud (IICS).; Create and maintain mappings, mapplets, sessions, and workflows for data extraction, transformation, and loading.; Work with various source and target systems, including Oracle, SQL Server, Snowflake, Salesforce, flat files, and JSON\/XML.; Implement SCD Type-1 and Type-2, incremental loads, and data cleansing strategies.; Design and monitor Task Flows with advanced steps (Decision, Assignment, Notification, Command, File Watch, Jump, etc.).; Develop and optimize lookup (connected\/unconnected\/dynamic) and implement parameterized mappings.; Ensure data quality and integrity across all stages of data movement.; Collaborate with business analysts and stakeholders to understand data requirements.; Perform root cause analysis and fix issues in existing ETL processes.; Create test plans, perform unit testing, and assist in QA\/UAT support.; Provide production support for deployed workflows and resolve incidents.; Required Skills:; 5+ years of ETL Development experience using Informatica PowerCenter and IICS.; Proficiency in SQL, data modeling, and RDBMS concepts.; Hands-on experience with Oracle, Snowflake, Salesforce, and flat file integrations.; Familiarity with data warehousing and BI reporting environments.; Strong understanding of ETL performance optimization techniques.; Knowledge of shell scripting and automation is a plus.; Experience with JIRA, Agile methodologies, and technical documentation.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85838975","Role":"Data Engineer - ETL (MOH ITDG)","Company":"Synapxe","Location":"One North","Publish_Time":"2025-07-17 09:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/85838975","job_desc":"Company description:; ; Synapxe is the national HealthTech agency inspiring tomorrow's health. The nexus of HealthTech, we connect people and systems to power a healthier Singapore.; ; Together with partners, we create intelligent technological solutions to improve the health of millions of people every day, everywhere. Reimagine the future of health together with us at www.synapxe.sg; ; ; Job description:; ; Position Overview Role & Responsibilities; Develop TRUST data strategy:; Work with stakeholders to understand data analytics needs, data structure requirements (both in terms of scalability and accessibility), and translate this into a coherent near to long term data strategy for TRUST; Support translation of data business needs into technical system requirements for MCDR, in terms of collection, storage, batch -time processing, as well as analysis of information from structured and unstructured sources in a scalable, repeatable, and secure manner; Identify opportunities for improvements and optimisation e.g., Implement best practices and performance optimization on Big Data and Cloud to achieve the best data engineering outcomes; Oversee data preparation and data provisioning for TRUST:; Collaborate with data engineers to organise and prepare anonymised datasets in MCDR according to TRUST standards, and then providing the data in accordance with the approved TRUST Data Request. This involves working with the data engineers closely to ensure that the datasets meet the required standards and are made available as per the specific data request guidelines set by TRUST; Oversee implementation of common data model and data quality programme in TRUST and MCDR; Work with data analysts, data scientists, clinicians and other stakeholders to implement common data models to support analytics use cases; Design and implement tools to enhance the data strategy and enable seamless integration with the data, potentially leveraging API calls for efficient integration; Implement data management standards and practices; Requirements; Degree\/master's in computer science, Information Technology, Computer Engineering or equivalent; At least ten (10) years of relevant working experience in Data management \/ Integration \/ Modelling the data warehouse or advanced analytics solutions; Demonstrate good, in-depth knowledge in relevant Extract-Transform-Load (ETL) hardware\/software products, frameworks, and methodologies; Experience in designing and implementing cloud-based data solutions using cloud platforms (e.g., AWS cloud native tools); Databases (e.g., Oracle, MS SQL, MySQL, Teradata); Big data (e.g., Hadoop ecosystem); ETL development using ETL tools (e.g., Informatica, IBM DataStage, Talend); Data repository design (e.g., operational data stores, dimensional data stores, data marts); Experience in interacting with analytics stakeholders (economists, statisticians, clinicians, policy makers) on a business or domain level; Comfortable working independently to carry out data analysis, estimate data quality and sufficiency; Good interpersonal skills, a detail-oriented & flexible person who can work across different areas within the team; The following will be preferred: Some understanding of Singapore Healthcare System and healthcare data governance, management; and\/or familiarity with health informatics; Apply Now; NOTE: It only takes a few minutes to apply for a meaningful career in HealthTech - GO FOR IT!!; #LI-SYNX40","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85807353","Role":"Associate Data Engineer - ETL (Engineering & Ops)","Company":"Synapxe","Location":"One North","Publish_Time":"2025-07-16 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/85807353","job_desc":"Company description:; ; Synapxe is the national HealthTech agency inspiring tomorrow's health. The nexus of HealthTech, we connect people and systems to power a healthier Singapore.; ; Together with partners, we create intelligent technological solutions to improve the health of millions of people every day, everywhere. Reimagine the future of health together with us at www.synapxe.sg; ; ; Job description:; ; Position Overview; This role is for Engineering &Ops team, Level 2 support\/BAU work for KTLO Operations to ensure smooth operation, operational efficiency and enhance user satisfaction.; Role & Responsibilities; Manage and prioritize user queries and production issues for existing applications in Engineering and Operations; Track and resolve production support incidents; Attend user meetings to document and analyze change request requirements or conduct regular workgroup meetings with stakeholders; Perform data profiling and mapping to define data requirements for new projects or change requests; Provide support for production reports, dashboards, and metadata; Collaborate with vendors and developers to design, configure, and test enhancements per Synapxe project methodologies; Translate user requirements into analytics, reporting needs, and ETL rules for new data mart applications and enhancements; Identify and document business attributes and metrics by analyzing existing data and reporting requirements; Conduct technical data mapping for potential data warehouse sources; Execute testing phases (system integration testing, user acceptance testing) before implementation; Provide 24\/7 primary application maintenance support; Assist the Project Manager in assessing technical feasibility for cost evaluations; Requirements; Bachelor's degree in Computer Science, Information Technology, or a related field; At least 4 years of experience in the IT industry, including:; a. Development, implementation, and maintenance of IT systems, preferably in Data Warehousing, ETL rules, data modeling, and BI applications; b. Operations support and business analysis experience.; c. Strong MS-SQL and Oracle Database scriptin; Experience in diagnosing, troubleshooting, and performing root cause analysis; Ability to diagnose and troubleshoot problems with BI reports and ETL processes; Experience with AWS, Data Lake, Databricks, and the healthcare domain is a plus; Able to work independently and as an effective team player with a strong desire to deliver results; Adaptable, meticulous, and possess strong analytical skills; Good communication skills (both written and spoken); Strong team player; Apply Now; NOTE: It only takes a few minutes to apply for a meaningful career in HealthTech - GO FOR IT!!; #LI-SYNX40","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85782530","Role":"Junior Data Engineer ( ETL \/ Python) | Singaporean Only!","Company":"APBA TG Human Resource Pte Ltd","Location":"Central Region","Publish_Time":"2025-07-15 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/85782530","job_desc":"Requirements:; Diploma in Computer Science, AI, Data Analytics, or related disciplines.; 1\u20132 years of relevant experience is an advantage .; Proficiency in Python, R, SQL, and data visualization tools (e.g., Tableau, Qlik) 2.; Familiarity with Windows and UNIX operating systems, networking, and system administration.; Ability to work independently and as part of a team.; Willingness to travel to sites and perform off-site standby duties.; Preferred Skills:; Experience with statistical packages, ETL tools, and scripting languages.; Knowledge of AI model development and implementation.; Class 3 driving license is a plus.;  Apply, please kindly email your updated resume to akshya.raman@tg-hr.com. ; Only shortlisted applicants will be notified. ; APBA TG Human Resource Pte Ltd (14C7275) || Akshya R (R24122440)","salary":"$3,000 \u2013 $3,500 per month (SGD)","work_type":"Kontrak\/Temporer","country":"singapore"}
{"Job_ID":"85550111","Role":"Data Engineer (ETL & Data Integration) \/ $5,500","Company":"APBA TG Human Resource Pte Ltd","Location":"Jurong East","Publish_Time":"2025-07-07 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/85550111","job_desc":"Key Responsibilities:; Design, develop, and maintain scalable ETL pipelines for production-grade datasets.; Partner with business units to model data across multiple domains.; Ensure the timely and reliable delivery of high-quality datasets for analytics, reporting, and application consumption.; Curate large and complex datasets to support data science and BI initiatives.; Implement and maintain data virtualization views for seamless data access.; Collaborate with infrastructure teams on secure, efficient deployment practices.; Monitor and optimize data workflows for performance and reliability.; Ensure compliance with data governance, access control, and security policies.; Automate and schedule data jobs using enterprise-grade schedulers.; Participate in continuous improvement of data architecture and engineering standards.; Technical Skill Set:; ETL & Data Streaming: Talend, Qlik Replicate (Attunity), Apache Kafka, REST APIs, JSON\/XML; Databases & Querying: Proficient in SQL; experience with MS SQL Server and MongoDB; Data Virtualization: Experience with Denodo (VQL, data services, scheduling); Scheduling Tools: Talend Scheduler, Denodo Scheduler, Unix\/Linux cron jobs; Programming & Scripting: SQL, Python, Shell scripting; Data Modeling: Familiarity with ER modeling and dimensional modeling (e.g., Kimball methodology); Infrastructure & Deployment: Linux\/Unix, Kubernetes (on Nutanix), Jenkins CI\/CD, Nexus Repository; Security & Access Control: Knowledge of LDAP, SAML, Azure Entra ID; To Apply, please kindly email your updated resume to weizhe.teoh@tg-hr.com; Regret to inform that only shortlisted candidates will be notified.; CEI: R25127749; EA License: 14C7275","salary":"$4,500 \u2013 $5,500 per month (SGD)","work_type":"Kontrak\/Temporer","country":"singapore"}
{"Job_ID":"85782700","Role":"Data engineer","Company":"UARROW PTE. LTD.","Location":"Singapore","Publish_Time":"2025-07-15 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/85782700","job_desc":"We are looking for a Data engineer, who will be responsible for architecting and implementing very large scale data intelligence solutions around Snowflake Data Warehouse. A solid experience and understanding of architecting, designing and operationalization of large scale data and analytics solutions on Snowflake Cloud Data Warehouse is a must.; Developing ETL pipelines in and out of data warehouse using combination of Python and Snowflakes Snow SQL; Writing SQL queries against Snowflake.; Developing scripts Unix, Python etc. to do Extract, Load and Transform data; Provide production support for Data Warehouse issues such data load problems, transformation translation problems; Translate requirements for BI and Reporting to Database design and reporting design; Understanding data transformation and translation requirements and which tools to leverage to get the job done; Understanding data pipelines and modern ways of automating data pipeline using cloud based; Testing and clearly document implementations, so others can easily understand the requirements, implementation, and test conditions.; Good hands on experience in developing data integration jobs in Talend 7.3 (TMC) , Talend 8.0.; Extensive experience in developing packages using SQL Server Integration Services (SSIS).; Experience in creating Dimensional Model, Logical\/Physical model, and performing forward \/; Reverse Engineering using ERwin data modeller.; Certified Data Vault practitioner from data vault alliance.; Skilled in writing complex SQL queries with joins and sub queries.; Experience in Performance Tuning and Query Optimization.; Good Knowledge on Version control systems and working experience on tools like TFS for SSIS; and GIT for Talend.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"75168122","Role":"Senior\/ Data Engineer - DSC\/EZ","Company":"ST Engineering Mission Software & Services Pte Ltd","Location":"North-East Region","Publish_Time":"2025-07-06 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/75168122","job_desc":"About our Line of Business \u2013 Mission Software & Services; Our Mission Software & Services business provides leading-edge mission critical command, control, and communications (C3) systems with secured IT infrastructure and managed services. We support our client\u2019s innovation journey through design thinking, analytics, and AI-enabled decision support with our full suite of cloud computing solutions. We provide intelligent, actionable insights and sustainable solutions to our valued partners in diverse industries including defence, government, and commercial sectors.; ; Together, We Can Make A Significant Impact; The Data Engineer supports the design, implementation and maintenance of data flow channels and data processing systems that support the collection, storage, batch and real-time processing, and analysis of information in a scalable, repeatable and secure manner. The Data Engineer focuses on defining optimal solutions to data collection, processing and warehousing. He \/ She designs, codes and tests data systems and works on implementing those into the internal infrastructure.; Be Part of Our Success; Work with stakeholders including customers, partners and colleagues on data-related technical issues and support their data infrastructure needs;; Work closely with data scientists to solicit data requirements to support modeling works; Design, develop, document, manage and maintain data models, ETL processes, data warehouse, data management and pipeline solutions for large volume of structured\/unstructured data from disparate sources and with different latencies (e.g. on-demand, batch, real-time, near-real-time);; Define, monitor and report SLAs for data pipelines and data products;; Understand data security and governance standards or requirements to implement solutions that ensure adherence to these standards or meet such requirements;; Drive\/execute data quality assurance practices; and; Support data management solutions pre-sales initiatives, proposal development and provide post-sales support.; Qualities We Value; In-depth technical knowledge in:; Data Modelling;; Data Pipelines;; OLAP;; Data Ingestion & Integration techonlogies;; Query Optimisation; Technical expertise in:; Java, C\/C++, Python, Scala, SQL etc.; Big data technologies e.g. Hadoop, Spark, Hive, HBase etc.; Experience in master data management, data governance, data lifecycle management etc.;; Experience in designing, documenting, implementing and supporting data management solutions;; Experience in using software engineering best practices in development, programming, testing, version control etc.;; Knowledge of data privacy and security assurance;","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85619847","Role":"SQL Developer","Company":"Snaphunt","Location":"Singapore","Publish_Time":"2025-07-10 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/85619847","job_desc":"Company; Snaphunt Pte Ltd; snaphunt.com; Designation; SQL Developer; Date Listed; 12 May 2025; Job Type; Entry Level \/ Junior Executive; Full\/Perm; Job Period; Immediate Start, Permanent; Profession; IT \/ Information Technology; Industry; Food Services \/ F&B; Location Name; Singapore; Allowance \/ Remuneration; $5,900 - 7,850 monthly; Company Profile; Our client has rapidly evolved into a distinguished solution provider in the cybersecurity space and SI integrator in Digital Transformation using Machine Learning & Artificial Intelligence.; Job Description; Fantastic work culture; The Job; Responsibilities:; Design, develop, and implement database solutions, including tables, views, stored procedures, and functions.; Write complex SQL queries to retrieve, manipulate, and analyze data.; Optimize database performance for speed and efficiency.; Ensure data quality, accuracy, and consistency.; Troubleshoot and resolve database issues.; Collaborate with developers, analysts, and other stakeholders to understand data requirements.; Create and maintain database documentation.; Participate in database design and code reviews.; Stay up-to-date with the latest SQL Server features and database technologies.; Develop ETL processes to move data between databases.; Ideal Candidate; Qualifications:; Bachelor's degree in Computer Science, Information Systems, or a related field.; 2-3 years of experience as a SQL Developer.; Strong proficiency in SQL (e.g., T-SQL, PL\/SQL).; Experience with database management systems such as SQL Server, MySQL, or Oracle.; Understanding of database design principles and normalization.; Experience with query optimization and performance tuning.; Knowledge of data warehousing concepts.; Excellent analytical and problem-solving skills.; Strong communication and collaboration skills.; Ability to work independently and as part of a team.; Preferred Skills:; Knowledge of database administration tasks.; Familiarity with cloud-based database services (e.g., Azure SQL Database, Amazon RDS, Snowflake).; Experience with data visualization tools (e.g., Power BI, Tableau).; Knowledge of scripting languages (e.g., Python, PowerShell).; Ref: F4EMNOGBSY; Application Instructions; Apply this role at https:\/\/snaphunt.com\/jobs\/F4EMNOGBSY; Agent Note This position is posted on behalf of a client by a third party agent.; Apply for this position","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85483082","Role":"DATA ENGINEER","Company":"KAVESSAA CONSULTANCY PTE. LTD.","Location":"Singapore","Publish_Time":"2025-07-04 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/85483082","job_desc":"Key Responsibilities:; 1. ETL\/ELT Development & Data Integration:; \u2022Design, develop, and implement ETL\/ELT pipelines using Informatica IDMC, PowerCenter, IICS (Informatica Intelligent Cloud Services).; \u2022Integrate data from multiple sources, including databases, cloud storage, APIs, and streaming services.; \u2022Optimize data transformations, mappings, workflows, and parameterized sessions.; \u2022Implement real-time and batch data processing solutions.; 2. Cloud & On-Premises Data Integration:; \u2022Work with IDMC (Informatica Cloud) to integrate data across AWS, And Databricks.; \u2022Design data pipelines for hybrid cloud environments (on-prem to cloud migrations).; \u2022Utilize Cloud Data Integration (CDI), Cloud Application Integration (CAI), and Data Quality (CDQ) modules.; \u2022Leverage Informatica Data Governance, Metadata Management, and Axon Data Governance.; 3. Performance Optimization & Troubleshooting:; \u2022Monitor and optimize ETL\/ELT jobs for performance, scalability, and efficiency.; \u2022Troubleshoot data integration failures, bottlenecks, and error handling.; \u2022Optimize SQL queries, session parameters, and pushdown optimization.; 4. Data Governance & Security:; \u2022Implement data quality rules, profiling, cleansing, and standardization.; \u2022Manage role-based access control (RBAC), encryption, and audit logs.; 5.Collaboration & Documentation:; \u2022Work with data analysts, business intelligence (BI) teams, and cloud engineers to ensure high-quality data availability.; \u2022Document ETL\/ELT processes, technical specifications, and best practices.; \u2022Provide training and support to data teams on Informatica tools and cloud data integration best practices.; Required Skills & Qualifications:; Technical Skills:; \u2022Strong experience with Informatica IDMC, PowerCenter, IICS (CDI, CAI, CDQ), and Informatica Cloud.; \u2022Expertise in SQL, PL\/SQL, and relational databases (Oracle, SQL Server, PostgreSQL, databricks, Redshift).; \u2022Experience with cloud data services (AWS Glue, Databricks).; \u2022Knowledge of APIs, RESTful services, JSON, XML, and web service integrations.; \u2022Proficiency in performance tuning, pushdown optimization, and parallel processing.; \u2022Familiarity with data governance, data lineage, and metadata management tools.; Soft Skills:; \u2022Strong problem-solving and analytical skills.; \u2022Ability to work independently and collaboratively with cross-functional teams.; \u2022Excellent communication, documentation, and presentation skills.; \u2022Strong attention to detail and ability to manage multiple projects simultaneously.; Qualifications:; \u2022Informatica Certified Professional (ICP) or IDMC certification.; \u2022Experience with Python, Spark, or Shell scripting for automation.; By submitting the application, you have agreed and consented to us collecting, using, retaining, and disclosing your personal date to prospective employers for their consideration.; EA Reg. No: R24122248 |EA License No. 24C2237","salary":"","work_type":"Kontrak\/Temporer","country":"singapore"}
{"Job_ID":"85572857","Role":"Snowflake Data Engineer (6 Months Extendable)","Company":"Robert Half International Pte Ltd","Location":"Central Region","Publish_Time":"2025-07-08 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/85572857","job_desc":"The Company; ; We are seeking an experienced Snowflake Data Engineer to support our client within the Manufacturing industry on a mission-critical project for 6 months with potential of extension. This role is critical in supporting enterprise-wide data integration, transformation, and analytics initiatives by designing robust ETL\/ELT solutions and optimizing data workflows across platforms.; ; ; The Role; Design, develop, and optimize complex SQL-based transformations on the Snowflake platform.; Build and maintain reliable ETL\/ELT pipelines using Snowflake and Fivetran to seamlessly ingest data from diverse sources such as SharePoint and Oracle ERP.; Collaborate closely with analytics teams to create and optimize Power BI data models and reports that empower business decision-making.; Partner with the Data Architect Manager to drive best practices in data architecture, modeling, and governance across the organization.; Lead efforts in ensuring data quality and integrity by implementing validation checks and quickly resolving inconsistencies.; Monitor and optimize pipeline performance and Snowflake warehouse usage to maximize efficiency and control costs.; Provide ongoing technical support and knowledge transfer to internal teams and end users across APAC.; Document data flows, transformation logic, and system configurations clearly to maintain transparency and support ongoing development.; Use GitLab to manage code, collaborate with peers through version control, and automate deployment with CI\/CD pipelines.; Participate in Agile ceremonies: sprint planning, stand-ups, retrospectives, and peer reviews.; Your Profile; Degree in Computer Science, Information Technology, Data Science, Software Engineering, or a related field.; 8+ years' experience in data engineering or enterprise data warehousing; Expert in Snowflake, advanced SQL, data modeling (star\/snowflake schemas), performance tuning, and security.; Proven skills building and managing ETL\/ELT pipelines, preferably using Fivetran.; Hands-on experience with Power BI for data modeling, report building, and DAX.; Comfortable with GitLab for version control, CI\/CD, and code collaboration.; Experience with cloud platforms (AWS, Azure, or GCP) and orchestration tools like dbt, Airflow, or NiFi.; Strong focus on data quality, governance, and lineage best practices.; Excellent problem-solving, communication, and teamwork skills.; ; Apply Today; ; Please send your resume, in WORD format only and quote reference number GO13257376, by clicking the apply button. Please note that only short-listed candidates will be contacted.; ; ; Robert Half International Pte Ltd. Co. Registration no.: 200612189E | EA Licence No.: 07C5595 | Gabriela De Brito Lopes Prestes Oxby EA Registration no.: 1989404; By clicking 'apply', you give your express consent that Robert Half may use your personal information to process your job application and to contact you from time to time for future employment opportunities. For further information on how Robert Half processes your personal information and how to access and correct your information, please read the Robert Half privacy notice: https:\/\/www.roberthalf.com.sg\/privacy-statement. Please do not submit any sensitive personal data to us in your resume (such as government ID numbers, ethnicity, gender, religion, marital status or trade union membership) as we do not collect your sensitive personal data at this time.","salary":"Competitive (SGD)","work_type":"Kontrak\/Temporer","country":"singapore"}
{"Job_ID":"85615625","Role":"Senior Executive (Data Science), Data Analytics Office","Company":"KK Women's & Children's Hospital","Location":"Bukit Timah","Publish_Time":"2025-07-10 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/85615625","job_desc":"We are seeking talented individuals with a robust background and a passion for analytics and generative AI to join our dynamic team.  In this role, you will utilize your expertise to analyze complex healthcare data, develop predictive models, and create generative AI solutions that enhance patient care and organizational efficiency.;  You will be responsible for planning, supporting, and coordinating data and digital strategic projects and initiatives for the hospital, ensuring data quality, operational management, and governance across various stakeholders.  Your role will involve providing vital insights through advanced data analysis techniques, employing appropriate evaluation methodologies to support management decision-making.;  Additionally, you will explore solutions that evaluate, determine, and improve the efficiency of related processes, ultimately enhancing the quality of patient care delivery.  Effective communication and collaboration with internal and external stakeholders will be essential, as you will also support the department with administrative tasks as needed.;  Requirements:; Recognized Degree (STEM) with 4 years\u2019 relevant work experience preferred.; Experienced in data programming languages such as Python, R programming and SQL(eg libraries such as TensorFlow, PyTorch, or Keras); Proficient in Microsoft office applications especially Microsoft Excel skills (pivot tables, macros) and visual basic is essential.; Hands on analytical experience in data mining\/simulation modelling, operations research & quantitative analysis such as forecasting and predictive analysis is essential.; Prior experience in ETL (Extract, Transform, Load) and unstructured data management including dashboard development experience.; Experience with data science techniques such as text analytics, data mining, clustering, and machine learning .; Strong analytical and critical thinking abilities to tackle complex problems and derive actionable insights.; Generative Models: Familiarity with generative adversarial networks (GANs), variational autoencoders (VAEs), and other generative modeling techniques including deployment preferred.; Collaboration and Communication: Ability to work effectively in teams and communicate technical concepts to non-technical stakeholders.; A team player with a positive mindset, meticulous and resourceful; Able to work independently with good time management skills and initiative to keep up with latest advancements.; Experience in the healthcare industry is an advantage.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85795244","Role":"Data Engineer","Company":"SMRT Corporation Ltd","Location":"Bishan","Publish_Time":"2025-07-16 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/85795244","job_desc":"Company description:; ; SMRT Corporation Ltd is a public transport service provider. Our primary business is to manage and operate train services on the North-South Line, East-West Line, the Circle Line, the Thomson-East Coast Line and Bukit Panjang Light Rail Transit. This is complemented by our bus, taxi and private hire vehicle services.; ; Our core values are Respect, Integrity, Safety and Service, and Excellence. We are committed to provide safe, reliable and comfortable service for all our commuters.; ; ; Job description:; ; Job Purpose; The Data Engineer will be part of the team to develop operation & maintenance decision-support tools to enhance train reliability and maintenance efficiency. This position involves designing, developing, and maintaining data pipelines, APIs, and cloud infrastructure for various rail-oriented applications. The ideal candidate will have expertise in data analysis, transformation, ingestion, database design, API development, and preferably, cloud infrastructure setup. Collaborating closely with software engineers, data scientists, and frontend developers, the Data Engineer will contribute to building efficient, scalable, and reliable systems.; ; Responsibilities; The duties and responsibilities for Data Engineer, are as listed below. The list is not comprehensive and related duties and responsibilities may be assigned from time to time.; Data Engineering & Processing:; Develop and maintain data pipelines for efficient data ingestion and transformation.; Work with structured and unstructured data to ensure optimal storage and retrieval.; Perform data analysis and report on results.; Database Design & Management:; Design and implement relational and NoSQL database schemas for scalability.; Optimize database performance through indexing, partitioning, and query tuning.; Implement data security and compliance best practices.; API Development & Backend Engineering:; Design and develop APIs for data access and application integration.; Implement authentication, authorization, and API security best practices.; Cloud Infrastructure & Deployment (Supporting Role):; Assist in design Azure cloud architectures; Work with IT infrastructure team to set up cloud infrastructure for application hosting, data storage and processing.; Collaboration & Best Practices:; Collaborate with internal stakeholders to understand their business needs.; Work with software engineers, data scientist, frontend developer to understand the data requirement and design architecture of the data platform.; Implement CI\/CD pipelines for automated testing, deployment and monitoring.; Write testable and maintainable code and documentation to deploy to production.; Engage continuously with end-user for feedback and improvements.; Qualifications & Work Experience; Degree in Science, Technology, Engineering or Mathematics (STEM); Previous experience as a data engineer or in a similar role; Data engineering certification is a plus; Knowledge of security best practices in cloud and database management is a plus; Skills; Technical skills include:; Programming and Data processing: MATLAB, Python, SQL, or similar languages.; Databases: My SQL, SQL Server, MongoDB or similar.; Cloud Platforms: Azure; DevOps & CI\/CD: Git Lab CI\/CD, Docker; Generic skills include:; Strong inclination and eager for continual learning and development; Strong team player; Critical thinking and problem-solving skills; Ability to understand and explain complex data and effective interactions with the stakeholders; Ability to think independently and actively propose solutions to the team.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85784716","Role":"Data Engineer","Company":"Helius Technologies Pte Ltd","Location":"Central Region","Publish_Time":"2025-07-15 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/85784716","job_desc":"\ud83d\udd0d Key Responsibilities:; 6+ Years in Design, develop, and maintain end-to-end data lakes and data warehouse solutions; Work with Microsoft Fabric, Azure Data Factory (ADF), or Informatica IDMC for data integration and orchestration; Build scalable data pipelines using Python\/PySpark; Develop and maintain data models for both transactional and analytical systems; Tune and optimize complex queries and analyze performance for large-scale datasets; Collaborate in master data management (MDM) initiatives, preferably in customer domains; (Optional) Contribute to Big Data and DataSecOps initiatives for enterprise-grade solutions; \u2705 Key Requirements:; Strong hands-on experience in Azure Data Services, ideally Microsoft Fabric; Proficiency in Python \/ PySpark scripting; Solid background in data modeling, database design, and SQL performance tuning; Experience with large datasets and real-time data environments; Exposure to MDM tools\/processes, particularly in customer data domains; Advantageous to have familiarity with Big Data tools and DataSecOps practices; Thanks, and Best Regards; Karanam Vijaya Kiran; (EA Registration no: R1443178); HP: +65 92333815; Email: vijay@helius-tech.com; Recruitment Manager; Helius Technologies Pte Ltd (EA Licence No: 11C3373)","salary":"$7,500 \u2013 $8,500 per month (SGD)","work_type":"Full time","country":"singapore"}
{"Job_ID":"85252285","Role":"Data Engineer (Data visualization \/ETL \/ Python) | Singaporean Only!","Company":"APBA TG Human Resource Pte Ltd","Location":"Central Region","Publish_Time":"2025-06-30 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/85252285","job_desc":"Requirements:; Diploma in Computer Science, AI, Data Analytics, or related disciplines.; 1\u20132 years of relevant experience is an advantage .; Proficiency in Python, R, SQL, and data visualization tools (e.g., Tableau, Qlik) 2.; Familiarity with Windows and UNIX operating systems, networking, and system administration.; Ability to work independently and as part of a team.; Willingness to travel to sites and perform off-site standby duties.; Preferred Skills:; Experience with statistical packages, ETL tools, and scripting languages.; Knowledge of AI model development and implementation.; Class 3 driving license is a plus.;  Apply, please kindly email your updated resume to akshya.raman@tg-hr.com. ; Only shortlisted applicants will be notified. ; APBA TG Human Resource Pte Ltd (14C7275) || Akshya R (R24122440)","salary":"$3,000 \u2013 $4,000 per month (SGD)","work_type":"Kontrak\/Temporer","country":"singapore"}
{"Job_ID":"85150830","Role":"Data Engineer","Company":"NEO GARDEN CATERING PTE. LTD.","Location":"Boon Lay","Publish_Time":"2025-06-25 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/85150830","job_desc":"We\u2019re looking for a hands-on Data Engineer to lead the transformation of our data landscape. In this role, you\u2019ll drive the migration to Microsoft Fabric, improve data quality in our ERP system, and build robust data architecture to support advanced analytics and future AI initiatives. You\u2019ll work closely with cross-functional teams to enhance reporting, optimize sales operations, and shape our data-driven strategy.; ; Role Summary; Serve as primary data professional responsible for transforming existing data landscape into robust, scalable architecture; Address critical data quality challenges within current ERP system; Lead Microsoft Fabric migration initiative and establish foundation for AI applications; Build comprehensive data solutions while maintaining operational flexibility for sales optimization; Key Responsibilities; Data Infrastructure and Quality Management; Lead data quality remediation for account and items data within ERP system; Develop comprehensive data validation processes and governance standards; Design and implement robust ETL processes for in-house ERP system integration; Establish data integrity frameworks while preserving operational flexibility required for sales growth; Microsoft Fabric Migration and Architecture; Spearhead migration of company data to Microsoft Fabric platform; Design target architecture consolidating data from mixed POS systems and BCMS; Establish unified data models supporting current reporting and future AI applications; Advanced Analytics and AI Foundation; Prepare data infrastructure for AI solutions; Integrate user behavioral data from web analytics tools; Create customer segmentation and personalization data frameworks; Business Intelligence and Reporting Enhancement; Expand existing Power BI dashboard capabilities for comprehensive business insights; Work directly with stakeholders understanding reporting requirements across organizational levels; Establish data-driven decision-making frameworks; Required Qualifications; Educational Background; Bachelor's degree in Computer Science, Information Systems, Data Science, or related technical field; 3\u20135 years progressive experience in data engineering roles; Background working in small to medium enterprise environments with diverse responsibilities; Technical Expertise; Advanced proficiency in Microsoft ecosystem technologies including SQL Server, Azure data services, and Power BI; Preparation knowledge for Microsoft Fabric implementation and migration; Expertise in data modeling, ETL development, and database optimization; Experience with data quality assessment and remediation methodologies; Proficiency in Python or R for data processing and analysis; Experience integrating diverse data sources including ERP systems and web analytics platforms; Essential Competencies; Strategic and Technical Skills; Ability to design data architecture supporting immediate operational needs and long-term strategic objectives; Strong analytical thinking with proven problem-solving capabilities in complex data environments; Experience leading data transformation projects and managing competing priorities; Communication and Leadership; Excellent communication skills translating technical concepts into business value propositions; Ability to collaborate effectively with non-technical stakeholders across departments; Experience presenting data insights to management and training team members on data tools; Confidence making technical recommendations shaping organizational data strategy; Independent Work and Project Management; Strong project management capabilities with ability to prioritize multiple initiatives simultaneously; Proven ability to work autonomously while making sound technical decisions; Experience establishing data standards and implementing solutions independently; Track record of successful data transformation project leadership; Application Requirements; Resume demonstrating relevant data engineering experience and technical qualifications; Cover letter detailing experience with data quality remediation projects and Microsoft ecosystem implementations","salary":"$4,000 \u2013 $6,000 per month (SGD)","work_type":"Full time","country":"singapore"}
{"Job_ID":"84393093","Role":"Senior Data Engineer","Company":"Avatar Techno Service","Location":"Singapore","Publish_Time":"2025-06-17 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/84393093","job_desc":"Company; Avatar Techno Service; avatartechno.com; Designation; Senior Data Engineer; Date Listed; 21 May 2025; Job Type; Experienced \/ Senior Executive; Full\/Perm; Job Period; Immediate Start, Permanent; Profession; IT \/ Information Technology; Industry; Computer and IT; Location Name; Singapore; Allowance \/ Remuneration; $8,000 monthly; Company Profile; Avatar Techno Services is a global IT Solutions and Services Company established in 2019 with its corporate headquarters in Singapore. We continue to expand our global network while providing value-added cost-effective consulting services to our clients. By 2020 Avatar Techno Services became a leading provider of client-focused IT services and started focusing on IT solutions.; Our services are designed to drive innovation and expansion into new marketplaces while reducing overall costs.; In today\u2019s competitive global market, businesses need technology partners who can understand business strategy and deliver seamless solutions with emerging technologies.; Our team is committed in providing all IT activities right from outsourcing solutions, Infra-structure setup, security consultancy, maintenance, support, project management, software development, testing and much more, which can be tailored on a case-to-case basis.; Our business is committed to offer a resource pool of highly skilled, industry-savvy professionals with a wide range of experience to meet the client\u2019s project requirement or as permanent addition.; Job Description; - Builds and maintains data pipelines.; - Works related to extracting transactional data, transforming for enrichment\/cleansing, and loading into data warehouses or reconciliation tools.; - Works closely with TLM (Transaction Lifecycle Management) teams for accurate and timely data integration.; Key Responsibilities:; ETL Pipeline Design and Development:; Design, develop, and maintain robust ETL processes and pipelines to extract data from diverse banking systems (e.g., CRM, transaction systems, loan management systems).; Data Transformation and Validation:; Implement data cleansing, transformation, and validation steps to ensure data quality and consistency.; Data Warehouse Management:; Work with data warehouse load and manage transformed data, optimizing performance and scalability.; Design and implement data models to support business reporting and analytics needs.; Integration with Existing Systems:; Collaborate with other teams to integrate new ETL processes with existing banking systems.; Data Security and Governance:; Implement data security measures and ensure compliance with regulatory requirements related to data protection and privacy.; Monitoring and Optimization:; Monitor ETL pipeline performance, identify bottlenecks, and optimize processes for efficiency and scalability.; Troubleshooting and Support:; Provide technical support for ETL processes, troubleshoot issues, and resolve data-related problems.; Documentation:; Maintain comprehensive documentation of ETL processes, data models, and data flows.; Collaboration:; Collaborate with other data engineers, data scientists, and business stakeholders to understand requirements and deliver solutions.; Required Skills:; ETL Tools and Technologies: Proficiency in ETL tools (e.g., Talend, Informatica, SSIS), scripting languages (e.g., Python, SQL), and data warehouse technologies; Data Modeling: Understanding of data modeling principles and techniques, including relational and dimensional modeling.; Data Quality and Governance: Knowledge of data quality principles and data governance frameworks.; Database Management: Experience with database systems (e.g., Oracle, SQL Server, MySQL) and SQL.; Programming and Scripting: Proficiency in programming languages like Python or Scala.; Communication and Collaboration: Strong communication and collaboration skills to work effectively with cross-functional teams.; Experience:; Seven years of experience in data engineering, with a focus on ETL processes in a banking or financial services environment.; Experience with data warehousing,; Application Instructions; Please apply for this position by submitting your text CV using InternSG.; Kindly note that only shortlisted candidates will be notified.; Apply for this position","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"84101499","Role":"SQL developer","Company":"Jaish Global Tech Private Limited","Location":"Singapore","Publish_Time":"2025-06-17 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/84101499","job_desc":"Established in 2019, Jaish Global has rapidly evolved into a distinguished solution provider in the cybersecurity space and SI integrator in Digital Transformation using Machine Learning & Artificial Intelligence. Jaish Global Consultancy is an Umbrella company certified by ISO 9001:2015 and ISO 27001. Our unwavering focus on quality and innovative niche technology solutions creates a true edge. Our commitment to excellence has positioned us as a trusted technology-driven organization, safeguarding customer businesses from the ever-evolving landscape of cyber threats and creating business values in customers' enterprise portfolios using our AI tools and services. We never stop pushing the boundaries of what is possible. We are always learning, growing, and re-skilling in Technology solutions and process mapping to stay at the forefront of the industry. Our dynamic culture reflects our drive to be the best. We are providing the services to our customers keeping in view to -> Quick response to change and support for enterprise agility aligned with the business strategy -> Organizational transformation, adopting new trends in business and technology -> Organizational change to support Digital Transformation -> Organizational and operating model changes to improve efficiency and effectiveness -> Lower business operation costs Improved business productivity -> Extending the effective reach of the enterprise (e.g., through digital capability); The Role; Role Overview:; We are seeking a skilled and enthusiastic SQL Developer to join our growing data team. In this role, you will be responsible for designing, developing, and maintaining databases and data solutions. The ideal candidate will have a strong understanding of SQL and database principles, with a proven track record of optimizing database performance and ensuring data integrity.; Responsibilities:; Design, develop, and implement database solutions, including tables, views, stored procedures, and functions.; Write complex SQL queries to retrieve, manipulate, and analyze data.; Optimize database performance for speed and efficiency.; Ensure data quality, accuracy, and consistency.; Troubleshoot and resolve database issues.; Collaborate with developers, analysts, and other stakeholders to understand data requirements.; Create and maintain database documentation.; Participate in database design and code reviews.; Stay up-to-date with the latest SQL Server features and database technologies.; Develop ETL processes to move data between databases.; Ideal Profile; Qualifications:; Bachelor's degree in Computer Science, Information Systems, or a related field.; 2-3 years of experience as a SQL Developer.; Strong proficiency in SQL (e.g., T-SQL, PL\/SQL).; Experience with database management systems such as SQL Server, MySQL, or Oracle.; Understanding of database design principles and normalization.; Experience with query optimization and performance tuning.; Knowledge of data warehousing concepts.; Excellent analytical and problem-solving skills.; Strong communication and collaboration skills.; Ability to work independently and as part of a team.; Preferred Skills:; Knowledge of database administration tasks.; Familiarity with cloud-based database services (e.g., Azure SQL Database, Amazon RDS, Snowflake).; Experience with data visualization tools (e.g., Power BI, Tableau).; Knowledge of scripting languages (e.g., Python, PowerShell).; What's on Offer?; Fantastic work culture","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85436798","Role":"VP \/ AVP, Data Analytics ETL Lead, Information Security Services, Group Technolo","Company":"DBS Bank Limited","Location":"Central Region","Publish_Time":"2025-07-03 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/85436798","job_desc":"DBS is a leading financial services group in Asia, with over 280 branches across 18 markets. Headquartered and listed in Singapore, DBS has a growing presence in the three key Asian axes of growth: Greater China, Southeast Asia and South Asia. The bank's capital position, as well as \"AA-\" and \"Aa1\" credit ratings, is among the highest in Asia-Pacific. DBS has been recognised for its leadership in the region, having been named \u201cAsia\u2019s Best Bank\u201d by The Banker, a member of the Financial Times group, and \u201cBest Bank in Asia-Pacific\u201d by Global Finance. The bank has also been named \u201cSafest Bank in Asia\u201d by Global Finance for seven consecutive years from 2009 to 2015.; Business Function; Group Technology enables and empowers the bank with an efficient, nimble, and resilient infrastructure through a strategic focus on productivity, quality & control, technology, people capability and innovation. In Group Technology, we manage most the Bank's operational processes and inspire to delight our business partners through our multiple banking delivery channels.; We are looking for an experienced data platform ETL lead with coding background to join the Data Protection Program in Information Security Services.; Responsibilities; Platform Lead:; Lead and manage the data security log onboarding process; Lead the data security related log onboarding platform and streamline the current log onboarding process.; Design, develop, and maintain ETL (Extract, Transform, and Load) required to populate the data for in house data analytics models.; Engage with application teams and system owners to validate onboarding plans and requirements.; Coordinate task tracking and prioritization across application teams and data analytics teams to ensure timely execution.; Oversee operational monitoring activities and manage timely escalations to resolve issues.; Maintain clear documentation and regularly report onboarding progress, risks, and outstanding action items.; Lead and facilitate Sprint Planning, Daily Scrums, Sprint Reviews, and Sprint Retrospectives. Ensure these events are time-boxed, productive, and valuable to the team.; Facilitate Communication; Ensure clear and open communication within the team and with stakeholders. Facilitate effective information sharing and transparency.; Conduct ongoing monitoring to support BAU operations, including source availability checks, troubleshooting, incident escalation, and implementation of corrective actions or long-term fixes.; Requirements; Minimum 3 years of experience in building or managing ETL process for enterprise data platform.; Minimum 5 years of coding experience with Java.; Proficiency in Java and the Spring framework.; Good Experience with ETL tools, Mysql\/MariaDB and data warehouse concepts.; Experience with Python, including data analytics libraries such as Pandas, Scikit-learn, and TensorFlow, is highly desirable.; Good understanding of machine learning and data mining techniques, particularly their application in detecting insider threats.; Excellent communication skills, with the ability to present technical concepts clearly and manage stakeholder expectations effectively.; Project management experience, with the ability to coordinate the ETL process with relevant teams and ensure delivery of key milestones within agreed timelines.; Familiarity with DevOps practices, including continuous integration and release automation.; Experience with container technologies, especially platforms managed via OpenShift and Tanzu.; Experience in Detection Engineering and\/or User Entity Behaviour Analytics (UEBA) is a plus.; Apply Now; We offer a competitive salary and benefits package and the professional advantages of a dynamic environment that supports your development and recognises your achievements.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85306740","Role":"Data Engineer","Company":"Singapore LNG Corporation Pte Ltd","Location":"Telok Blangah","Publish_Time":"2025-07-01 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/85306740","job_desc":"Company description:; ; Singapore LNG Corporation Pte Ltd (SLNG) was incorporated by the Energy Market Authority of Singapore in June 2009 to build, own and operate Singapore's very first open-access, multi-user LNG Terminal. This is a key national infrastructure that supports Singapore's energy diversification strategy and future economic development in the energy sector.; ; ; With more than 95% of electricity in Singapore being generated using natural gas, the SLNG Terminal helps to enhance the country's energy security by enabling natural gas to be shipped to Singapore from anywhere in the world. It also serves as a platform to facilitate the development of new LNG-related businesses, thereby contributing to the growth of Singapore's energy industry and the creation of new job opportunities.; ; ; Job description:; ; Roles and Responsibilties; Build and maintain scalable and reliable analytical solutions and dashboards to support commercial operations, throughput monitoring, and performance analysis.; Design, develop, and maintain scalable data pipelines, ETL processes, and data integration frameworks from commercial, operational, and external sources.; Collaborate with Commercial and Operations teams to model, analyse, and forecast terminal usage, cargo schedules, and system capacity.; Support the development of real-time and batch analytics dashboards for terminal throughput, slot utilisation, and performance benchmarking.; Implement and maintain data quality and validation frameworks to ensure high-integrity inputs into business-critical decisions.; Optimise data structures for simulation tools, commercial scenario modelling, and throughput planning systems.; Assist in the development of predictive models and optimisation algorithms in collaboration with data scientists and business analysts.; Keep up with emerging technologies and recommend tools or solutions to improve data analytics capabilities.; Database Management; Administer and maintain Wallix MSSQL databases, including writing complex queries, stored procedures, indexing, and performance tuning.; Support data integration efforts during the terminal expansion project, including real-time and batch data processing.; Automation & Scheduling; Develop scripts and automation workflows (e.g., via SQL Agent, Python, or PowerShell) to streamline data loading and quality checks.; Troubleshoot data issues and optimize data workflows to enhance system performance and reliability.; Data Quality & Governance; Implement processes to validate, monitor, and improve data accuracy, completeness, and consistency across all sources.; Develop and maintain documentation for data flows, architecture, and metadata. Implement data best practices within the team and ensure compliance with data governance and cybersecurity policies.; Work Requirements; 3+ years of experience in a data engineering or data-intensive role, preferably in energy, utilities, shipping, or logistics.; Proven experience as a Data Engineer, preferably within the energy, LNG, or industrial sector.; Strong proficiency in SQL, Python, or similar programming languages for data processing.; Experience with data pipeline and workflow management tools.; Familiarity with cloud platforms and data warehousing solutions.; Technical Skills:; Proficient in SQL, Python, and modern ETL tools (e.g., Airflow, DBT).; Experience with database MSSQL management.; Familiarity with data modelling, API integration, and streaming technologies (Kafka, Spark, etc.) is advantageous.; Experience working with visualisation tools like Power BI, Tableau, or similar.; Familiarity with commercial or operational systems such as ERP, SCADA, TMS, or LNG scheduling tools is a plus.; Experience in machine learning techniques and frameworks (e.g., scikit-learn, TensorFlow, PyTorch) to develop predictive models and advanced analytics is desirable; Soft Skills:; Strong analytical mindset with excellent problem-solving abilities.; Ability to work cross-functionally and communicate effectively with both technical and non-technical stakeholders.; Comfortable in a fast-paced, evolving environment where innovation and initiative are valued.; Understanding of commercial operations and throughput metrics in terminal or energy infrastructure is advantageous.; Strong communication skills to translate technical concepts for non-technical stakeholders.; Education Requirements; Bachelor's degree in Computer Science, Data Engineering, Information Systems, or related discipline.; Why Join SLNG?; An opportunity to contribute to Singapore's energy security and the region's LNG ecosystem.; Work on high-impact projects at the intersection of commercial strategy and operational excellence.; Collaborative and innovative work culture with professional development opportunities.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85690631","Role":"Data Engineer","Company":"PricewaterhouseCoopers","Location":"Singapore","Publish_Time":"2025-07-11 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/85690631","job_desc":"Line of Service; Internal Firm Services; Industry\/Sector; Not Applicable; Specialism; IFS - Information Technology (IT); Management Level; Associate; Job Description & Summary; At PwC, we help clients build trust and reinvent so they can turn complexity into competitive advantage. We\u2019re a tech-forward, people-empowered network with more than 370,000 people in 149 countries. Across audit and assurance, tax and legal, deals and consulting we help clients build, accelerate and sustain momentum. Find out more at www.pwc.com. ; How will you value-add?; Design, develop, and maintain robust ETL\/ELT pipelines using tools like Azure Data Factory, Azure Data Bricks and MS Fabric. ; Build and optimize Data Architectures (Data lakes, Data warehouses) on Azure Data Services, Azure SQL PaaS and Power BI ; Collaborate with data analysts, and business stakeholders to understand data requirements. ; Ensure data quality, integrity, and security across all data systems along with authentication and authorizations for Database. ; Monitor and troubleshoot data pipeline performance and reliability. ; Implement best practices for data governance, metadata management, and documentation. ; Memory management for database systems. ; Develop database schemas, tables and dictionaries.  ; Ensure the data quality and integrity in databases and fix any issues related to database performance and provide corrective measures. ; Work with structured and unstructured data from various sources (APIs, databases, flat files, etc.). ; Perform regular Audit checks ; About you; Bachelor's or master's degree in computer science, Engineering, Information Systems, or related field. ; Good to have Azure Data Engineer certificate   ; 2-3 years of experience in Data Engineering or related roles. ; Proficiency in MS SQL T-SQL and programming languages such as Python or C# .Net. ; Experience with data pipeline orchestration tool Azure Data Factory or Azure Data Bricks. ; Familiarity with cloud data platforms MS Fabric. ; Strong understanding of data modeling and warehousing concepts ; Education (if blank, degree and\/or field of study not specified); Degrees\/Field of Study required:Degrees\/Field of Study preferred:; Certifications (if blank, certifications not specified); Required Skills; Optional Skills; Accepting Feedback, Accepting Feedback, Active Listening, Ansible (Open-Source Tool for Software Provisioning, Configuring, and Deployment), AWS CloudFormation, Azure DevOps Server, Cloud Infrastructure, Communication, Continuous Deployment, Continuous Integration (CI), CrowdStrike, Cybersecurity, Deployment Management, Dynatrace APM, Emotional Regulation, Empathy, GitHub (Version Control Platform), GitLab (DevOps Tool), Google Cloud Platform, Incident Remediation, Inclusion, Infrastructure as a Service (IaaS), Intellectual Curiosity, Jenkins (Software), Jira Software {+ 21 more}; Desired Languages (If blank, desired languages not specified); Travel Requirements; Not Specified; Available for Work Visa Sponsorship?; No; Government Clearance Required?; No; Job Posting End Date","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85530003","Role":"Data Analytics Manager (Data Engineering, Azure, SQL)","Company":"Prestige Professions Pte Ltd","Location":"North-East Region","Publish_Time":"2025-07-07 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/85530003","job_desc":"\u2022 Highest remuneration and bonus package in market; \u2022 Promising career prospects; \u2022 Conducive working environment; Job Scopes:; Responsible for developing robust data pipelines, ensuring data quality, and optimizing data storage and retrieval systems to support our organization's data-driven decision-making processes; Leading a team of Data Analysts (local and remote), the Manager, Data Engineer & Data Analyst; Ensures all company-wide reporting and analytical services are delivered to the business teams; Providing leadership in the areas of Data Strategy and Data Analytics and other emerging areas related to the business; Design, develop, and maintain data pipelines, ensuring a smooth flow of data from multiple sources to data warehouses or lakes; Enable, maintain, and support pipelines to provide unified and reliable data sources; Data architecture design and ETL processes; Scalability and optimization of data infrastructure; Job Requirements:; Degree qualification; 5 years of related experience (hands-on technical); Microsoft Office; Proficiency in Azure cloud environment, SQL, PowerBI, SSIS, ETL will be plus; Strong analytical skills; *** Sincere & Interested applicants, kindly forward your *Updated resume (word doc format) to allan(at)prestigeprofessions.com.sg and CC: Allan (R1223894) ***","salary":"$6,000 \u2013 $7,500 per month (SGD)","work_type":"Full time","country":"singapore"}
{"Job_ID":"85646746","Role":"Data Engineer","Company":"StarHub Ltd","Location":"Singapore","Publish_Time":"2025-07-10 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/85646746","job_desc":"The Customer Lifecycle Management (CLM) team at StarHub is dedicated to understanding, enhancing, and optimizing the customer journey. From acquisition to retention, the CLM team employs data-driven strategies to provide unparalleled customer experiences. Through a combination of data science, business intelligence, customer insights, NPS, and digital analytics, the CLM team ensures that StarHub's offerings are aligned with customer needs, leading to increased loyalty, satisfaction, and growth.; The Data Engineer plays a crucial role in the CLM team by designing, implementing, and maintaining the data infrastructure that supports the team's analytics and data science initiatives. This position is responsible for developing and optimizing data pipelines, ensuring data quality and accessibility, and collaborating with data scientists and analysts to enable efficient data-driven decision-making. The Data Engineer will work on integrating data from various sources, implementing data governance practices, and creating scalable solutions that support the CLM team's objectives in enhancing customer experiences and driving business growth.;   Data Pipeline Development : Design, implement, and maintain efficient ETL (Extract, Transform, Load) processes to integrate data from various sources. Optimize existing data pipelines for improved performance and scalability.; Data Warehouse Management: Develop and maintain the data warehouse architecture, ensuring it meets the needs of the CLM team. Implement data modeling techniques to optimize data storage and retrieval.; Data Quality Assurance: Implement data quality checks and monitoring systems to ensure the accuracy and reliability of data used in analytics and reporting. Develop and maintain data documentation and metadata.; Big Data Technologies: Utilize big data technologies (e.g., Hadoop, Spark) to process and analyze large volumes of customer data efficiently. Implement solutions for real-time data processing when required.; Data Governance: Collaborate with relevant stakeholders to implement data governance policies and procedures. Ensure compliance with data privacy regulations and internal data management standards.; Infrastructure Optimization: Continuously assess and optimize the data infrastructure to improve performance, reduce costs, and enhance scalability. Implement automation solutions to streamline data processes.;   Education Level:; Bachelor's degree in Computer Science, Information Systems, Data Engineering, or a related field. Master's degree in a relevant field is preferred.; Required Experience and Knowledge; 3-5 years of experience in data engineering or a related field.; Strong knowledge of data warehouse concepts, ETL processes, and data modeling techniques.; Experience with cloud-based data platforms (e.g., AWS, SnowFlake).; Proficiency in SQL and experience with NoSQL databases.; Experience with big data technologies such as Hadoop, Spark, or Kafka.; Knowledge of data governance principles and data privacy regulations.; Job-Specific Technical Skills:; Proficiency in Python or Scala for data processing and automation.; Experience with ETL tools (e.g., Apache NiFi, Talend, Informatica).; Knowledge of data visualization tools (e.g., Tableau, PowerBI) to support data quality checks and pipeline monitoring.; Familiarity with version control systems (e.g., Git) and CI\/CD practices.; Experience with container technologies (e.g., Docker) and orchestration tools (e.g., Kubernetes).; Understanding of data security best practices and implementation.; Behavioural Skills:; Strong problem-solving and analytical skills.; Excellent communication abilities to collaborate with technical and non-technical team members.; Proactive approach to identifying and resolving data-related issues.; Ability to manage multiple projects and priorities effectively.; Detail-oriented with a focus on data quality and system reliability.; Adaptability to work with evolving technologies and changing business requirements.; Strong teamwork skills and ability to work in a collaborative environment.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"84360660","Role":"Data Engineer","Company":"Avatar Techno Services","Location":"Singapore","Publish_Time":"2025-06-17 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/84360660","job_desc":"Company; Avatar Techno Services; avatartechno.com; Designation; Data Engineer; Date Listed; 20 May 2025; Job Type; Experienced \/ Senior Executive; Full\/Perm; Job Period; Immediate Start, Permanent; Profession; IT \/ Information Technology; Industry; Computer and IT; Location Name; Singapore; Allowance \/ Remuneration; $8,000 monthly; Company Profile; Avatar Techno Services is a global IT Solutions and Services Company established in 2019 with its corporate headquarters in Singapore. We continue to expand our global network while providing value-added cost-effective consulting services to our clients. By 2020 Avatar Techno Services became a leading provider of client-focused IT services and started focusing on IT solutions.; Our services are designed to drive innovation and expansion into new marketplaces while reducing overall costs.; In today\u2019s competitive global market, businesses need technology partners who can understand business strategy and deliver seamless solutions with emerging technologies.; Our team is committed in providing all IT activities right from outsourcing solutions, Infra-structure setup, security consultancy, maintenance, support, project management, software development, testing and much more, which can be tailored on a case-to-case basis.; Our business is committed to offer a resource pool of highly skilled, industry-savvy professionals with a wide range of experience to meet the client\u2019s project requirement or as permanent addition.; Job Description; Experience 7 + years; Builds and maintains data pipelines.; - Works related to extracting transactional data, transforming for enrichment\/cleansing, and loading into data warehouses or reconciliation tools.; - Works closely with TLM (Transaction Lifecycle Management) teams for accurate and timely data integration; Required Skills:; ETL Tools and Technologies: Proficiency in ETL tools (e.g., Talend, Informatica, SSIS), scripting languages (e.g., Python, SQL), and data warehouse technologies (e.g., Snowflake, Redshift, BigQuery).; Data Modeling: Understanding of data modeling principles and techniques, including relational and dimensional modeling.; Data Quality and Governance: Knowledge of data quality principles and data governance frameworks.; Database Management: Experience with database systems (e.g., Oracle, SQL Server, MySQL) and SQL.; Big Data Technologies: Familiarity with big data technologies such as Hadoop, Spark, Cloud-based platforms.; Programming and Scripting: Proficiency in programming languages like Python or Scala.; Communication and Collaboration: Strong communication and collaboration skills to work effectively with cross-functional teams.; Experience:; Seven years of experience in data engineering, with a focus on ETL processes in a banking or financial services environment.; Experience with data warehousing, data lakes, or big data platforms.; Experience with data modeling and database design.; Experience with ETL tools and technologies.; Experience with big data technologies (e.g., Hadoop, Spark).; Application Instructions; Please apply for this position by submitting your text CV using InternSG.; Kindly note that only shortlisted candidates will be notified.; Apply for this position","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85182562","Role":"Integration Lead [Kafka, MQ and ETL] \/ Singaporean Only!","Company":"APBA TG Human Resource Pte Ltd","Location":"Central Region","Publish_Time":"2025-06-26 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/85182562","job_desc":"Key Responsibilities:; Oversee and manage external interface projects from inception to completion.; Collaborate with internal and external teams to ensure effective communication and integration.; Troubleshoot and resolve interface-related issues promptly.; Develop and implement strategies to optimize interface performance and reliability.;  Qualifications:; Bachelor's degree in Computer Science, Information Technology, or a related field.; Minimum of 5 years of experience in external interface management. ; Proficiency in Apache Kafka for real-time data streaming and integration.; Experience with IBM MQ for message queuing.; Strong understanding of RESTful APIs and SOAP for web services.; Hands-on experience with XML\/JSON data formats.; Expertise in ETL processes for data integration.; Familiarity with middleware technologies and integration platforms.; Ability to work effectively in a fast-paced, dynamic environment.; Strong analytical and troubleshooting skills.; To Apply, please kindly email your updated resume to weizhe.teoh@tg-hr.com; Regret to inform that only shortlisted candidates will be notified.; CEI: R25127749; EA License: 14C7275","salary":"$6,000 \u2013 $7,500 per month (SGD)","work_type":"Kontrak\/Temporer","country":"singapore"}
{"Job_ID":"85034742","Role":"Junior Data Engineer (Python \/ JAVA \/ SQL)","Company":"Vanguard Software Pte Ltd","Location":"Central Region","Publish_Time":"2025-06-19 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/85034742","job_desc":"JOB SUMMARY:; We\u2019re looking for a Data Engineer to join our growing data team. This role is open to fresh graduates and early-career professionals (1\u20132 years of experience) who are excited about building data pipelines, transforming raw data into meaningful insights, and working with modern data technologies. You'll collaborate with data analysts, software engineers, and product teams to ensure that data flows smoothly across our systems and is reliable, secure, and accessible.; Whether you\u2019re just starting out or already have some experience, this is a great opportunity to develop your data engineering skills and contribute to impactful, data-driven decision-making.; ; KEY RESPONSIBILITIES:; Design, develop, and maintain scalable data pipelines and ETL\/ELT workflows; Clean, transform, and optimize raw data for storage and analysis; Work with structured and unstructured data from various sources (databases, APIs, files, etc.); Ensure data quality, accuracy, consistency, and availability; Support data infrastructure (e.g., data lakes, data warehouses) and performance tuning; Collaborate with analysts, data scientists, and backend teams; Document data models, processes, and technical decisions; What You\u2019ll Learn; Real-world data engineering with modern tools like Apache Spark\/Flink, Kafka, Airflow; Working with SQL\/NoSQL databases, data lakes, and cloud platforms (AWS, GCP, Azure); Building batch and streaming data pipelines; Data modeling, warehousing; Orchestration and monitoring of data workflows; Best practices in data governance, privacy, and security; Collaboration in agile, cross-functional teams with product, engineering, and analytics; JOB REQUIREMENTS:; \ud83c\udf93For Fresh Graduates; Bachelor\u2019s degree in Computer Science, Data Engineering, Software Engineering, or a related field (or graduating soon); Understanding of SQL and at least one programming language (Python preferred); Exposure to data concepts through coursework, internships, or projects; Eagerness to work with large datasets and cloud-based data platforms; Willingness to learn new tools and follow team best practices; \ud83d\udcbc For 1\u20132 Years Experience; 1\u20132 years of experience in data engineering, backend development, or analytics engineering; Proficient in SQL and Python; Familiar with ETL tools, data pipeline design, and version control (Git); Experience with cloud services (e.g., S3, Lambda, Cloud Functions, or GCP Dataflow); Able to troubleshoot data issues and build scalable data solutions; Nice to Have (For All Levels); Experience with data orchestration tools (Airflow, Prefect, Dagster, etc.); Familiarity with big data tools (Spark, Kafka, Hadoop); Exposure to data visualization tools (e.g., Looker, Tableau); Understanding of CI\/CD, containerization (Docker), and infrastructure-as-code; Contributions to personal or open-source data projects; Knowledge of data privacy and compliance (GDPR, HIPAA, etc.); Soft Skills; Analytical mindset and strong attention to detail; Team player with good communication skills; Open to feedback and continuous improvement; Responsible and proactive in solving data challenges; Eagerness to explore new tools and share knowledge; What We Offer; Structured onboarding and mentorship to grow your data skills; Opportunities to work on real-world data systems with production impact; A collaborative, knowledge-sharing team culture; Clear growth paths toward analytics engineering, senior data engineering, or data platform roles","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85034742","Role":"Junior Data Engineer (Python \/ JAVA \/ SQL)","Company":"Vanguard Software Pte Ltd","Location":"Central Region","Publish_Time":"2025-06-19 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/85034742","job_desc":"JOB SUMMARY:; We\u2019re looking for a Data Engineer to join our growing data team. This role is open to fresh graduates and early-career professionals (1\u20132 years of experience) who are excited about building data pipelines, transforming raw data into meaningful insights, and working with modern data technologies. You'll collaborate with data analysts, software engineers, and product teams to ensure that data flows smoothly across our systems and is reliable, secure, and accessible.; Whether you\u2019re just starting out or already have some experience, this is a great opportunity to develop your data engineering skills and contribute to impactful, data-driven decision-making.; ; KEY RESPONSIBILITIES:; Design, develop, and maintain scalable data pipelines and ETL\/ELT workflows; Clean, transform, and optimize raw data for storage and analysis; Work with structured and unstructured data from various sources (databases, APIs, files, etc.); Ensure data quality, accuracy, consistency, and availability; Support data infrastructure (e.g., data lakes, data warehouses) and performance tuning; Collaborate with analysts, data scientists, and backend teams; Document data models, processes, and technical decisions; What You\u2019ll Learn; Real-world data engineering with modern tools like Apache Spark\/Flink, Kafka, Airflow; Working with SQL\/NoSQL databases, data lakes, and cloud platforms (AWS, GCP, Azure); Building batch and streaming data pipelines; Data modeling, warehousing; Orchestration and monitoring of data workflows; Best practices in data governance, privacy, and security; Collaboration in agile, cross-functional teams with product, engineering, and analytics; JOB REQUIREMENTS:; \ud83c\udf93For Fresh Graduates; Bachelor\u2019s degree in Computer Science, Data Engineering, Software Engineering, or a related field (or graduating soon); Understanding of SQL and at least one programming language (Python preferred); Exposure to data concepts through coursework, internships, or projects; Eagerness to work with large datasets and cloud-based data platforms; Willingness to learn new tools and follow team best practices; \ud83d\udcbc For 1\u20132 Years Experience; 1\u20132 years of experience in data engineering, backend development, or analytics engineering; Proficient in SQL and Python; Familiar with ETL tools, data pipeline design, and version control (Git); Experience with cloud services (e.g., S3, Lambda, Cloud Functions, or GCP Dataflow); Able to troubleshoot data issues and build scalable data solutions; Nice to Have (For All Levels); Experience with data orchestration tools (Airflow, Prefect, Dagster, etc.); Familiarity with big data tools (Spark, Kafka, Hadoop); Exposure to data visualization tools (e.g., Looker, Tableau); Understanding of CI\/CD, containerization (Docker), and infrastructure-as-code; Contributions to personal or open-source data projects; Knowledge of data privacy and compliance (GDPR, HIPAA, etc.); Soft Skills; Analytical mindset and strong attention to detail; Team player with good communication skills; Open to feedback and continuous improvement; Responsible and proactive in solving data challenges; Eagerness to explore new tools and share knowledge; What We Offer; Structured onboarding and mentorship to grow your data skills; Opportunities to work on real-world data systems with production impact; A collaborative, knowledge-sharing team culture; Clear growth paths toward analytics engineering, senior data engineering, or data platform roles","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85788052","Role":"Data Engineer Project Intern (Data Platform, TikTok) - 2025 Start (BS\/MS)","Company":"TikTok Pte. Ltd.","Location":"Singapore","Publish_Time":"2025-07-16 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/85788052","job_desc":"Responsibilities; About the team The mission of the Data Platform Singapore Business Partnering (DPSG BP) team is to empower the TikTok Business with data. Our goal is to build a Data Warehouse that can cater to batch and streaming data, Data Products that provide useful information to build efficient data metrics & dashboards which will be used to make smarter business decisions to support business growth.; If you're looking for a challenging ground to push your limits, this is the team for you! As a project intern, you will have the opportunity to engage in impactful short-term projects that provide you with a glimpse of professional real-world experience. You will gain practical skills through on-the-job learning in a fast-paced work environment and develop a deeper understanding of your career interests.; Applications will be reviewed on a rolling basis; we encourage you to apply early.; Successful candidates must be able to commit to at least 3 months long internship period. Responsibilities:; Design and implement data warehouse architectures, data modeling, and ETL pipeline development.; Optimize data ETL process and resolve technical issues related to large-scale data ETL.; Participate in data governance under complex data link dependencies and diverse data content ecosystems.; Implement data solutions for business scenarios, leveraging ByteDance's robust mid-tier architecture and product systems.; Qualifications; Minimum Qualifications: - Proficiency in data warehouse implementation methodologies, with in-depth understanding of data warehouse systems and experience supporting business scenarios. - Strong coding skills, with proficiency in multiple tools\/languages including SQL, Python, Java, Hive, Spark, Kafka and Flink.; Preferred Qualifications: - Data-sensitive, meticulous and adept at identifying anomalies in data. - Strong communication skills and excellent ability in integrating technical expertise with business needs.; By submitting an application for this role, you accept and agree to our global applicant privacy policy, which may be accessed here: https:\/\/careers.tiktok.com\/legal\/privacy If you have any questions, please reach out to us at apac-earlycareers@tiktok.com.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"82451779","Role":"Business Intelligence Platform Architect","Company":"ESR Group","Location":"Singapore","Publish_Time":"2025-06-17 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/82451779","job_desc":"Position Title; Business Intelligence (BI) Platform Architect; Reports to; Group Direction, Business Intelligence and Data Integration; Position Summary; We are seeking a detail-oriented and proactive Business Intelligence (BI) Platform Architect with a strong foundation in data management. In this role, you will be responsible for developing robust BI solutions while ensuring high standards of data integrity, governance, and usability. You will work closely with both technical teams and business stakeholders to deliver meaningful insights and maintain a reliable data ecosystem that supports strategic decision-making.; Key Responsibilities include but not limit to:; BI Development & Reporting; Design, develop, and maintain interactive dashboards and reports using tools like Power BI.; Collaborate with stakeholders to gather reporting requirements and translate them into effective BI solutions.; Create and optimize SQL queries and stored procedures to support reporting needs.; Develop and manage data models and visualizations that support business analysis and operational monitoring.; Data Management & Governance; Maintain data quality, integrity, and consistency across multiple systems and sources.; Define and enforce data management standards, including naming conventions, metadata definitions, and data classification.; Collaborate with data owner and IT to manage data cataloging, lineage, and documentation.; Support the implementation of master data management (MDM), data cleansing, and data validation rules.; Participate in data audits and work to resolve data discrepancies and issues.; Data Integration & Architecture; Assist in building and maintaining data pipelines for ingestion and transformation using ETL\/ELT tools.; Support integration of data from cloud and on-premise sources into centralized data platforms (e.g., Fabric, data lakes, data warehouses).; Work closely with IT infra engineers and architects to ensure optimal performance and scalability of data infrastructure.; User Support & Enablement; Provide user training and support on BI tools, reports, and dashboards.; Create user documentation and data dictionaries for self-service BI.; Troubleshoot and resolve BI\/reporting-related issues.; Requirements; Education:; Bachelor\u2019s degree in Computer Science, Information Systems, Data Science, or a related field; 10+ years of experience as a BI Developer, Data Analyst, or similar role.; Strong hands-on experience with BI\/reporting tools (e.g., Power BI).; Proficient in SQL and working with relational databases (e.g., SQL Server).; Solid understanding of data management principles, data governance, and data quality best practices.; Familiarity with data warehousing concepts and ETL processes.; Strong analytical and problem-solving skills with attention to detail.; Excellent communication and stakeholder management abilities.; Experience:; Experience with data governance frameworks and tools (e.g., Collibra, Informatica, Azure Purview).; Knowledge of cloud-based data platforms (Azure, AWS, or GCP).; Understanding of data privacy regulations (e.g., GDPR, HIPAA) and compliance requirements.; Exposure to Business Application: Enterprise Resource Planning (e.g. SAP ECC\/S\/4), Property Management (e.g. Yardi, SAP), Enterprise Performance Management (EPM) solution (e.g. Tagetik); Experience integrating ERP systems (such as SAP, Microsoft Dynamics 365, etc.) with BI tools and platforms.; Proficient in at least two programming languages: Python, Java\/Scala, R, Rest APIs, T-SQL; Experience in the real estate and finance industries is strongly preferred.; Skills:; Strong understanding of BI tools and analytics frameworks, especially in Power BI.; Solid experience with data integration and system interfaces between ERP, EPM systems and BI platforms.; Excellent problem-solving and analytical skills, with a keen eye for detail.; Proficiency in SQL and data querying for reporting and analysis.; Hands on experience to Data platform (Fabric), Data Visualization (MS Power BI), MS Power Platform (Power Automate, Power Apps); Soft Skills:; Strong communication skills, both verbal and written, with the ability to engage with both technical and non-technical stakeholders.; Strong interpersonal skills and ability to work collaboratively with cross-functional teams.; Ability to handle multiple priorities in a fast-paced environment.; Strong organizational and time management skills.; Proactive attitude toward identifying and addressing business challenges","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"84542865","Role":"Senior Data Engineer","Company":"Plaud AI","Location":"Singapore","Publish_Time":"2025-06-17 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/84542865","job_desc":"PLAUD AI is a pioneering AI-native hardware and software company that turns meetings and conversations into actionable insights with AI devices like PLAUD NOTE and PLAUD NotePin. By recording, transcribing, and summarizing real-life conversations, our solutions boost productivity and save time. Designed for precision and flexibility, whether in meetings or on the go, our products empower you to focus on creative, high-value work while AI handles the details.; We are a growing global team of hardware and software experts seeking advanced AI innovations that integrate with real-life user scenarios. Our newly established headquarters in San Francisco will collaborate with our teams in Shenzhen, Beijing, and Tokyo to extend AI benefits to users globally.; Visit https:\/\/www.plaud.ai to learn more.; WHY JOIN US; Join a skyrocketing team where your impact drives success and your career reaches new heights, along with what we have achieved, as shared below.; Global Leadership: Positioned uniquely to lead the future of work by leveraging innovative AI-driven devices and solutions.; Founded in December 2021: Bootstrapped, profitable, and experiencing explosive growth.; 10x Revenue Growth: Achieved 10x revenue growth for two consecutive years, reaching a $100 million run rate, with expectations for even greater expansion in 2025.; Proven Product-Market Fit: Over 700,000 devices shipped globally since November 2023, with users engaging for an average of 30 hours per month to enhance productivity.; New Initiatives: Expanding from consumer-focused products to industry-specific solutions and enterprise-level services.; Loved by Professionals: Our products are trusted by professionals in sectors such as healthcare and sales, where conversations drive success.; WHAT YOU WILL DO; Infrastructure Design and Maintenance: Design, build, and maintain the infrastructure for big data platforms.; ETL Workflow Development: Develop and maintain efficient ETL preprocessing workflows to ensure smooth data extraction, transformation, and loading.; Data Warehouse Design and Optimization: Design, implement, and optimize data warehouses to ensure data is easily accessible and well-organized for analysis.; Data Analysis and Reporting: Conduct detailed data analysis, including data modeling, dashboard development, and data mining, to extract actionable insights for the business.; Optimization and Performance Tuning: Continuously monitor and optimize the performance of data pipelines and systems to ensure data processing is efficient and scalable.; WHAT YOU WILL BRING; Programming Expertise: Proficient in at least one object-oriented programming language, such as Python, Java, or Scala.; Analytical and Problem-Solving Skills: Strong analytical abilities with innovative thinking, capable of independently solving complex technical problems.; Experience: Over 6 years of experience in big data development, including real-time and offline data processing, modeling, ETL development, and data analysis.; Big Data Technologies Knowledge: Deep understanding and practical experience with big data technologies, including Spark, Flink, Hive, and other mainstream big data processing frameworks.; Collaboration and Communication: Strong self-motivation, excellent teamwork spirit, and communication skills. Able to thrive in a fast-paced and dynamic work environment.; Language Proficiency: Proficient in both English and Mandarin for effective work communication in a bilingual environment.; P.S. Mention Tech in Asia Jobs when you apply! Helps keep the good stuff coming \ud83d\ude09","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85695546","Role":"Data Analytics (Tableau Banking, SQL) CBD ~","Company":"PERSOLKELLY Singapore Pte Ltd (Formerly Kelly Services Singapore Pte Ltd)","Location":"Raffles Place","Publish_Time":"2025-07-12 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/85695546","job_desc":"Wealth and Retail Banking Industry;  Hybrid Working arrangements; Duration: 6 months subject to extendable\/convertible; Working Location: Central Business District, CBD; Working hours: 09.00am \u2013 6.00pm (Monday to Friday);  Job Duties; \u00b7         Strong Proficiency in SQL for data extraction, transformation, and analysis.; \u00b7         Expertise with visualization tools like Tableau, PowerBi in building dynamic dashboard, visual analytics and story telling with data; \u00b7         Experience with DataIku, Alteryx or other ETL tools for designing and operationalizing end-to-end data pipelines and workflows; \u00b7         Proficiency in Python for data wrangling, statistical analysis, machine learning and automation of data tasks.; \u00b7         Ability to analyze large datasets, identify trends, outliers, and generate business insights; \u00b7         Strong grasp of statistical techniques; \u00b7         Designing and maintaining data pipelines; \u00b7         Strong communication and storytelling skills for both technical and non-technical stakeholders; \u00b7         Experience working cross-functionally with product, engineering, or business teams; \u00b7         Domain knowledge and experience in Finance industry is a plus;  Requirements; Bachelor\u2019s or Master\u2019s degree in Computer Science, Statistics, Data Science, Engineering, or related field; 8+ years of professional experience in a data science or analytics role; Interested candidates, please click on the following link to begin your job search journey and submit your curriculum vitae (CV) directly through the official PERSOLKELLY job application platform - GO. https:\/\/sg.go.persolkelly.com\/job\/apply\/13140;  Contact number: 8189 1194;  We regret to inform that only shortlisted candidates will be notified.;  By sending us your personal data and CV, you are deemed to consent to PERSOLKELLY Singapore Pte Ltd and its affiliates to collect, use and disclose your personal data for account creation in GO and the purposes set out in the Privacy Policy https:\/\/www.persolkelly.com.sg\/policies. You acknowledge that you have read, understood, and agree with GO\u2019s Terms of Use https:\/\/go.persolkelly.com\/Tac and the Privacy Policy. If you wish to withdraw your consent, please email us at dataprotection@persolkelly.com. Please feel free to contact us if you have any queries.; PERSOLKELLY Singapore Pte Ltd \u2022 UEN No. 200007268E\u2022 EA License No. 01C4394\u2022 Reg. \u2022 R1981246 \u2022 Bertram Lee Kian Hui","salary":"$5,000 \u2013 $6,000 per month (SGD)","work_type":"","country":"singapore"}
{"Job_ID":"85035755","Role":"Middleware Integration Lead [Kafka, MQ and ETL]","Company":"APBA TG Human Resource Pte Ltd","Location":"Central Region","Publish_Time":"2025-06-19 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/85035755","job_desc":"Key Responsibilities:; Oversee and manage external interface projects from inception to completion.; Collaborate with internal and external teams to ensure effective communication and integration.; Troubleshoot and resolve interface-related issues promptly.; Develop and implement strategies to optimize interface performance and reliability.;  Qualifications:; Bachelor's degree in Computer Science, Information Technology, or a related field.; Minimum of 5 years of experience in external interface management. ; Proficiency in Apache Kafka for real-time data streaming and integration.; Experience with IBM MQ for message queuing.; Strong understanding of RESTful APIs and SOAP for web services.; Hands-on experience with XML\/JSON data formats.; Expertise in ETL processes for data integration.; Familiarity with middleware technologies and integration platforms.; Ability to work effectively in a fast-paced, dynamic environment.; Strong analytical and troubleshooting skills.; To Apply, please kindly email your updated resume to weizhe.teoh@tg-hr.com; Regret to inform that only shortlisted candidates will be notified.; CEI: R25127749; EA License: 14C7275","salary":"$6,000 \u2013 $7,500 per month (SGD)","work_type":"Full time","country":"singapore"}
{"Job_ID":"85441115","Role":"VP \/ AVP, Data Analytics ETL Lead, Information Security Services, Group...","Company":"DBS Bank Limited","Location":"Singapore","Publish_Time":"2025-07-03 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/85441115","job_desc":"Business Function; Group Technology enables and empowers the bank with an efficient, nimble, and resilient infrastructure through a strategic focus on productivity, quality & control, technology, people capability and innovation. In Group Technology, we manage most the Bank's operational processes and inspire to delight our business partners through our multiple banking delivery channels.; We are looking for an experienced data platform ETL lead with coding background to join the Data Protection Program in Information Security Services.; Responsibilities; Platform Lead\\:; Lead and manage the data security log onboarding process; Lead the data security related log onboarding platform and streamline the current log onboarding process.; Design, develop, and maintain ETL (Extract, Transform, and Load) required to populate the data for in house data analytics models.; Engage with application teams and system owners to validate onboarding plans and requirements.; Coordinate task tracking and prioritization across application teams and data analytics teams to ensure timely execution.; Oversee operational monitoring activities and manage timely escalations to resolve issues.; Maintain clear documentation and regularly report onboarding progress, risks, and outstanding action items.; Lead and facilitate Sprint Planning, Daily Scrums, Sprint Reviews, and Sprint Retrospectives. Ensure these events are time-boxed, productive, and valuable to the team.; Facilitate Communication; Ensure clear and open communication within the team and with stakeholders. Facilitate effective information sharing and transparency.; Conduct ongoing monitoring to support BAU operations, including source availability checks, troubleshooting, incident escalation, and implementation of corrective actions or long-term fixes.; Requirements; Minimum 3 years of experience in building or managing ETL process for enterprise data platform.; Minimum 5 years of coding experience with Java.; Proficiency in Java and the Spring framework.; Good Experience with ETL tools, Mysql\/MariaDB and data warehouse concepts.; Experience with Python, including data analytics libraries such as Pandas, Scikit-learn, and TensorFlow, is highly desirable.; Good understanding of machine learning and data mining techniques, particularly their application in detecting insider threats.; Excellent communication skills, with the ability to present technical concepts clearly and manage stakeholder expectations effectively.; Project management experience, with the ability to coordinate the ETL process with relevant teams and ensure delivery of key milestones within agreed timelines.; Familiarity with DevOps practices, including continuous integration and release automation.; Experience with container technologies, especially platforms managed via OpenShift and Tanzu.; Experience in Detection Engineering and\/or User Entity Behaviour Analytics (UEBA) is a plus.; Apply Now; We offer a competitive salary and benefits package and the professional advantages of a dynamic environment that supports your development and recognises your achievements.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85782713","Role":"Data Engineer (IDMC - CAI, CDI, XQuery, API Integration)","Company":"NEPTUNEZ SINGAPORE PTE. LTD.","Location":"Singapore","Publish_Time":"2025-07-15 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/85782713","job_desc":"About the role:; We are looking for a Data Engineer with strong expertise in Informatica Intelligent Data Management Cloud (IDMC) to architect, build, and optimize scalable, real-time, and batch data integration pipelines. The role requires deep knowledge of Informatica Cloud Application Integration (CAI) and Cloud Data Integration (CDI) within the IDMC platform.; Responsibilities:; Design and implement real-time and batch data pipelines using IDMC\u2013CAI and CDI modules.; Build and manage complex Advanced Taskflows with parallel paths, branching, and exception handling.; Develop CAI processes using the Process Designer to integrate with REST\/SOAP APIs and web services.; Create and configure Service Connectors, App Connections, and Business Services for secure and reusable integrations.; Leverage XQuery for advanced payload transformation and manipulation of XML\/JSON data.; Implement robust version control, deployment strategies, and reusable design patterns for scalable integration.; Monitor data flows, perform incident management, and conduct root cause analysis for process failures.; Develop technical documentation, support runbooks, and conduct code reviews.; Collaborate cross-functionally with business stakeholders and development teams to define integration requirements.; Mentor junior developers on best practices for CAI and CDI design and development.; Requirements; 5+ years of experience in data integration and engineering, with strong experience in Informatica IDMC.; Proven experience building processes with Cloud Application Integration (CAI) and Cloud Data Integration (CDI).; Expertise in XQuery, API integration, and working with structured and semi-structured data (JSON, XML).; Proficient with Snowflake, Oracle, Salesforce, and AWS S3 as data sources\/targets.; Experience with event-driven workflows, scheduling, and monitoring of production pipelines.; Hands-on with task flow orchestration, parameterized pipelines, and deployment automation.; Strong knowledge of data governance, security protocols, and cloud data architectures.; Excellent troubleshooting and performance tuning skills for integration flows.; Familiarity with DevOps tools for deployment and versioning of IDMC assets.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85680851","Role":"IT Engineer (IT Support | Power Platform | SQL)","Company":"EA RECRUITMENT PTE LTD","Location":"Kaki Bukit","Publish_Time":"2025-07-13 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/85680851","job_desc":"Well Established Company; Basic $3500 - $4500 + Variable Bonus; Excellent Welfare and Benefits + Career progression; Working location: Kaki Bukit; Working days : Monday to Friday; Working hours : 9am to 6pm;  Role Summary; Deliver seamless end-user support while designing, building, and maintaining custom business applications using Microsoft Power Platform and SQL. Drive efficiency by automating workflows, solving operational challenges, and managing software lifecycle processes. Ensure compliance with regulatory standards (e.g., ISO 27001) while balancing reactive support and proactive development.;  Job Responsibilities:; Core IT Support (40%); Provide Tier 1-2 desktop support: Hardware \/ OS troubleshooting (Windows 10 \/ 11), M365 suite, network \/ printer issues, and video conferencing systems.; Manage user lifecycle: Onboarding \/ offboarding, account provisioning (Active Directory \/ Azure AD), access controls, and software deployment.; Maintain IT asset inventory (hardware, peripherals, software licenses) and resolve tickets via ITSM tools (e.g., ServiceNow, Jira).; Coordinate local IT contractors to ensure SLAs and deliverables meet targets.; Diagnose \/ escalate hardware \/ software \/ network issues to internal teams \/ vendors.; Serve as primary IT contact for offices \/ sites: Troubleshoot Wi-Fi, phones, printers, and digital tools.; Conduct employee training on OS, M365 (Teams, SharePoint), and company applications.; Create knowledge base articles and user documentation.;  Solutions Development (60%); Application Development:; Design, build, and deploy custom apps using Power Apps (Canvas + Model-Driven) to digitize forms \/ reports.; Develop SQL databases (queries, stored procedures, ETL) and integrate with Dataverse \/ SharePoint \/ SQL Server.; Automate workflows via Power Automate (approvals, data syncs, notifications).; Project Leadership:; Manage software development lifecycle (SDLC) from ideation to deployment using Agile \/ Scrum.; Direct validation \/ testing methods, bug fixes, and performance monitoring.; Deploy IT \/ Digital \/ Knowledge Management projects and ensure compliance (ISO 27001, DPTM).;  Stakeholder & Vendor Management:; Collaborate with HR \/ Finance \/ Ops to translate business needs into technical solutions.; Source \/ evaluate software for internal \/ customer projects; specify requirements for vendors.; Manage software subscriptions, licensing, and vendor deployments.;  Data & Compliance:; Create Power BI dashboards for operational metrics (ticket trends, asset utilization).; Apply best practices for documentation, security, and regulatory standards.;  Job Requirements:; Minimum Diploma in Information Technology, Computer Science, or related field; Minimum 2 years in end-user support (Windows 10 \/ 11, M365, networking, remote tools like Intune); Possess experience in building apps with Microsoft Power Apps + proficiency in SQL (queries, schema design) in an advantage;   Candidates are encouraged to apply this position via Apply Now button with the following information in the resume;  Work experiences and job responsibilities; Current and Expected salary; Reason for leaving; Date of availability; Education background;  We regret that only shortlisted candidates will be contacted.;  LIONG ZHAO GUAN (R22107632); EA Recruitment Pte Ltd; EA License No: 21C0492","salary":"$3,500 \u2013 $4,500 per month (SGD)","work_type":"Full time","country":"singapore"}
{"Job_ID":"85576102","Role":"Data Engineer","Company":"RSK Group","Location":"Singapore","Publish_Time":"2025-07-08 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/85576102","job_desc":"We are seeking a Data Engineer with experience or interest in IoT technologies and cloud-based data engineering to join our team. This role blends the management of high-volume data flows and IoT-specific analytics with general data engineering practices to deliver robust and scalable data solutions. The ideal candidate will balance technical skills and domain knowledge, enabling data-driven insights for utility operations, customer services, and infrastructure optimisation initiatives.; Key Responsibilities:; Design, develop, and maintain scalable and efficient data pipelines and ETL processes for IoT and enterprise data systems.; Implement data ingestion workflows from IoT devices and integrate with enterprise platforms using Azure Data Factory or similar tools.; Ensure data quality through validation, cleansing, and monitoring processes to address issues such as missing data, duplicates, and inconsistencies.; Define data attributes and formats for IoT device and network data to support seamless integration with existing systems and standards.; Optimise data storage solutions in Azure Data Lake Storage (or equivalent) for structured and unstructured data.; Develop APIs and data interfaces for real-time or near-real-time data transfer between IoT components and enterprise platforms.; Apply advanced analytics techniques to IoT data for performance monitoring, usage profiling, and network management.; Leverage BI tools (e.g., Power BI) to enable business intelligence and operational insights.; Implement robust data security and privacy measures, ensuring compliance with relevant regulations.; Collaborate with cross-functional teams to gather requirements and deliver high-quality, documented solutions.; Required Skills & Qualifications:; Bachelor\u2019s degree in Computer Science, Information Technology, or a related field. Advanced degrees or certifications are advantageous.; Academic or industrial experience in data engineering, including SQL Databases and cloud platforms (Azure, AWS, or GCP).; Experience or interest in IoT technologies and data systems.; Proficiency in data ingestion tools such as Azure Data Factory and ETL processes.; Strong programming skills in Python, SQL, and one or more of Java or Scala.; Familiarity with big data technologies (e.g., Hadoop, Kafka) and enterprise service buses (ESB).; Knowledge of data management systems and integration with enterprise applications.; Understanding of operational data flows, business cycles, and regulatory requirements.; Preferred Skills & Qualifications:; Familiarity with additional Azure ecosystem tools (e.g., Synapse Analytics, Event Hub, Stream Analytics).; Experience with APIs, data interfaces, and integrating IoT systems with enterprise data platforms.; Relevant certifications in Microsoft Azure or other recognised credentials.; Knowledge of DevOps practices and CI\/CD pipelines (e.g., Azure DevOps).; Familiarity with containerisation technologies such as Docker.; Background in implementing Change Data Capture (CDC) designs and scalable data architectures.; Personal Attributes:; Excellent communication and collaboration skills.; Strong analytical and problem-solving mindset.; Proactive approach to continuous professional development.; Ability to work effectively in dynamic, fast-paced environments.; This position offers an opportunity to work at the intersection of IoT data systems and modern cloud engineering, supporting innovation and operational excellence across industries.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85703055","Role":"Data Engineer","Company":"ONE NORTH AI PTE. LTD.","Location":"Singapore","Publish_Time":"2025-07-13 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/85703055","job_desc":"One North, a Singapore based firm specializing in providing Technology Solutions is currently hiring Data Engineers with about 5~10 years of experience especially in Databricks as per details given below.; Job Description & Requirements: -; As Data Engineer, you will support Data Engineering team in setting up the Data Lake on Cloud and the implementation of standardized Data Model, single view of customer.; You will develop data pipelines for new sources, data transformations within the Data Lake , implementing GRAPHQL, work on no sql database, CI\/CD and data delivery as per the business requirements.; Job Description:; Build pipelines to bring in a wide variety of data from multiple sources within the organization as well as from social media and public data sources.; Collaborate with cross functional teams to source data and make it available for downstream consumption.; Work with the team to provide an effective solution design to meet business needs.; Ensure regular communication with key stakeholders, understand any key concerns in how the initiative is being delivered or any risks\/issues that have either not yet been identified or are not being progressed.; Ensure dependencies and challenges (risks) are escalated and managed. Escalate critical issues to the Sponsor and\/or Head of Data Engineering team.; Ensure timelines (milestones, decisions and delivery) are managed and achieved, without compromising quality and within budget.; Ensure an appropriate and coordinated communications plan is in place for initiative execution and delivery, both internal and external.; Ensure final handover of initiative to business-as-usual processes, carry out a post implementation review (as necessary) to ensure initiative objectives have been delivered, and any lessons learnt are included in future processes.; Who we are looking for:; Competencies & Personal Traits:-; Expertise in Databricks; Experience with at least one Cloud Infra provider (Azure\/AWS); Experience in building data pipelines using batch processing with Apache Spark (Spark SQL, Dataframe API) or Hive query language (HQL); Experience in building streaming data pipeline using Apache Spark Structured Streaming or Apache Flink on Kafka & Data Lake; Knowledge of NOSQL databases.; Expertise in Cosmos DB, Restful APIs and GraphQL; Knowledge of Big data ETL processing tools, Data modelling and Data mapping.; Experience with Hive and Hadoop file formats (Avro \/ Parquet \/ ORC); Basic knowledge of scripting (shell \/ bash); Experience of working with multiple data sources including relational databases (SQL Server \/ Oracle \/ DB2 \/ Netezza), NoSQL \/ document databases, flat files; Experience with CI CD tools such as Jenkins, JIRA, Bitbucket, Artifactory, Bamboo and Azure Dev-ops.; Basic understanding of DevOps practices using Git version control; Ability to debug, fine tune and optimize large scale data processing jobs; Excellent problem analysis skills; Working Experience; 7+ years (no upper limit) of experience working with Enterprise IT applications in cloud platform and big data environments.; Professional Qualifications; Certifications related to Data and Analytics would be an added advantage","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85632965","Role":"Data Engineer","Company":"MOURI TECH PTE. LTD.","Location":"North Region","Publish_Time":"2025-07-10 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/85632965","job_desc":"You will be responsible for implementing advanced data transformation techniques, building analytical applications, and applying machine learning and natural language processing (NLP) to support business intelligence and decision-making.; \u2022 Bachelor\u2019s or Master\u2019s degree in Computer Science, Data Engineering, Information Systems, or a related field; \u2022 Proficiency in SQL, Python, or R for data transformation and analysis. Apply data mining, data modeling, machine learning, and NLP techniques on large structured and unstructured datasets.; \u2022 Experience with Databricks and\/or Azure data services (e.g., Azure Data Lake, Synapse, ADF). Design and maintain data workflows and orchestration using platforms like Azure Data Factory, Apache Airflow, or similar.; \u2022 Hands-on experience using dbt for data modeling and transformation. Build scalable ETL\/ELT pipelines using tools such as dbt, Databricks, or similar platforms; \u2022 Experience developing dashboards and reports using Qlik Sense, Power BI, or Tableau; \u2022 Solid understanding of data warehousing, data modeling, and performance optimization; \u2022 Collaborate with cross-functional teams to understand business needs and translate them into technical solutions; \u2022 Strong problem-solving and analytical skills","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85270581","Role":"Data Analytics (Banking, SQL) CBD ~","Company":"PERSOLKELLY Singapore Pte Ltd (Formerly Kelly Services Singapore Pte Ltd)","Location":"Raffles Place","Publish_Time":"2025-06-30 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/85270581","job_desc":"Wealth and Retail Banking Industry;  Hybrid Working arrangements; Duration: 6 months subject to extendable\/convertible; Working Location: Central Business District, CBD; Working hours: 09.00am \u2013 6.00pm (Monday to Friday);  Job Duties; \u00b7         Strong Proficiency in SQL for data extraction, transformation, and analysis.; \u00b7         Expertise with visualization tools like Tableau, PowerBi in building dynamic dashboard, visual analytics and story telling with data; \u00b7         Experience with DataIku, Alteryx or other ETL tools for designing and operationalizing end-to-end data pipelines and workflows; \u00b7         Proficiency in Python for data wrangling, statistical analysis, machine learning and automation of data tasks.; \u00b7         Ability to analyze large datasets, identify trends, outliers, and generate business insights; \u00b7         Strong grasp of statistical techniques; \u00b7         Designing and maintaining data pipelines; \u00b7         Strong communication and storytelling skills for both technical and non-technical stakeholders; \u00b7         Experience working cross-functionally with product, engineering, or business teams; \u00b7         Domain knowledge and experience in Finance industry is a plus;  Requirements; Bachelor\u2019s or Master\u2019s degree in Computer Science, Statistics, Data Science, Engineering, or related field; 8+ years of professional experience in a data science or analytics role; Interested candidates, please click on the following link to begin your job search journey and submit your curriculum vitae (CV) directly through the official PERSOLKELLY job application platform - GO. https:\/\/sg.go.persolkelly.com\/job\/apply\/13140;  Contact number: 8189 1194;  We regret to inform that only shortlisted candidates will be notified.;  By sending us your personal data and CV, you are deemed to consent to PERSOLKELLY Singapore Pte Ltd and its affiliates to collect, use and disclose your personal data for account creation in GO and the purposes set out in the Privacy Policy https:\/\/www.persolkelly.com.sg\/policies. You acknowledge that you have read, understood, and agree with GO\u2019s Terms of Use https:\/\/go.persolkelly.com\/Tac and the Privacy Policy. If you wish to withdraw your consent, please email us at dataprotection@persolkelly.com. Please feel free to contact us if you have any queries.; PERSOLKELLY Singapore Pte Ltd \u2022 UEN No. 200007268E\u2022 EA License No. 01C4394\u2022 Reg. \u2022 R1981246 \u2022 Bertram Lee Kian Hui","salary":"$5,000 \u2013 $6,000 per month (SGD)","work_type":"","country":"singapore"}
{"Job_ID":"84883591","Role":"Data Engineer","Company":"ExpressVPN","Location":"Singapore","Publish_Time":"2025-06-17 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/84883591","job_desc":"The Data Engineer will be responsible for designing, developing, and maintaining robust data pipelines and data warehouse architectures. The ideal candidate will have expertise in tools like AWS Redshift, Athena, Snowflake, and other leading databases, with a strong focus on building scalable and efficient data solutions.; What you\u2019ll do; Design and implement scalable and efficient data pipelines to ingest, process, and store large datasets from multiple sources.; Develop and maintain data warehouse solutions using AWS Redshift, Athena, Snowflake, and other modern data platforms.; Optimize data models and queries to ensure high performance and cost-effectiveness.; Collaborate with data analysts, scientists, and cross-functional teams to understand data requirements and translate them into technical solutions.; Ensure data integrity, security, and privacy throughout the pipeline and storage processes.; Monitor data workflows and troubleshoot issues to ensure reliability and accuracy.; Implement best practices for data governance and ensure compliance with data policies.; Stay updated with the latest trends and advancements in data warehousing technologies.; What you\u2019ll need to succeed; Bachelor\u2019s or Master\u2019s degree in Computer Science, Information Technology, or related field.; 5+ years of experience as a Data Engineer or similar role with a focus on data warehousing and pipeline development.; Strong proficiency with AWS Redshift, Athena, Snowflake, and other top-tier databases.; Experience with data pipeline tools and frameworks (e.g., Apache Airflow, Glue, or similar).; Proficiency in SQL and scripting languages such as Python or Shell.; Familiarity with cloud services (AWS preferred) and infrastructure management.; Experience with data modeling, schema design, and ETL processes.; Excellent problem-solving skills and attention to detail.; Strong communication skills and the ability to work collaboratively across teams.; Nice to Have:; Experience with BI tools like Tableau, Looker, or Power BI.; Knowledge of data security and privacy best practices.; Experience with DevOps tools and practices.; P.S. Mention Tech in Asia Jobs when you apply! Helps keep the good stuff coming \ud83d\ude09","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"84249971","Role":"Data Engineer, Associate\/Senior Associate, Data & AI, Technology Consulting","Company":"Ernst & Young Solutions LLP","Location":"Singapore","Publish_Time":"2025-06-17 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/84249971","job_desc":"At EY, you\u2019ll have the chance to build a career as unique as you are, with the global scale, support, inclusive culture and technology to become the best version of you. And we\u2019re counting on your unique voice and perspective to help EY become even better, too. Join us and build an exceptional experience for yourself, and a better working world for all. ; The opportunity; EY DnA is the data and advanced analytics capability within EY Asia-Pacific, with over 500 specialist employees working across multiple industry sectors.; We implement information-driven strategies, data platforms and advanced data analytics solution systems that help grow, optimize and protect client organizations. We go beyond strategy and provide end to end design, build and implementation of real life data environments and have some of the best architects, project managers, business analysts, data scientists, big data engineers, developers and consultants in the region.; We are looking for a Data Engineer (Associate\/Senior Associate) within the DnA team in our Singapore office. This role is offered on a full time basis.; Your key responsibilities; Design, build, and maintain efficient, reusable, and scalable ETL\/ELT pipelines for structured and unstructured data.; Develop and optimize data workflows to support advanced analytics, machine learning models, and reporting tools.; Collaborate with data scientists, analysts, and business stakeholders to gather requirements and ensure data quality and availability; Work with cloud and on-prem data platforms (e.g., AWS, Azure, GCP, Hadoop, or on-prem SQL\/NoSQL systems).; Ensure data integrity, governance, and security best practices across pipelines and data lakes\/warehouses.; Troubleshoot and resolve data issues and performance bottlenecks in real-time and batch pipelines.; Monitor job performance and implement automation and alerting for data operations.; Contribute to documentation, code reviews, and development best practices.; Skills and attributes for success; 2\u20136 years of experience in data engineering or related roles.; Proficient in SQL, Python, or Scala for data processing.; Experience with data orchestration tools like Apache Airflow, DBT, or Luigi.; Familiarity with cloud platforms (AWS Glue, Azure Data Factory, GCP Dataflow, etc.).; Hands-on experience with data warehouses such as Snowflake, BigQuery, or Redshift.; Understanding of data modeling, normalization, and data warehousing concepts.; Exposure to CI\/CD, version control (Git), and agile development practices.; To qualify for the role, you must have; Bachelor\u2019s or Master\u2019s degree in Computer Science, Engineering, or related field.; Knowledge of streaming data frameworks (Kafka, Spark Streaming, Flink) is a plus.; Experience working in consulting or client-facing environments (for Senior Associate roles).; Certifications in AWS\/Azure\/GCP or specific data tools.; What we look for; Highly motivated individuals with excellent problem-solving skills and the ability to prioritize shifting workloads in a rapidly changing industry.; An effective communicator, you\u2019ll be a confident leader equipped with strong people management skills and a genuine passion to make things happen in a dynamic organization.; What we offer; EY offers a competitive remuneration package commensurate with your work experience where you\u2019ll be rewarded for your individual and team performance. We are committed to being an inclusive employer and are happy to consider flexible working arrangements, where this may be needed, guided by our FWA Policy.; Plus, we offer:; Continuous learning: You\u2019ll develop the mindset and skills to navigate whatever comes next.; Success as defined by you: We\u2019ll provide the tools and flexibility, so you can make a meaningful impact, your way.; Transformative leadership: We\u2019ll give you the insights, coaching and confidence to be the leader the world needs.; Diverse and inclusive culture: You\u2019ll be embraced for who you are and empowered to use your voice to help others find theirs.; If you can demonstrate that you meet the criteria above, please contact us as soon as possible.; The exceptional EY experience. It\u2019s yours to build.; Apply now.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85692981","Role":"Lead Data Engineer - Cloud Migration","Company":"TRIPLE 8 CONSULTING SERVICES PTE. LTD.","Location":"Anson","Publish_Time":"2025-07-11 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/85692981","job_desc":"Roles And Responsibilities:; \u2022 Design and architect data storage solutions, including databases, data lakes, and warehouses, using AWS services such as Amazon S3, Amazon RDS, Amazon Redshift, and Amazon DynamoDB, along with Databricks' Delta Lake. Integrate Informatica IDMC for metadata management and data cataloging.; Create, manage, and optimize data pipelines for ingesting, processing, and transforming data using AWS services like AWS Glue, AWS Data Pipeline, and AWS Lambda, Databricks for advanced data processing, and Informatica IDMC for data integration and quality.; Integrate data from various sources, both internal and external, into AWS and Databricks environments, ensuring data consistency and quality, while leveraging Informatica IDMC for data integration, transformation, and governance.; ; \u2022 Develop ETL (Extract, Transform, Load) processes to cleanse, transform, and enrich data, making it suitable for analytical purposes using Databricks' Spark capabilities and Informatica IDMC for data transformation and quality.; \u2022 Monitor and optimize data processing and query performance in both AWS and Databricks environments, making necessary adjustments to meet performance and scalability requirements. Utilize Informatica IDMC for optimizing data workflows.; \u2022 Implement security best practices and data encryption methods to protect sensitive data in both AWS and Databricks, while ensuring compliance with data privacy regulations. Employ Informatica IDMC for data governance and compliance.; \u2022 Implement automation for routine tasks, such as data ingestion, transformation, and monitoring, using AWS services like AWS Step Functions, AWS Lambda, Databricks Jobs, and Informatica IDMC for workflow automation.; \u2022 Maintain clear and comprehensive documentation of data infrastructure, pipelines, and configurations in both AWS and Databricks environments, with metadata management facilitated by Informatica IDMC.; \u2022 Collaborate with cross-functional teams, including data scientists, analysts, and software engineers, to understand data requirements and deliver appropriate solutions across AWS, Databricks, and Informatica IDMC.; \u2022 Identify and resolve data-related issues and provide support to ensure data availability and integrity in both AWS, Databricks, and Informatica IDMC environments.; \u2022 Optimize AWS, Databricks, and Informatica resource usage to control costs while meeting performance and scalability requirements.; \u2022 Stay up-to-date with AWS, Databricks, Informatica IDMC services, and data engineering best practices to recommend and implement new technologies and techniques.; Requirements \/ Qualifications; \u2022 Bachelor\u2019s or master\u2019s degree in computer science, data engineering, or a related field.; Minimum 10 years of experience in data engineering, with expertise in AWS services, Databricks, and\/or Informatica IDMC.; Proficiency in programming languages such as Python, Java, or Scala for building data pipelines.; ; \u2022 Evaluate potential technical solutions and make recommendations to resolve data issues especially on performance assessment for complex data transformations and long running data processes.; \u2022 Strong knowledge of SQL and NoSQL databases.; \u2022 Familiarity with data modeling and schema design.; \u2022 Excellent problem-solving and analytical skills.; \u2022 Strong communication and collaboration skills.; \u2022 AWS certifications (e.g., AWS Certified Data Analytics - Specialty, AWS Certified Data Analytics - Specialty), Databricks certifications, and Informatica certifications are a plus.; Preferred Skills:; \u2022 Experience with big data technologies like Apache Spark and Hadoop on Databricks.; Knowledge of data governance and data cataloguing tools, especially Informatica IDMC.; Familiarity with data visualization tools like Tableau or Power BI.; ; \u2022 Knowledge of containerization and orchestration tools like Docker and Kubernetes.; \u2022 Understanding of DevOps principles for managing and deploying data pipelines.; \u2022 Experience with version control systems (e.g., Git) and CI\/CD pipelines.","salary":"","work_type":"Kontrak\/Temporer","country":"singapore"}
{"Job_ID":"85584364","Role":"Data Engineer, Healthcare","Company":"Menrva","Location":"Marina South","Publish_Time":"2025-07-08 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/85584364","job_desc":"About the Organization; The organization plays a central role in architecting and implementing cutting-edge digital health solutions \u2014a major preventive care strategy aimed at improving long-term population health.; Why This Role Matters; As a Data Engineer, you will be integral to building and maintaining the data infrastructure that fuels analytics, reporting, and predictive modelling. Your work will directly influence how data is accessed, secured, and leveraged to improve healthcare outcomes.; Key Responsibilities; Design, build, and maintain robust data pipelines for integrating and processing data from diverse sources and formats.; Clean, prepare, and transform data for analytics, business intelligence, and data science use cases.; Ensure comprehensive documentation of data processes and pipeline architecture.; Monitor, troubleshoot, and improve the performance of data systems and pipelines.; Identify optimisation opportunities for scalability, repeatability, and security.; Handle data system errors and contribute to testing configurations for improved efficiency.; Requirements; Must-Have Skills:; Proven experience in designing scalable ETL pipelines to support AI and data science initiatives.; Strong proficiency in SQL, NoSQL, and Python for data preparation, transformation, and automation.; Hands-on experience with data lake management and data pipeline development.; Familiarity with cloud collaboration and development tools (e.g., Office 365, Atlassian, AWS, Azure).; Nice-to-Have Skills:; Exposure to AWS services (e.g., S3, Athena, Lambda, IAM, CloudWatch).; Domain knowledge in health informatics; Experience supporting machine learning, clinical data projects, or hospital information systems.; Familiarity with modern data engineering frameworks like Airflow, Docker, Kubernetes.; Qualifications; Bachelor's degree in Computer Science, Information Technology, Computer Engineering, or a related field.; 3 to 7 years of hands-on experience in data engineering, pipeline development, and production deployment.; Strong scripting abilities, preferably in Python.; Solid SQL skills and experience with platforms like Informatica, Teradata, or SQL Server.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"84359312","Role":"Vice President, Data Engineer, Group Technology","Company":"United Overseas Bank Limited (UOB)","Location":"Singapore","Publish_Time":"2025-06-17 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/84359312","job_desc":"United Overseas Bank Limited (UOB) is a leading bank in Asia with a global network of more than 500 branches and offices in 19 countries and territories in Asia Pacific, Europe and North America. In Asia, we operate through our head office in Singapore and banking subsidiaries in China, Indonesia, Malaysia and Thailand, as well as branches and offices. Our history spans more than 80 years. Over this time, we have been guided by our values \u2013 Honorable, Enterprising, United and Committed. This means we always strive to do what is right, build for the future, work as one team and pursue long-term success. It is how we work, consistently, be it towards the company, our colleagues or our customers.; ; Group Technology and Operations (GTO) provides software and system development, information technology support services and banking operations.; We have centralized and standardized the technology components into Singapore, creating a global footprint which can be utilized for supporting our regional subsidiaries and the branches around the world. We operate and support 19 countries with this architecture to provide a secure and flexible banking infrastructure.; Our Operations divisions provide transactional customer services for our businesses while also focusing on cost efficiency through process improvements, automation and straight through processing.; You will be responsible for the end-to-end software development and support for all work related to Enterprise Data Warehouse (EDW) projects, quarterly change requests, L3 production fixes. This includes software product implementation and administration, application design, development, implementation, testing and support. You will be expected to work on Data team.; ; You will also be responsible for quality assurance of the team\u2019s delivery in conformance with the Bank-defined software delivery methodology and tools. You will partner with other technology functions to help deliver required technology solutions.; Create frameworks, technical features which helps in faster operationalisation of Data models, Analytical models(including AI\/ML) and user generated contents (dashboards, reports etc); Effectively partner with citizen data scientists in enabling faster adoption of AL\/ML model based systems; Independently install, customise and integrate software packages and programs; Carry out POCs involving new data technologies; Design and develop application frameworks for data integration; Create technical documents such as solution design, program specifications for target solutions; Perform design and development of applications which may not be limited to: Software Applications, Data Integration, User Interfaces, Automation Maintain and recommend software improvements to ensure a platform centric management of software applications; Performance tuning; Work with production support team members to conduct root cause analysis of issues, review new and existing code and\/or perform unit testing; Perform tasks as part of a cross functional development team using agile or other methodologies and utilising project management software; Hands-on experience in implementing large scale data warehouse, Data mart & analytics platforms in financial services industry with good functional knowledge of products & services offered in Retail bank \/ Wholesale \/ Global Markets covering some of the following analytics domains:; Experience in Data Modeling, Data mapping for Data Warehouse and Data Marts solutions; Expertise in FSLM or similar industry models; Expertise in Reference data management \u2013 Tools experience such as MDM (Master Data Management); Experience in functional domain - Retail , Wholesale, Compliance , Digital; Experience in analytics - Retail Analytics; Expertise in design of role based fine grained access control; Designing cloud ready data solutions, Virtualization  ; Technical skillsets -ML Model operationalization, Building Data Pipelines and Hadoop based Data marts; Expertise in implementing Big Data frameworks using multiple SQL engine such as Spark, Impala, Hive, etc; Expertise in implementing Hadoop based Data mart using Spark based framework (Java, Scala, Pyspark); Leverage LLM model to build intelligent data applications (eg: Natural Language based SQL generation); Experience in end to end AI \/ ML life cycle (Data pipeline, model training, Model development, deployment, fine tuning, etc); Expertise in building Data federation solution(Trino, Presto, Dremio, Query Grid) along with Data caching \/ Indexing; Experience in working with any of Data Catalog tools and Automating Data Quality checks using frameworks; Good Working experience in core technical area using Python, Java, PySpark and Scala; Expertise in Cloudera CDP components; Good knowledge in developing Spark based ingestion framework (Java, Scala, Pyspark); Experience in building and operationalising feature pipeline to support AI\/ML model execution, data pipelines for supporting large scale data warehouse\/data marts; Additional requirements -   2 to 3 technical certifications from enclosed list:; Cloudera Hadoop distribution     \u2013 Hive, Impala, Spark, Kudo, Kafka, Flume; Teradata             \u2013 Bteq, Query Grid, GCFR, MDM, TAS, Data Mover, BAR; Informatica Data Integration     \u2013 PC, IDR, BDM, MM, IDQ, EDC; Data modelling tools (Erwin); QlikSense; Microsoft \u2013 R; Data science workbenches \u2013 Cloudera Machine Language, Jupyter, DataRobot, H2O.AI, IBM DSX; Data Virtualization tool (Denodo, Dremio); AS400; Language \u2013 SQL, Java, Python, Scala, Pyspark; Automation \/ scripting \u2013 CtrlM, Shell Scripting, Groovy ; UOB is an equal opportunity employer. UOB does not discriminate on the basis of a candidate's age, race, gender, color, religion, sexual orientation, physical or mental disability, or other non-merit factors. All employment decisions at UOB are based on business needs, job requirements and qualifications. If you require any assistance or accommodations to be made for the recruitment process, please inform us when you submit your online application.; Apply now and make a difference.; Competencies; 1. Strategise; 2. Engage; 3. Execute; 4. Develop; 5. Skills; 6. Experience","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85607474","Role":"Data & Quality Assurance Engineer Executive","Company":"Surbana Technologies","Location":"Singapore","Publish_Time":"2025-07-09 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/85607474","job_desc":"We are looking for a Data & Quality Assurance Engineer who will play a hybrid role in data analytics, software testing, and documentation management. This role is ideal for someone with a strong analytical mindset, attention to detail, and technical expertise in both data science and software testing. You will ensure data accuracy, optimize testing processes, and maintain high-quality documentation to support product development and business decisions.; Key Responsibilities; 1. Data Analytics & Data Science; Analyze and interpret large datasets to extract meaningful insights and support business decisions.; Develop data models, machine learning algorithms, and predictive analytics solutions.; Design and implement ETL (Extract, Transform, Load) pipelines for data processing.; Collaborate with stakeholders to define data-driven strategies and improve decision-making.; Utilize SQL, Python, or R for data analysis, visualization, and automation.; 2. Software Testing & Quality Assurance; Develop and execute test strategies for data-driven applications, APIs, and software systems.; Perform functional, regression, performance, and automated testing.; Identify, document, and track bugs to resolution using test management tools (Jira, TestRail, etc.).; Implement automation frameworks to improve efficiency in software testing.; Ensure data accuracy and integrity in software outputs and reports.; 3. Documentation & Process Management; Maintain clear and comprehensive documentation for data models, testing processes, and technical workflows.; Create test plans, test cases, and user guides to ensure product quality and knowledge transfer.; Document best practices for data governance, security, and compliance.; Support cross-functional teams with well-structured reports and documentation.; Qualifications & Skills; Bachelor\u2019s or Master\u2019s degree in Computer Science, Data Science, Software Engineering, or a related field.; Strong experience in SQL, Python, and data visualization tools (Tableau, Power BI, Matplotlib, etc.).; Hands-on experience with software testing methodologies, test automation, and defect tracking.; Knowledge of machine learning frameworks (Scikit-learn, TensorFlow, PyTorch) is a plus.; Experience with version control (Git) and Agile methodologies.; Excellent analytical, problem-solving, and communication skills.; Strong documentation skills with experience in Confluence, Markdown, or technical writing tools.; Preferred Qualifications; Experience in cloud platforms (AWS, Azure, GCP) for data processing and model deployment.; Familiarity with CI\/CD pipelines and DevOps tools for software testing.; Exposure to API testing tools such as Postman or RestAssured.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85272022","Role":"(Sr.) Data Engineer","Company":"BTSE","Location":"Singapore","Publish_Time":"2025-06-30 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/85272022","job_desc":"About BTSE:; DGTL SG is a specialized service provider dedicated to delivering a full spectrum of front-office and back-office support solutions, each of which are tailored to the unique needs of global financial technology firms. DGTL SG is engaged by BTSE Group to offer several key positions, enabling the delivery of cutting-edge technology and tailored solutions that meet the evolving demands of the fintech industry in a competitive global market. ; BTSE Group is a leading global fintech and blockchain company that is committed to building innovative technology and infrastructure. BTSE empowers businesses and corporate clients with the advanced tools they need to excel in a rapidly evolving and competitive market. BTSE has pioneered numerous trading technologies that have been widely adopted across the industry, setting new benchmarks for innovation, performance, and security in fintech. BTSE\u2019s diverse business lines serve both retail (B2C) customers and institutional (B2B) clients, enabling them to launch, operate, and scale fintech businesses. BTSE is seeking ambitious, motivated professionals to join our B2C and B2B teams.; About the opportunity:; The Data Architecture team is responsible for designing and implementing scalable data platforms and systems on AWS to support enterprise data warehousing and pipeline infrastructure. As a Data Engineer on this team, you will design, build, and maintain a robust data platform that enables reliable data integration, processing, and analytics across the organization.; Responsibilities:; Design, build, and maintain scalable data platforms and systems on AWS and Databricks to support analytics, reporting, and data processing workloads.; Work closely with the infrastructure team to build and operate the foundational components such as compute, storage, and networking that support data pipeline execution at scale.; Develop and maintain a reusable data job framework to enable efficient and scalable orchestration of data pipelines using PySpark and Databricks Workflow.; Optimize performance of distributed data processing systems, including Spark tuning and resource configuration, to ensure high efficiency and reliability.; Define and implement monitoring, alerting, and observability for the data platform infrastructure to maintain system health and support proactive issue resolution.; Collaborate cross-functionally with data engineers, analysts, and DevOps teams to deliver governed, high-quality data with strong platform-level reliability.; Requirements:; 3+ years of experience in data engineering or a related field, with hands-on experience in building cloud-based data systems (preferably AWS).; Strong proficiency in PySpark, SQL, and Python for large-scale data processing and performance tuning.; Hands-on experience with Databricks and orchestration tools such as Workflow or Apache Airflow, with a proven track record of designing reusable frameworks to run and manage data workflows.; Familiarity with CI\/CD practices and version control systems like GitLab.; Working knowledge of AWS services commonly used in data platforms, such as Amazon Glue, PostgreSQL (RDS or Aurora), and ElastiCache for Redis.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85571031","Role":"Data Scientist (Tableau, SQL, Banking) Central, CBD ~","Company":"PERSOLKELLY Singapore Pte Ltd (Formerly Kelly Services Singapore Pte Ltd)","Location":"Raffles Place","Publish_Time":"2025-07-08 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/85571031","job_desc":"Wealth and Retail Banking Industry;  Hybrid Working arrangements; Duration: 6 months subject to extendable\/convertible; Working Location: Central Business District, CBD; Working hours: 09.00am \u2013 6.00pm (Monday to Friday);  Job Duties; \u00b7         Strong Proficiency in SQL for data extraction, transformation, and analysis.; \u00b7         Expertise with visualization tools like Tableau, PowerBi in building dynamic dashboard, visual analytics and story telling with data; \u00b7         Experience with DataIku, Alteryx or other ETL tools for designing and operationalizing end-to-end data pipelines and workflows; \u00b7         Proficiency in Python for data wrangling, statistical analysis, machine learning and automation of data tasks.; \u00b7         Ability to analyze large datasets, identify trends, outliers, and generate business insights; \u00b7         Strong grasp of statistical techniques; \u00b7         Designing and maintaining data pipelines; \u00b7         Strong communication and storytelling skills for both technical and non-technical stakeholders; \u00b7         Experience working cross-functionally with product, engineering, or business teams; \u00b7         Domain knowledge and experience in Finance industry is a plus;  Requirements; Bachelor\u2019s or Master\u2019s degree in Computer Science, Statistics, Data Science, Engineering, or related field; 8+ years of professional experience in a data science or analytics role; Interested candidates, please click on the following link to begin your job search journey and submit your curriculum vitae (CV) directly through the official PERSOLKELLY job application platform - GO. https:\/\/sg.go.persolkelly.com\/job\/apply\/13140;  Contact number: 8189 1194;  We regret to inform that only shortlisted candidates will be notified.;  By sending us your personal data and CV, you are deemed to consent to PERSOLKELLY Singapore Pte Ltd and its affiliates to collect, use and disclose your personal data for account creation in GO and the purposes set out in the Privacy Policy https:\/\/www.persolkelly.com.sg\/policies. You acknowledge that you have read, understood, and agree with GO\u2019s Terms of Use https:\/\/go.persolkelly.com\/Tac and the Privacy Policy. If you wish to withdraw your consent, please email us at dataprotection@persolkelly.com. Please feel free to contact us if you have any queries.; PERSOLKELLY Singapore Pte Ltd \u2022 UEN No. 200007268E\u2022 EA License No. 01C4394\u2022 Reg. \u2022 R1981246 \u2022 Bertram Lee Kian Hui","salary":"$4,500 \u2013 $6,000 per month (SGD)","work_type":"Kontrak\/Temporer","country":"singapore"}
{"Job_ID":"85231462","Role":"Azure Data Technical Lead","Company":"Manpower Staffing Services (S) Pte Ltd - Head Office","Location":"Central Region","Publish_Time":"2025-06-27 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/85231462","job_desc":"We are looking for a skilled Azure Data Platform Technical Lead to drive the migration of on-premises systems to Azure. This role involves defining the migration methodology, ensuring data requirements and compliance strategies are met, and providing technical guidance to internal teams. You will play a key role in building and optimizing our client's Azure-based data warehouses, Databricks solutions, data pipelines, and Power BI reports.; Key Responsibilities; Analyze current practices, processes, and procedures to identify opportunities for leveraging Microsoft Azure Data and Analytics services.; Provide technical leadership in areas such as data access, ingestion, integration, processing, modeling, database design, visualization, and advanced analytics.; Collaborate with project managers to estimate technical tasks and deliverables.; Develop best practices, reusable code, libraries, and frameworks for cloud-based data warehousing and ETL processes.; Establish and maintain best practices for cloud-based data warehouse solutions, including naming conventions and development standards.; Job Requirements; Education: Bachelor's degree in Information Technology, Computer Science, Management Information Systems, Banking & Finance, or a related field.; Experience:; At least 10 years in Data Warehousing, Data Analytics, Real-time Data Integration, or Business Intelligence.; Minimum 3 years as a Data Architect, Solution Architect, or Technical Lead.; At least 2 years of experience leading technical teams in Azure data platform implementation.; Technical Skills:; Good understanding of both traditional and modern data architectures (SQL, Hadoop, Spark, Kafka, etc.).; Expertise in Databricks is mandatory.; Hands-on experience with:; Azure database platforms: Azure SQL Database, Azure Databricks, Azure Data Lake Gen2.; Azure data integration tools: Azure Synapse, Azure Data Factory, Azure Event Hub.; Data visualization tools: Power BI, QlikView.; Programming skills: Python, R.; Ability to design data architecture solutions connecting structured and unstructured data sources.; Knowledge of traditional ETL processes is a plus.; Experience in financial sector technologies.; If you are passionate about Azure data solutions and enjoy leading high-impact projects, we invite you to apply!; Gurram Sravanthi EA License No.: 02C3423 Personnel Registration No.: R2197596; Please note that your response to this advertisement and communications with us pursuant to this advertisement will constitute informed consent to the collection, use and\/or disclosure of personal data by ManpowerGroup Singapore for the purpose of carrying out its business, in compliance with the relevant provisions of the Personal Data Protection Act 2012. To learn more about ManpowerGroup's Global Privacy Policy, please visit https:\/\/www.manpower.com.sg\/privacy-policy","salary":"10000 - 13000 (SGD)","work_type":"Full time","country":"singapore"}
{"Job_ID":"84067715","Role":"Senior Data Engineer","Company":"Tencent International Service Pte. Ltd.","Location":"Singapore","Publish_Time":"2025-06-17 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/84067715","job_desc":"About the Hiring Team; Tencent Overseas IT has the mission to empower Tencent\u2019s rapid global growth with future ready, global IT platforms, applications and services. We are chartered to lead the Overseas IT strategy, architecture, roadmap and execution. Satisfying our internal\/external customers and becoming a world class global IT team are our top aspirations. What the Role Entails; Tencent Overseas IT aims to empower its rapid global growth with future-ready, global IT platforms, applications, and services. We are chartered to lead the Overseas IT strategy, architecture, roadmap, and execution. Our top aspirations are to satisfy our internal\/external customers and become a world-class global IT team.; Data platform architecture design and pipeline development, ensuring system stability and scalability; Work closely with analysts and product teams to define data standards and key metrics support business decisions; Own the deployment, management, and optimization of big data components such as Airflow, Spark, Flink and data warehouse; Continuously improve data workflows and promote automation and engineering efficiency; Who We Look For; Bachelor\u2019s degree or above in Computer Science or related fields\uff0c3+ years in data engineering; Experience with SQL and Python; and at least one of Java or Scala; Familiar with ETL pipeline design, scheduling, and performance tuning; Familiar with databases and data warehouses such as PostgreSQL, MySQL, StarRocks, ClickHouse; 5Experience with using and managing big data tools like Airflow, Spark, and Flink; Cross-team communication, able to independently engage with business teams; Fluent in Chinese and English is preferred in order to communicate with various stakeholders in headquarters; Equal Employment Opportunity at Tencent; As an equal opportunity employer, we firmly believe that diverse voices fuel our innovation and allow us to better serve our users and the community. We foster an environment where every employee of Tencent feels supported and inspired to achieve individual and common goals.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85579374","Role":"Lead Data Engineer","Company":"AIRR Labs","Location":"Central Region","Publish_Time":"2025-07-07 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/85579374","job_desc":"Reporting to the Regional Head of Data & BI, this role requires a hands-on approach to lead a team of data engineers to build and upkeep data pipelines, data warehouse, and analytics platforms to ensure data accuracy, data integrity and security.; Responsibilities; Lead the design, development, and implementation of scalable and efficient data pipelines, databases, and data infrastructure on cloud solutions; Implement ETL processes to integrate data from various sources, ensuring data quality, consistency, and integrity.; Optimize data pipelines, queries, and infrastructure for performance, scalability, and cost-efficiency.; Collaborate with cross-functional teams to gather requirements, assess technological options, and propose architectural solutions.; Mentor, coach, and guide a team of data engineers, providing technical leadership and support.; Requirements; Solid understanding of relational databases.; Extensive experience in Python, SQL, or other scripting languages is required. Experience with AWS services (S3, EC2, EMR, RDS) is a must.; Self-motivated, proactive and excellent communication skills. Experience in ecommerce industry is an advantage.; Bachelor or degrees above in Computer Science, Computer Engineering or other relevant degrees.; 6+ years of experience in data engineering, with at least 3 years in a lead or senior engineering role. Strong technical background in designing and implementing data pipelines, databases, and analytical tools.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85219825","Role":"Core Engineering, Data Software Engineer, Analyst\/ Associate, Singapore","Company":"Goldman Sachs","Location":"Singapore","Publish_Time":"2025-06-27 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/85219825","job_desc":"Core Engineering, Data Software Engineer, Analyst\/ Associate, Singapore; Goldman Sachs employs thousands of engineers across many divisions. We write software that powers every aspect of our business. It's no surprise that we collect and analyze vast amounts of data relating to the efficiency of our development processes, the value that we drive through software, and the effectiveness of how we leverage our internal tooling and services to empower development. Our data allows our internal businesses to optimize, focus their attention and manage costs. It allows us to assess the impact and performance of developer productivity initiatives, cost saving initiatives, and manage our huge software inventory.; What We Need; We are seeking a highly skilled Data Software Engineer to design, implement, and maintain robust data systems and pipelines that empower our organization to leverage data effectively. The ideal candidate will come from a software engineering background, with commercial development experience in one or more object-oriented languages (Python, Go, Java, C#). Knowledge of data modeling, pipeline construction, data normalization and sanitization, and data governance would be highly advantageous. They will collaborate with cross-functional teams to ensure the availability, quality, and security of data to support business objectives.; Key Responsibilities; Data Pipeline Development; Design, build, and maintain scalable and efficient data pipelines for processing and transforming large datasets.; Ensure pipelines are optimized for reliability and performance.; Data Modeling and Architecture; Develop and maintain logical and physical data models tailored to business needs.; Optimize data storage solutions for scalability and performance.; Data Quality and Sanitization; Implement processes for data normalization, deduplication, and cleaning to ensure high-quality datasets.; Identify and resolve data inconsistencies, errors, and anomalies.; Data Governance and Security; Establish and enforce data governance standards, including policies for data access, compliance, and privacy.; Implement security measures to protect sensitive data and ensure compliance with regulatory requirements.; Monitoring and Optimization; Develop monitoring solutions to ensure the health and reliability of data pipelines and systems.; Continuously optimize performance, storage, and costs of data infrastructure.; Qualifications; Education:; Bachelor\u2019s or Master\u2019s degree in Computer Science, Engineering, Data Science, or a related field.; Experience:; Min. 1 year (for Analyst)\/ 3 years (for Associate) of experience in software engineering, data engineering or a related role.; Proven experience with data modeling, ETL\/ELT pipelines, and data architecture design.; Proficiency in programming languages such as Python, Java, or Scala.; Strong knowledge of SQL and relational databases (e.g., PostgreSQL, Sybase, SQL Server).; Desirable Technical Skills:; Experience with big data technologies (e.g., Hadoop, Spark) and cloud platforms (e.g., AWS, GCP, Azure).; Familiarity with data integration tools (e.g., Apache Airflow, Talend, Informatica) and streaming technologies (e.g., Kafka, Flink).; Experience with data warehouse technologies (e.g., Snowflake, Redshift, BigQuery).; Soft Skills:; Strong analytical and problem-solving abilities.; Effective communication and collaboration skills to work with diverse teams.; Attention to detail and a proactive approach to ensuring data integrity.; Preferred Qualifications; Experience in conforming to data governance frameworks.; Knowledge of data lake architectures and unstructured data processing.; Familiarity with machine learning workflows and AI-driven data applications.; Certifications in relevant technologies (e.g., AWS Certified Data Analytics, Google Professional Data Engineer).; ABOUT GOLDMAN SACHS; At Goldman Sachs, we commit our people, capital and ideas to help our clients, shareholders and the communities we serve to grow. Founded in 1869, we are a leading global investment banking, securities and investment management firm. Headquartered in New York, we maintain offices around the world. ; We believe who you are makes you better at what you do. We're committed to fostering and advancing diversity and inclusion in our own workplace and beyond by ensuring every individual within our firm has a number of opportunities to grow professionally and personally, from our training and development opportunities and firmwide networks to benefits, wellness and personal finance offerings and mindfulness programs. Learn more about our culture, benefits, and people at GS.com\/careers. ; We\u2019re committed to finding reasonable accommodations for candidates with special needs or disabilities during our recruiting process. Learn more: https:\/\/www.goldmansachs.com\/careers\/footer\/disability-statement.html; \u00a9 The Goldman Sachs Group, Inc., 2023. All rights reserved.; Goldman Sachs is an equal opportunity employer and does not discriminate on the basis of race, color, religion, sex, national origin, age, veterans status, disability, or any other characteristic protected by applicable law.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85072379","Role":"Data engineer - Azure","Company":"Flintex Consulting Pte Ltd","Location":"City Hall","Publish_Time":"2025-06-21 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/85072379","job_desc":"Benefits: 13th Month Salary; Data engineer (Azure) \u2013 Synapse and Pyspark, Python,  Datawarehouse and Power BI , Azure Devops; Skills & Experience; Bachelor\u2019s Degree in Computer Science or Engineering with 3-5 years of experience in Azure Data engineering, Python, Pyspark or Big Data development; Sound Knowledge of Azure Synapse analytics for pipelines, orchestration, set up; 1-2  experience in Visualization design and development with Power BI. Knowledge on row-level security, access control; Sound experience in SQL, Datawarehouse, data marts, data ingestion with Pyspark and Python; Expertise in developing and maintaining ETL processing pipelines in cloud-based platforms such as AWS, Azure, etc. (Azure Synapse or data factory preferred); Team player with good interpersonal, communication, and problem-solving skills.; Job Scope; Design, review and development of Pyspark scripts. Testing, troubleshooting of data pipelines, orchestration; Designing and developing reports and dashboards in Power BI, setting up access control with row-level security, DAX query experience; Establishing connections to source data systems, including internal systems e.g. SAP, Historians, Data Lake, etc. as well as external systems such as Web APIs, etc; Managing the collected data in appropriate storage\/data-base solutions e.g. file systems, SQL servers, Big Data platforms such as Hadoop, HANA, etc. as required by the specific project requirements; Design, development of data marts and relevant data pipelines using pyspark, data copy activities for batch ingestion; Deployment of pipeline artifacts from one environment to the other using Azure Devops; Performing data integration e.g. using database table joins, or other mechanisms at an appropriate level as required by the analysis requirements of the project.; Good to have; Data catalog with Purview  enabling effective metadata management, lineage tracking, and data discovery; Candidates should demonstrate the ability to leverage Purview to ensure data governance, compliance, and efficient data exploration within Azure environments.; Others; Able to work independently on assignment according to agreed schedule without much supervision; Own assignment and take initiative to resolve issues hinder completion of assignment Proactively reach out for help\/guidance whenever required.","salary":"$7,000 \u2013 $10,000 per month (SGD)","work_type":"Full time","country":"singapore"}
{"Job_ID":"85155271","Role":"Big Data Engineer - TikTok","Company":"TikTok Pte. Ltd.","Location":"Singapore","Publish_Time":"2025-06-17 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/85155271","job_desc":"Responsibilities; TikTok is the leading destination for short-form mobile video. Our mission is to inspire creativity and bring joy. TikTok has global offices including Los Angeles, New York, London, Paris, Berlin, Dubai, Singapore, Jakarta, Seoul and Tokyo. Why Join Us Creation is the core of TikTok's purpose. Our platform is built to help imaginations thrive. This is doubly true of the teams that make TikTok possible. Together, we inspire creativity and bring joy; a mission we all believe in and aim towards achieving every day.; To us, every challenge, no matter how difficult, is an opportunity; to learn, to innovate, and to grow as one team. Status quo? Never. Courage? Always. At TikTok, we create together and grow together. That's how we drive impact; for ourselves, our company, and the communities we serve.; Join us. Team Introduction The mission of the Data Platform Singapore Business Partnering (DPSG BP) team is to empower the TikTok Business with data. Our goal is to build a Data Warehouse that can cater to batch and streaming data, Data Products that provide useful information to build efficient data metrics & dashboards which will be used to make smarter business decisions to support business growth. If you're looking for a challenging ground to push your limits, this is the team for you!; Translate business requirements & end to end designs into technical implementations and responsible for building batch and real-time data warehouse; Manage data modeling design, writing, and optimizing ETL jobs; Collaborate with the business team to building data metrics based on data warehouse; Responsible for building and maintaining data products; Involvement in rollouts, upgrades, implementation, and release of data system changes as required for streamlining of internal practices; Develop and implement techniques and analytics applications to transform raw data into meaningful information using data-oriented programming languages and visualisation software.; Apply data mining, data modelling, natural language processing, and machine learning to extract and analyse information from large structured and unstructured datasets.; Visualise, interpret, and report data findings and may create dynamic data reports as well.; Qualifications; At least 5 years in software engineering and 2 years of relevant experience in data engineering; Proficient in creating and maintaining complex ETL pipelines end-to-end while maintaining high reliability and security; Familiar with data warehouse concept and have production experience in modeling design; Familiar with at least 1 distributed computing engine (e.g. Hive, Spark, Flink); Familiar with at least 1 NoSQL database is a plus (e.g. HBase); Excellent interpersonal and communication skills with the ability to engage and manage internal and external stakeholders across all levels of seniority; Strong collaboration skills with the ability to build rapport across teams and stakeholders; TikTok is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives. Our platform connects people from across the globe and so does our workplace. At TikTok, our mission is to inspire creativity and bring joy. To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach. We are passionate about this and hope you are too.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85220289","Role":"AVP - Data Platform Operations","Company":"The Great Eastern Life Assurance Company Limited","Location":"Central Region","Publish_Time":"2025-06-27 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/85220289","job_desc":"We\u2019re looking for a motivated Platform Engineer to help design, build, and maintain scalable data platform infrastructure that supports smooth operations across our hybrid cloud environment. The role involves close collaboration with database, network, infrastructure, and business teams.; Data & AI Platform Operation and Support ; Configure and administer data platforms including security, performance, and scalability ; Create and manage user accounts and permissions, and manage data access ; Support the administration and daily operations of the data and AI platform, including issue resolution as needed ; Plan and execute upgrades, patches, and maintenance ; Develop and implement backup and recovery strategies ; Collaborate with IT teams to ensure integration with other systems ; Create and maintain SOP documentation ; Align with systems engineering to deploy\/ expand environments ; Collaborate with application teams for OS installations and updates ; Platform Optimization, Automation & Developer Enablement ; Drive continuous improvement in automation, optimization, and developer experience ; Explore and implement new data platform capabilities aligned with business needs ; Optimize platform performance and cost ; Champion \u201ccode base first\u201d DevOps\/ DataOps\/ MLOps practices and build CI\/CD pipelines for reliable Data & AI workflows ; Observability and Reliability ; Monitor platform performance and troubleshoot issues ; Responsible for high availability and performance of applications on platforms ; Capacity planning and scaling ; 24x7 monitoring of server performance, connectivity, and security ; File system management and monitoring ; Teaming with infrastructure, network, and BI teams to ensure data quality and availability ; Governance and Compliance ; Ensure smooth operations and support for the Data & AI platform ; Drive platform optimization, automation, and developer enablement ; Strengthen observability and system reliability ; Bachelor\u2019s Degree in Computer Science, Information Systems, or a related field ; At least 10 years of experience in platform implementation and administration with mandatory 2 years of hands-on experience in Tableau, Hadoop\/Cloudera, Informatica, Dataiku and AWS or equivalent technologies. ; Practical experience with DevOps practices including automation, CI\/CD, infrastructure as code, and code-first approaches for data workflows and deployments; Strong expertise in implementing comprehensive monitoring solutions using tools like Prometheus, Grafana, Dynatrace and Datadog, combined with deep experience in SRE practices including SLI\/SLO definition, incident response automation, chaos engineering, and distributed tracing across microservices architectures; Skills in driving continuous improvement through automation tools, optimizing platform performance and costs, and enhancing developer experience; Good understanding of OS concepts, process management and resource scheduling; Basics of networking, CPU, memory and storage; Good hold of shell scripting","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85652652","Role":"Data Engineer","Company":"Safran Helicopter Engines","Location":"North Region","Publish_Time":"2025-07-10 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/85652652","job_desc":"The Data Engineer's mission is to collect data, aggregate it into analytical or BI solutions, and make it available to the systems and\/or organizations that require it. The engineer also carries out developments to collect, cleanse, and structure data within our platform from internal and external information systems.; Roles and Responsibilities:; - Build and develop data models.; - Build APIs to exchange data.; - Propose and implement data retrieval tools, processing, and solutions.; - Implement and\/or industrialize data entry and retrieval algorithms.; - Validate and monitor platform developments.; - Perform maintenance and upkeep of deployed solutions.; - IT Functional Maintenance (incidents and developments),; - POCs, POVs, and MVPs (implemented in QCD),; - Preliminary Projects (solution selection, definition of the provisional schedule, provisional budget, deployment, and risk identification),; - Projects (implemented in QCD),; - Technology monitoring,; - Compliance with the service level of the application solutions in operation; - Manage project life cycle:; Assists project owners in expressing requirements; Manages the design (technical and functional) and specifications; Evaluate software packages with the project owner; Participates in implementation in terms of specific developments or integration; Participates in the definition and implementation of test plans and business acceptance testing; - Project management:; Organizes, coordinates, and leads the entire project management team; Arbitrates any disputes between the team and other stakeholders; Supervises project progress; Coordinates, summarizes, and ensures the quality of approvals issued; Circulates and disseminates information to the project management team; Manages the relationship with the supplier(s) (from the (from contract signing to final project validation); - Technical deployment of the project and implementation of user support actions:; Deploys the new application or service; Organizes maintenance and SLAs; Participates in user training; Organizes user support Guarantees the best quality-cost-deadline match:; Ensures compliance with specifications; Ensures compliance with deadlines and costs; Challenges needs to ensure the convergence of needs\/solutions in compliance with information systems standards; Proposes to the business or project owner, during the project, any changes to objectives (quality, cost, deadline) related to implementation constraints or environmental changes; This role requires collaborations with:; * IT Business Leaders,; * IT Project Owners,; * Safran Group entities within the scope of the Group IT Department.; The ideal candidate would have more than 8 years of relevant working experience in similar role, possess at least a diploma in engineering field and is familiar with SAP.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85481896","Role":"Senior Data engineer","Company":"Flintex Consulting Pte Ltd","Location":"City Hall","Publish_Time":"2025-07-05 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/85481896","job_desc":"Benefits: 13th Month Salary; Responsibilities; Integrate data from multiple sources, such as databases, APIs, or streaming platforms, to provide a unified view of the data; Implement data quality checks and validation processes to ensure the accuracy, completeness, and consistency of data; Identify and resolve data quality issues, monitor data pipelines for errors, and implement data governance and data quality frameworks; Enforce data security and compliance with relevant regulations and industry-specific standards; Implement data access controls, encryption mechanisms, and monitor data privacy and security risks; Optimise data processing and query performance by tuning database configurations, implementing indexing strategies, and leveraging distributed computing frameworks; Optimize data structures for efficient querying and develop data dictionaries and metadata repositories; Identify and resolve performance bottlenecks in data pipelines and systems; Collaborate with cross-functional teams, including data scientists, analysts, and business stakeholders; Document data pipelines, data schemas, and system configurations, making it easier for others to understand and work with the data infrastructure; Monitor data pipelines, databases, and data infrastructure for errors, performance issues, and system failures; Set up monitoring tools, alerts, and logging mechanisms to proactively identify and resolve issues to ensure the availability and reliability of data; It would be a plus if he has software engineering background; Requirements; Bachelor's or master's degree in computer science, information technology, data engineering, or a related field; Strong knowledge of databases, data structures, algorithms; Proficiency in working with data engineering tools and technologies including knowledge of data integration tools (e.g., Apache Kafka, Azure IoTHub, Azure EventHub), ETL\/ELT frameworks (e.g., Apache Spark, Azure Synapse), big data platforms (e.g., Apache Hadoop), and cloud platforms (e.g., Amazon Web Services, Google Cloud Platform, Microsoft Azure); Expertise in working with relational databases (e.g., MySQL, PostgreSQL, Azure SQL, Azure Data Explorer) and data warehousing concepts.; Familiarity with data modeling, schema design, indexing, and optimization techniques is valuable for building efficient and scalable data systems; Proficiency in languages such as Python, SQL, KQL, Java, and Scala; Experience with scripting languages like Bash or PowerShell for automation and system administration tasks; Strong knowledge of data processing frameworks like Apache Spark, Apache Flink, or Apache Beam for efficiently handling large-scale data processing and transformation tasks; Understanding of data serialization formats (e.g., JSON, Avro, Parquet) and data serialization libraries (e.g., Apache Avro, Apache Parquet) is valuable; Having experience in CI\/CD and GitHub that demonstrates ability to work in a collaborative and iterative development environment; Having experience in visualization tools (e.g. Power BI, Plotly, Grafana, Redash) is beneficial; Preferred Skills & Characteristics; Consistently display dynamic independent work habits, goal oriented, passionate in growth mindsets and self-motivated professional. Self-driven and proactive in keeping up with new technologies and programming","salary":"$6,000 \u2013 $9,000 per month (SGD)","work_type":"Full time","country":"singapore"}
{"Job_ID":"85038918","Role":"Contract - Data Engineer [AI Data Pipeline Dvt & Mgt] (1 yr)","Company":"Infineon Technologies","Location":"Kallang","Publish_Time":"2025-06-19 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/85038918","job_desc":"#WeAreIn for driving decarbonization and digitalization.; As a global leader in semiconductor solutions in power systems and IoT, Infineon enables game-changing solutions for green and efficient energy, clean and safe mobility, as well as smart and secure IoT. Together, we drive innovation and customer success, while caring for our people and empowering them to reach ambitious goals. Be a part of making life easier, safer and greener.; Are you in?; ; We are on a journey to create the best Infineon for everyone.; This means we embrace diversity and inclusion and welcome everyone for who they are. At Infineon, we offer a working environment characterized by trust, openness, respect and tolerance and are committed to give all applicants and employees equal opportunities. We base our recruiting decisions on the applicant\u00b4s experience and skills.; Please let your recruiter know if they need to pay special attention to something in order to enable your participation in the interview process.; Click here for more information about Diversity & Inclusion at Infineon.; The Data Engineer will serve as a technical expert in the fields of design and develop AI data pipelines to manage both large unstructured and structured datasets, with a particular focus on GenAI RAG\/Agent solutions.; ; In your new role you will:; Working closely with data scientists and domain experts to design and develop AI data pipelines using agile development process.; Developing pipelines for ingesting and processing large unstructured and structured datasets from a variety of sources, ensure efficient and effective data processing.; Development of BIA solution using defined framework for Data Modelling; Data Profiling; Data Extraction, Transformation & Loading; Design and provide data\/information in form of reports, dashboards, scorecards and data storytelling using Visualization Tools such as Business Objects & Tableau.; Work with cloud technologies such as AWS to design and implement scalable data architectures; Supporting the operation of the data pipelines involves troubleshooting and bug fixing, as well as implementing change requests to ensure that the data pipelines continue to meet user requirements.; You are best equipped for this task if you have:; Master's or Bachelor's Degree in Computer Science\/Mathematics\/ Statistics or equivalent.; Minimum of 3 years of relevant work experience in data engineering, including in-depth technical knowledge of databases, BI tools, SQL, OLAP, ETL, RAG \/ Agentic Data pipeline.; Proficient in RDBMS: Oracle\/PL SQL; Extensive hands-on experience in conceptualising, designing, and implementing data pipelines. Proficiency in handling unstructured data formats (e.g., PPT, PDF, Docx), databases (RDMS, NoSQL such as Elasticsearch, MongoDB, Neo4j, CEPH) and familiarity with big data platforms (HDFS, Spark, Impala).; Experience in working with AWS technologies focusing on building scalable data pipelines.; Front-end Reporting & Dashboard and Data Exploration tools -Tableau; Strong background in Software Engineering & Development cycles (CI\/CD) with proficiency in scripting languages, particularly Python.; Good understanding and experience with Kubernetes \/ Openshift Platform.; Other Skills \/ Attributes:; Good understanding of data management, data governance, and data security practices.; Highly motivated, structured and methodical with high degree of self-initiative; Team player with good cross-cultural skills to work in an international team; Customer and result-oriented; This is a 12 months contract under 3rd party payroll partner and entitled to benefits according to partner company","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85570215","Role":"Contract - Data Engineer [AI Data Pipeline] (1 year)","Company":"Infineon Technologies","Location":"Kallang","Publish_Time":"2025-07-08 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/85570215","job_desc":"#WeAreIn for driving decarbonization and digitalization.; As a global leader in semiconductor solutions in power systems and IoT, Infineon enables game-changing solutions for green and efficient energy, clean and safe mobility, as well as smart and secure IoT. Together, we drive innovation and customer success, while caring for our people and empowering them to reach ambitious goals. Be a part of making life easier, safer and greener.; Are you in?; ; We are on a journey to create the best Infineon for everyone.; This means we embrace diversity and inclusion and welcome everyone for who they are. At Infineon, we offer a working environment characterized by trust, openness, respect and tolerance and are committed to give all applicants and employees equal opportunities. We base our recruiting decisions on the applicant\u00b4s experience and skills.; Please let your recruiter know if they need to pay special attention to something in order to enable your participation in the interview process.; Click here for more information about Diversity & Inclusion at Infineon.; The Data Engineer will serve as a technical expert in the fields of design and develop AI data pipelines to manage both large unstructured and structured datasets, with a particular focus on GenAI RAG\/Agent solutions.; ; In your new role you will:; Working closely with data scientists and domain experts to design and develop AI data pipelines using agile development process.; Developing pipelines for ingesting and processing large unstructured and structured datasets from a variety of sources, ensure efficient and effective data processing.; Development of BIA solution using defined framework for Data Modelling; Data Profiling; Data Extraction, Transformation & Loading; Design and provide data\/information in form of reports, dashboards, scorecards and data storytelling using Visualization Tools such as Business Objects & Tableau.; Work with cloud technologies such as AWS to design and implement scalable data architectures; Supporting the operation of the data pipelines involves troubleshooting and bug fixing, as well as implementing change requests to ensure that the data pipelines continue to meet user requirements.; You are best equipped for this task if you have:; Master's or Bachelor's Degree in Computer Science\/Mathematics\/ Statistics or equivalent.; Minimum of 3 years of relevant work experience in data engineering, including in-depth technical knowledge of databases, BI tools, SQL, OLAP, ETL, RAG \/ Agentic Data pipeline.; Proficient in RDBMS: Oracle\/PL SQL; Extensive hands-on experience in conceptualising, designing, and implementing data pipelines. Proficiency in handling unstructured data formats (e.g., PPT, PDF, Docx), databases (RDMS, NoSQL such as Elasticsearch, MongoDB, Neo4j, CEPH) and familiarity with big data platforms (HDFS, Spark, Impala).; Experience in working with AWS technologies focusing on building scalable data pipelines.; Front-end Reporting & Dashboard and Data Exploration tools -Tableau; Strong background in Software Engineering & Development cycles (CI\/CD) with proficiency in scripting languages, particularly Python.; Good understanding and experience with Kubernetes \/ Openshift Platform.; Other Skills \/ Attributes:; Good understanding of data management, data governance, and data security practices.; Highly motivated, structured and methodical with high degree of self-initiative; Team player with good cross-cultural skills to work in an international team; Customer and result-oriented; This is a 12 months contract under 3rd party payroll partner and entitled to benefits according to partner company","salary":"","work_type":"Kontrak\/Temporer","country":"singapore"}
{"Job_ID":"85206352","Role":"Data Engineer","Company":"MERIDIAN & SATURN CAPITAL PTE. LTD.","Location":"Singapore","Publish_Time":"2025-06-26 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/85206352","job_desc":"MS Capital is a private fund management company with a strong founding team of long-accumulated experience in strategy modelling, trading system and platform development, adopting advanced artificial intelligence technology as the cornerstone, and enforcing strict investment management, to achieve sustained and stable returns. We are expending the team, searching for experienced candidates have strong background & skills in programing, statistics modelling, data analysis, etc.; Roles & Responsibilities:; Ingest, standardize, and maintain global equity datasets from multiple third-party vendors (e.g., Bloomberg, Refinitiv, FactSet, S&P Global, Exegy, etc.).; Structure and align data to support quantitative research, factor modeling, and portfolio construction.; Develop systems to track and handle complex corporate actions including ticker changes, delistings, mergers, spin-offs, dual listings, ADR\/local shares.; Collaborate closely with PMs, quantitative researchers, and trading infrastructure engineers to define and fulfill data requirements.; Understand prime brokers\u2019 internal ticker conventions and develop robust mappings across trading venues, custodians, and OMS\/EMS systems.; Ensure data quality through rigorous validation, monitoring pipelines, and anomaly detection.; Help maintain symbol mapping libraries across time zones, exchanges, and asset classes.; Qualifications:; Bachelor's degree or above in Computer Science, Engineering or related field.; Deep familiarity with global equity markets, including North America, Europe, and Asia-Pacific exchanges.; Proficient in Python (Pandas, NumPy), SQL, and data pipeline orchestration tools (e.g., Airflow, Luigi).; Knowledge of major data vendors and experience working with financial reference and pricing data.; Strong understanding of market microstructure and security identifiers (e.g., ISIN, CUSIP, SEDOL, RIC, Bloomberg Ticker).; Ability to navigate prime brokerage data and internal ticker mapping for order execution and post-trade reconciliation.; experience in a data engineering or quantitative data operations role at a hedge fund, proprietary trading firm, or asset manager.; Prior experience working with tick- or bar-level data, especially for intraday equity strategies.; Familiarity with buy-side OMS\/EMS systems and order flow handling.; Interested applicants please apply directly here or send your resume to hr@mscapital.sg","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85640039","Role":"Data Engineer - Sanderson-iKas Singapore Pte Ltd","Company":"Sanderson-Ikas","Location":"Singapore","Publish_Time":"2025-07-10 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/85640039","job_desc":"Responsibilities; Design, develop, and maintain backend systems and data pipelines.; Support data integration, transformation, and delivery processes.; Implement solutions using Java, Scala, or Python.; Work within Agile development teams using tools such as Git, Bitbucket, Jenkins, and Jira.; Troubleshoot, optimize, and ensure performance of data systems.; Engage in code reviews and contribute to system documentation.; Skills; 4-10 years of experience in backend development or data engineering.; Proficiency in Java or Scala (ability to work with both is preferred).; Strong understanding of data modeling and data architecture.; Experience with cloud platforms (AWS, Azure, or GCP).; Familiarity with Databricks, Snowflake, or similar platforms is a plus.; Ability to work with both legacy and modern systems.; Strong analytical and problem-solving skills.; Bachelor's or Master's degree in Computer Science, Engineering, or a related field.; Prior experience in regulatory or risk-related systems is advantageous but not mandatory.; For a confidential discussion, please reach out to me at ali.zaidi@sanderson-ikas.sg; Personal data collected will be used for recruitment purposes only.; Only shortlisted candidates will be notified \/ contacted.; EA Registration No: R1988468; \"Sanderson-iKas\" is the brand name for iKas International (Asia) Pte Ltd, a company incorporated in Singapore under Company UEN No.: 200914065E with EA license number 16S8086.; Website: www.sanderson-ikas.sg; ; information_technology","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85783260","Role":"Attractive Job Opening for AWS Python Data Engineer in Singapore","Company":"TALENT XPERTS PTE. LTD.","Location":"Paya Lebar Air Base","Publish_Time":"2025-07-15 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/85783260","job_desc":"Mandatory skills*; Very Strong Proficiency:; Python: Extensive experience in Python for data manipulation, scripting, and building data applications.; PySpark: Deep expertise in developing and optimizing large-scale data transformations using PySpark.; SQL: Advanced SQL skills, including complex query writing, performance tuning, and database design.; AWS: Hands-on experience designing, deploying, and managing data solutions on various AWS services (S3, EMR, Glue, Lambda etc).; Solid understanding of data warehousing concepts, ETL\/ELT principles, and data pipeline best practices.; Excellent problem-solving, analytical, and communication skills.; Ability to work independently and as part of a collaborative team.; Desired skills*; Airflow: Experience with Airflow for orchestrating and managing data workflows.; Snowflake: Familiarity with Snowflake for cloud data warehousing and analytical processing.; Bitbucket (or Git): Proficient in using version control systems for collaborative development.; Domain*; Data Engineering; Mode of Interview: Telephonic\/Face to Face\/Skype Interview* -Teams or F2F","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85170784","Role":"SAS Data Engineer, Data & Analytics, Technology Consulting","Company":"Ernst & Young Solutions LLP","Location":"Singapore","Publish_Time":"2025-06-17 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/85170784","job_desc":"At EY, you\u2019ll have the chance to build a career as unique as you are, with the global scale, support, inclusive culture and technology to become the best version of you. And we\u2019re counting on your unique voice and perspective to help EY become even better, too. Join us and build an exceptional experience for yourself, and a better working world for all. ; The opportunity; EY DnA is the data and advanced analytics capability within EY Asia-Pacific, with over 500 specialist employees working across multiple industry sectors.; We implement information-driven strategies, data platforms and advanced data analytics solution systems that help grow, optimize and protect client organizations. We go beyond strategy and provide end to end design, build and implementation of real life data environments and have some of the best architects, project managers, business analysts, data scientists, big data engineers, developers and consultants in the region.; We are looking for a Data Engineer with SAS Viya skills within the DnA team in our Singapore office. This role is offered on a flexible full time basis.;  Your key responsibilities; Strong analytical and problem-solving skills; Strong drive to excel professionally, and to guide and motivate others; Advanced written and verbal communication skills; Dedicated, innovative, resourceful, analytical and able to work under pressure; Foster an efficient, innovative and team-oriented work environment; Skills and attributes for success; Experience in ETL, Data Engineering, Scripting.; Knowledge and experience in end-to-end project delivery, either traditional SDLC or agile delivery methodologies (or hybrid approaches); Experience in a delivery role on Business Intelligence, Data Warehousing, Big Data or analytics projects; Exceptional communication, documentation and presentation skills and stakeholder management experiences; Experience in business intelligence, data warehousing\/platform, and data strategy projects; To qualify for the role you must have; At least 5 years\u2019 experience as a SAS Engineer with experience in development and  maintenance support; Experience with SAS BASE, Enterprise Guide (EG), Event Stream Processing (ESP), Visual Analytics (VA) , Viya platforms (VI and VA) and Data Integration (DI) are required; Minimum 2yr experience in SAS application maintenance and SAS Management Console (SMC); Minimum 2yrs experience in dealing large data sets in Greenplum \/ Hadoop \/ Oracle \/ DB2; Dashboarding experience with tableau \/ power BI \/ SAS VA is desirable; Experience in shell \/batch scripting; Application packaging and deployment experience across DEV to PROD environments; Experience in maintenance reporting and managing change \/ service requests.; Ideally, you\u2019ll also have; Experience in engaging with both technical and non-technical stakeholders; Consulting experience and background, including engaging directly with clients; Degree in Computer Science or IT or Business Analytics; What we offer; EY offers a competitive remuneration package commensurate with your work experience where you\u2019ll be rewarded for your individual and team performance. We are committed to being an inclusive employer and are happy to consider flexible working arrangements, where this may be needed, guided by our FWA Policy.; Plus, we offer:; Continuous learning: You\u2019ll develop the mindset and skills to navigate whatever comes next.; Success as defined by you: We\u2019ll provide the tools and flexibility, so you can make a meaningful impact, your way.; Transformative leadership: We\u2019ll give you the insights, coaching and confidence to be the leader the world needs.; Diverse and inclusive culture: You\u2019ll be embraced for who you are and empowered to use your voice to help others find theirs.; If you can demonstrate that you meet the criteria above, please contact us as soon as possible.; The exceptional EY experience. It\u2019s yours to build.; Apply now.","salary":"","work_type":"Full time, Paruh waktu","country":"singapore"}
{"Job_ID":"85206114","Role":"Senior Data Engineer","Company":"CYBER RECRUITZ (PTE. LTD.)","Location":"Singapore","Publish_Time":"2025-06-26 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/85206114","job_desc":"We\u2019re Hiring: Senior Data Engineer; Location: Onsite \u2013 Singapore; Experience: 8\u201310 years; Employment Type: Full-time; \u2e3b; Role: Senior Data Engineer; We\u2019re looking for a hands-on Senior Data Engineer skilled in building scalable pipelines and transforming data using Databricks on AWS, PySpark, and Python. Experience with Informatica DEI is a bonus.; Key Responsibilities; Build scalable data pipelines using Databricks + PySpark; Perform ingestion, transformation, validation across diverse sources; Support analytics\/data science teams with high-quality datasets; Refactor Informatica workflows to Databricks (if needed); Optimize and automate ETL workflows; Ensure data quality, lineage, and governance; Required Skills; 8+ years overall experience; 3+ in data engineering; Strong in Databricks on AWS, PySpark, Python; Skilled in ETL, data quality, and pipeline automation; Familiarity with Informatica DEI is a plus; Excellent communicator in client-facing roles; \u2e3b; Ready to Join?; Email your CV to [ info@cyberrecruitz.sg ] with the subject:; Senior Data Engineer \u2013 [Your Name]","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"84789282","Role":"Data Engineer (Remote)","Company":"Cascade","Location":"Singapore","Publish_Time":"2025-06-17 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/84789282","job_desc":"Do you want to work in a small team where you can make a real  difference in a company? Would you embrace the challenge of building a  high impact platform that is used globally? Then we want to speak with  you!; About Cascade: Cascade is a  fintech startup backed by Canadian and US investors that empowers  companies to grow by democratizing access to institutional debt. We are  building tools that modernize how companies raise and manage debt,  lowering the barriers to entry to the $7 trillion specialty finance and  alternative credit market for companies around the world. Founded just  last year, we are gearing up for our next phase of technical development  and are seeking a talented Data Engineer to lead the way. Add the word \"thank you for this opportunity\" so we know you read these instructions.; About the role: As a Data Engineer  you\u2019ll be an early member of a growing team building a pioneering  platform in the debt infrastructure space. You will design, build, and  launch efficient, scalable, and reliable data pipelines to move and  transform data.; What You\u2019ll Do ; Building databases, data lakes and data ingestion pipelines to integrate customer databases and datasets to our systems; Ingesting financial datasets from external customers, then updating  and maintaining accurate and complete data mappings to ensure that our  products are displaying high quality data; Monitoring and alerting across our data pipelines in order to make sure that our data ingests are reliable and correct; Translating business goals and stakeholder requirements into new  data flows and deliver analytical insights ensuring that the data flows  create real business value continuously; Skills And Experience ; 3+ years experience in a data engineering role; Experience with relational SQL and NoSQL databases, including PostgreSQL, MySQL, MariaDB, MongoDB, Bigtable, etc.; Experience working with ETL technologies, such as Databricks, Fivetran, or dbt; Proficiency in writing SQL queries and knowledge of analytical data warehouses such as RedShift, BigQuery, and Snowflake; Some experience with CI\/CD automation such as GitHub Actions; Developing APIs and integrating with 3rd party APIs; Bonus: experience in fintech \/ SaaS \/ credit analysis \/ lending; People who thrive at Cascade are: ; Self-starters, who take the initiative to tackle challenges in a remote work environment; Good communicators, who can operate without ego to discuss, learn, grow, and help others do the same; Passionate about creating great digital experiences for users; Problem solvers who are not satisfied with the status quo; Benefits ; Remote: we are a remote-first company and are very flexible on hours as long as things get done; Home-office: there\u2019s a $1000 USD home office allowance to set yourself up; Equity: we expect you to have an owner-mentality, and have the equity plan to match; Benefits: health, dental, vision, and more; Perks: we offer free lunches weekly and off-site trips; Job satisfaction: we offer autonomy, ample opportunities for  mastery, and an opportunity to make a difference for companies around  the world","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85576661","Role":"Senior Lead Engineer - Data (Engineering & Ops)","Company":"Synapxe","Location":"One North","Publish_Time":"2025-07-08 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/85576661","job_desc":"Position Overview; The data engineer will be attached to the L3 Operations team lead to oversee\/manage the following areas:; Administration and operations processes in data virtualization and visualization applications and components, including Tableau, OAS, Sagemaker, Appsstream; Technical refresh projects for application components near EOS\/EOL; Security compliance and deviations; Facilitate the operations transition after migration to HEALIX; Role & Responsibilities; Manage systems administration function and lead the organisation's system projects and environments for sustainable operations; Plan and oversee systems upgrades and migrations, ensuring systems are up-to-date with the latest patches and coherent across the organisation; Address multi-faceted issues effectively and collaboratively across departments; Integrate diverse needs and perspectives from internal and external clients; Develop new and innovative ideas and solutions; Influence key stakeholders and clients to adopt a data-driven approach to resolve business issues; Oversee budgeting and planning; Set data standards, governance, and best practices; Develop standards, policies, and controls to ensure system security and compliance; Execute standard operating procedures to ensure system security and compliance; Maintain up-to-date inventory of existing system assets and architecture artifacts with the latest technologies; Provide guidance on best practices related to data governance and security compliance processes; Continually educate staff on the latest changes in standard operations practices and policies; Performance Optimization and Continuous Improvement:; Recommend process, product, or service improvements, resource optimization, and cost savings; Oversee hardware and software upgrades; Review resource utilization to identify areas for improvement or resolve bottlenecks; Recommend new technologies, methodologies, systems, or opportunities for cost savings, security, and service quality improvement; Optimize and automate processes to achieve higher efficiency and sustainability; Team Management:; Oversee team management, including budgets, forecasting, work allocations, and staffing; Develop staff through ongoing coaching, mentoring, and career discussions; Define common goals, direction, and accountability among staff; Drive effective performance management practices within the department in accordance with company policies and procedures; Requirements; Degree in Computer Science, Computer Engineering; Minimum 12-15 year working experience in system operations compliance and management areas; Highly-motivated self-starter who will undertake all activities to the highest professional standards; Must be cloud certified; Good in-depth understanding of data warehouse concepts, data profiling, data verification and advanced analytics techniques; Experience in managing a team in project implementation or support is a must; Certification in IT operation management is preferred; Possess prior hands-on experience and technical expertise such as Databricks, Informatica, Tableau, OBIEE, SQL databases & AWS cloud technologies; Good interpersonal skills with the ability to work with different groups of stakeholders; Exposure to hospital information \/ clinical systems is an added advantage; Apply Now; NOTE: It only takes a few minutes to apply for a meaningful career in HealthTech - GO FOR IT!!; #LI-SYNX34","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85685007","Role":"Associate Data Engineer - Datawarehouse (Engineering & Ops)","Company":"Synapxe","Location":"One North","Publish_Time":"2025-07-11 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/85685007","job_desc":"Position Overview; The Data Engineer supports the implementation of data structure and architecture, master\/meta-data management approach and data quality programme to facilitate access to data and information. He\/She support the design, implementation and maintenance of data flow channels and data processing systems that support the collection, storage, batch and real-time processing, and analysis of information from structured and unstructured sources in a scalable, repeatable and secure manner on on-premise or commercial cloud. He\/She implements data management standards and practices.; Role & Responsibilities; Manage and prioritize user queries and production issues for existing applications in Engineering and Operations; Track and resolve production support incidents; Attend user meetings to document and analyze change request requirements or conduct regular workgroup meetings with stakeholders; Perform data profiling and mapping to define data requirements for new projects or change requests; Provide support for production reports, dashboards, and metadata; Collaborate with vendors and developers to design, configure, and test enhancements per Synapxe project methodologies; Translate user requirements into analytics, reporting needs, and ETL rules for new data mart applications and enhancements; Identify and document business attributes and metrics by analyzing existing data and reporting requirements; Conduct technical data mapping for potential data warehouse sources; Execute testing phases (system integration testing, user acceptance testing) before implementation; Provide 24\/7 primary application maintenance support; Assist the Project Manager in assessing technical feasibility for cost evaluations; Requirements; Bachelor's degree in Computer Science, Information Technology, or a related field; At least 4 years of experience in the IT industry, including:; Development, implementation, and maintenance of IT systems, preferably in Data Warehousing, ETL rules, data modeling, and BI applications; Operations support and business analysis experience; Strong MS-SQL and Oracle Database scripting; Experience in diagnosing, troubleshooting, and performing root cause analysis; Ability to diagnose and troubleshoot problems with BI reports and ETL processes; Experience with AWS, Data Lake, Databricks, and the healthcare domain is a plus; Able to work independently and as an effective team player with a strong desire to deliver results; Adaptable, meticulous, and possess strong analytical skills; Good communication skills (both written and spoken); Strong team player; Apply Now; NOTE: It only takes a few minutes to apply for a meaningful career in HealthTech - GO FOR IT!!; #LI-SYNX40","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85703612","Role":"Big Data Engineer (Libra) - Data Platform","Company":"TikTok Pte. Ltd.","Location":"Central Region","Publish_Time":"2025-07-13 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/85703612","job_desc":"About TikTok; TikTok is the leading destination for short-form mobile video. At TikTok, our mission is to inspire creativity and bring joy. TikTok's global headquarters are in Los Angeles and Singapore, and we also have offices in New York City, London, Dublin, Paris, Berlin, Dubai, Jakarta, Seoul, and Tokyo.; Why Join Us; Inspiring creativity is at the core of TikTok's mission. Our innovative product is built to help people authentically express themselves, discover and connect \u2013 and our global, diverse teams make that possible. Together, we create value for our communities, inspire creativity and bring joy - a mission we work towards every day.; We strive to do great things with great people. We lead with curiosity, humility, and a desire to make impact in a rapidly growing tech company. Every challenge is an opportunity to learn and innovate as one team. We're resilient and embrace challenges as they come. By constantly iterating and fostering an \"Always Day 1\" mindset, we achieve meaningful breakthroughs for ourselves, our company, and our users. When we create and grow together, the possibilities are limitless. Join us.; Diversity & Inclusion; TikTok is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives. Our platform connects people from across the globe and so does our workplace. At TikTok, our mission is to inspire creativity and bring joy. To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach. We are passionate about this and hope you are too.; Job highlights; Positive team atmosphere, Career growth opportunity, Meals provided; Responsibilities; About the team; Libra is a large-scale online one-stop A\/B testing platform developed by TikTok Data Platform. Some of its features include:; - Provides experimental evaluation services for all product lines within the company, covering solutions for complex scenarios such as recommendation, algorithm, function, UI, marketing, advertising, operation, social isolation, causal inference, etc.; - Provides services throughout the entire experimental lifecycle from experimental design, experimental creation, indicator calculation, statistical analysis to final evaluation launch.; - Supports the entire company's business on the road of rapid iterative trial and error, boldly assuming and carefully verifying.; Responsibilities; - Responsible for data system of experimentation platform operation and maintenance.; - Construct PB-level data warehouses, participate in and be responsible for data warehouse design, modeling, and development, etc.; - Build ETL data pipelines and automated ETL data pipeline systems.; - Build an expert system for metric data processing that combines offline and real-time processing.; Qualifications; Minimum Qualifications; - Bachelor's degree in Computer Science, a related technical field involving software or systems engineering, or equivalent practical experience.; - Proficiency with big data frameworks such as Presto, Hive, Spark, Flink, Clickhouse, Hadoop, and have experience in large-scale data processing.; - Minimum 1 year of experience in Data Engineering.; - Experience writing code in Java, Scala, SQL, Python or a similar language.; - Experience with data warehouse implementation methodologies, and have supported actual business scenarios.; Preferred Qualifications; - Knowledge about a variety of strategies for ingesting, modeling, processing, and persisting data, ETL design, job scheduling and dimensional modeling.; - Expertise in designing, analyzing, and troubleshooting large-scale distributed systems is a plus (Hadoop, M\/R, Hive, Spark, Presto, Flume, Kafka, ClickHouse, Flink or comparable solutions).; - Work\/internship experience in internet companies, and those with big data processing experience are preferred.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85810846","Role":"Data Engineer, Global E-Commerce (Governance Service - Risk Control Platform)","Company":"TikTok Pte. Ltd.","Location":"Singapore","Publish_Time":"2025-07-16 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/85810846","job_desc":"Responsibilities; Global E-Commerce is a content e-commerce business with international short video product as the carrier. It is committed to becoming the first choice for users to discover and purchase good products with affordable prices. Global E-Commerce business team hopes to provide users with more tailored and efficient consumption experience, enabling merchants to receive reliable platform services in different scenarios such as live e-commerce, short video content e-commerce, thereby making more affordable and high-quality products easily accessible and improving lives.; The Global E-Commerce Risk Control Team is committed to combating risks with groups in the underground black industry, building a risk protection system for the platform, identifying current and future risk trends through human-machine collaboration, and promoting the prevention and control of business to support the rapid development of TikTok's e-commerce business. We are seeking passionate data engineers with strong problem-solving abilities to work hand in hand with talented cross-functional partners (business operations, data science, engineering, and product management) to efficiently solve some highly challenging data development and construction tasks. In this position, you will contribute to the company's core business, and your daily work will directly impact the company's and customers' financial security and business strategic planning.; Responsibilities:; Collaborate closely with product managers, strategic operations, internal engineering teams, etc. to understand data requirements and provide data solutions that meet business needs.; Evaluate, implement, and maintain data infrastructure tools and technologies to support efficient data processing, storage, and querying.; Design, build, and optimise scalable data pipelines to ingest, process, and transform large volumes of data to support complex analytical queries and reporting requirements.; Ensure data integrity, accuracy, and consistency by implementing data quality checks, validation processes, and monitoring mechanisms.; Continuously optimise data pipelines, queries, and processes to improve performance, reduce latency, and enhance scalability.; Build and maintain the data assets for the risk control business at a high level, serving as the foundation for engineering and product applications.; Qualifications; Minimum Qualifications:; Bachelor's or higher degree in Computer Science, Information Technology, Programming & System Analysis, Science (Computer Studies) or related discipline.; Have experience as a data engineer or similar role supporting data-centric businesses.; Solid knowledge of SQL and work experience with relational and non-relational databases.; Proficiency in at least one programming language such as Python, Java, Go, etc.; Solid mastery of data modeling and data warehouse concepts, data integration, and ETL\/ELT technologies.; Effective communication skills and the ability to collaborate effectively with cross-functional teams.; Excellent problem-solving skills, attention to detail, and the ability to thrive in a fast-paced environment.; Preferred Qualifications:; Experience in using big data technologies (such as Apache Hadoop, Spark, Kafka, Flink) and working experience in handling data ranging from TB to PB levels.; Experience in data governance, e-commerce data, data privacy, and compliance.; Experience in the cross-border\/payment\/e-commerce\/risk control industries.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85742158","Role":"Site Reliability Engineer - Data Management Suite","Company":"TikTok Pte. Ltd.","Location":"Central Region","Publish_Time":"2025-07-14 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/85742158","job_desc":"About TikTok; TikTok is the leading destination for short-form mobile video. At TikTok, our mission is to inspire creativity and bring joy. TikTok's global headquarters are in Los Angeles and Singapore, and we also have offices in New York City, London, Dublin, Paris, Berlin, Dubai, Jakarta, Seoul, and Tokyo.; Why Join Us; Inspiring creativity is at the core of TikTok's mission. Our innovative product is built to help people authentically express themselves, discover and connect \u2013 and our global, diverse teams make that possible. Together, we create value for our communities, inspire creativity and bring joy - a mission we work towards every day.; We strive to do great things with great people. We lead with curiosity, humility, and a desire to make impact in a rapidly growing tech company. Every challenge is an opportunity to learn and innovate as one team. We're resilient and embrace challenges as they come. By constantly iterating and fostering an \"Always Day 1\" mindset, we achieve meaningful breakthroughs for ourselves, our company, and our users. When we create and grow together, the possibilities are limitless. Join us.; Diversity & Inclusion; TikTok is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives. Our platform connects people from across the globe and so does our workplace. At TikTok, our mission is to inspire creativity and bring joy. To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach. We are passionate about this and hope you are too.; Job highlights; Industry experts, Meals provided, Competitive compensation, Flexible hours; Responsibilities; About the Team; The Data Management Suite team is building products that cover the whole lifecycle of data pipeline, including data ingestion and Integration, data development, data catalog, data security and data governance. These products support various businesses, so data engineers and data scientists could greatly boost their productivity.; As a software engineer in the data management suite team, you will have the opportunity to build, optimize and grow one of the largest data platforms in the world. You'll have the opportunity to gain hands-on experience on core systems in the data platform ecosystem. Your work will have a direct and huge impact on the company's core products as well as hundreds of millions of users.; Responsibilities:; Be responsible for the production stability for big data development and governance systems;; Engage in and improve the whole lifecycle of service, from inception and design, through to deployment, operation and refinement;; Maintain services once they are live by measuring and monitoring availability, latency and overall system health. Practice sustainable incident response and blameless postmortems;; Establish best engineering practice for engineers as well as non-technical people;; Design and implement reliable, scalable, robust and extensible big data systems that support core products and business;; Qualifications; Minimum Qualifications; Bachelor's degree in Computer Science, a related technical field involving software or systems engineering, or equivalent practical experience; Experience with site reliability engineering, monitoring, alerting for big data related systems; Experience writing code in Java, Go, Python or a similar language; Preferred Qualifications; Knowledge about a variety of strategies for ingesting, modeling, processing, and persisting data, ETL design, job scheduling and dimensional modeling;; Familiarity with running production grade services at scale and understanding cloud native technologies and networking;; Experience developing tools and APIs to reduce human interaction with systems and applications using a variety of coding and scripting standards;; Expertise in designing, analyzing, and troubleshooting large-scale distributed systems is a plus (Hadoop, M\/R, Hive, Spark, Presto, Flume, Kafka, ClickHouse, Flink or comparable solutions);; Systematic problem-solving approach, coupled with effective communication skills and a sense of drive;","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85219557","Role":"Senior Principal Consultant (Data Platform Engineering)","Company":"National University of Singapore","Location":"National University Of Singapore","Publish_Time":"2025-06-27 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/85219557","job_desc":"Company description:; ; The National University of Singapore is the national research university of Singapore. Founded in 1905 as the Straits Settlements and Federated Malay States Government Medical School, NUS is the oldest higher education institution in Singapore; ; ; Job description:; ; Job Purpose; We are seeking an experienced Data Platform Engineer to manage data platform operations, and to design and deliver ELT\/ETL frameworks at NUS.; The role will be responsible for addressing stakeholders' needs regarding data platform operations and ensuring alignment with business requirements. The role will also co-lead the design, development and maintenance of metadata driven ELT\/ETL frameworks in NUS.; This is a hands-on role, and the candidate should have at least 5 years of active, hands-on design and development experience.; Role and Responsibilities; Technical Expertise; \u2022 Participate in the technical design of data platforms and their technologies, including their framework, architecture, standards, and guidelines.; \u2022 Stay up-to-date with emerging data platform technologies and industry trends to drive innovation.; \u2022 Ensure adherence to best practices in ELT\/ETL frameworks design, development, testing, and maintenance.; \u2022 Provide technical guidance and expertise to the different stakeholders on data platform operations, helping to solve complex technical challenges.; Leadership Management; \u2022 Provide guidance to the technical data team on using the data platform and ELT\/ETL frameworks.; \u2022 Foster collaboration and high-performance culture within the team.; Delivery Management; \u2022 Ensure timely and high-quality ELT\/ETL frameworks design and delivery, aligned with project objectives, scope, and timelines.; Quality Assurance; \u2022 Implement quality control and testing procedures to guarantee the high-quality delivery.; \u2022 Ensure alignment with the university's regulatory and compliance requirements.; Qualifications and Requirements; \u2022 Bachelor's degree in Computer Science, Information Technology, or a related field.; \u2022 Minimum of 5 years of experience in technical leadership roles, with a strong focus on data platforms, ELT\/ETL frameworks, and data engineering solutions.; \u2022 Strong problem-solving and critical-thinking skills, with a proven track record of effectively addressing technical challenges and risk mitigation.; \u2022 Strong knowledge of quality assurance processes, encompassing testing methodologies, and quality control procedures.; \u2022 Relevant experience in designing, developing, and supporting ELT\/ETL frameworks and data engineering solutions.; \u2022 Strong verbal, written and interpersonal communication skills with the ability to interact and communicate effectively with all levels of management, users, and vendors.; \u2022 Must be a good team player, proactive in nature, fast learner, highly organized and go-getter attitude with can-do spirit.; Technical Expertise; \u2022 Strong working experience in managing enterprise data platform operations.; \u2022 Strong working experience in end-to-end development of ETL frameworks and data engineering solutions for data lakes and data warehouses.; \u2022 Strong working experience in Azure Data services, preferably Microsoft Fabric.; \u2022 Strong database working experience for transactional and analytics systems, including querying and tuning large, complex data sets and performance analysis.; \u2022 Working experience in big data and DataSecOps is an added advantage.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85612748","Role":"ETL Test Architect","Company":"Tech Mahindra","Location":"Singapore","Publish_Time":"2025-07-09 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/85612748","job_desc":"Skill Set : API testing; Total Experience : 4.00 to 12.00 Years; No of Openings : 1; Job Post Date : 09\/07\/2025; Job Expiry Date : 09\/08\/2025; Domain : IT; Location : SINGAPORE [Singapore]; Job Reference No : 1054930; Job Summary; Senior Data Test Engineer KEY ACCOUNTABILITIES Maintain and adopt Agile best practices and lifecycles for process workflows (e.g., Kanban, CI\/CD); Collaborate with business users and business analysts to refine and understand both functional and non-functional requirement during SIT & UAT stages. Develop automated test scripts to validate functional and technical requirements in data processing pipeline and to perform data quality checks Collaborate with data analysts in profiling data and monitoring data trends; Collaborate with Developers\/DevOps Engineers on code management, peer review, continuous integrated testing in CI\/CD pipelines Assure quality at different phases of SDLC by adhering to process and strategies defined by Eastspring IT Execute manual \/ automated \/ exploratory tests and provide QA sign-off to business users for releases; Maintain test process, design and execution artifacts in test management system complying the audit regulations Prepare testing traceability reports and other testing metrics QUALIFICATIONS \/ EXPERIENCE; Recognized degree or higher in Computer Science or related Engineering fields. At least 8 years of working experience in Test Automation, using test frameworks for Database (ETL Testing) and Data analytical testing. Working knowledge in testing Data management platform tools same-as\/similar-to \u2018Golden Source\u2019; Sound knowledge in Java programming, SQL queries and Cucumber (Java) testing framework. Good knowledge in testing scheduling\/orchestration tools (like Control-M, Azure Data Factory) Working knowledge of relational databases and comfortable with testing SQL jobs and stored procedures with awareness of data security.; Basic understanding of data quality, profiling, and analytics concepts. Working experience with test management tools such as Jira with Xray \/ Zephyr Working knowledge of tools such as bitbucket, Jenkins, confluence and familiar with Git branching model; Working experience in Agile projects, Behavior Driven Development (BDD) approach to software development and testing. Good to have basic programming knowledge in Python Good to have knowledge of Azure cloud platform.; Good to have working experience in investment Bank or Asset Management industry. OTHER TRAITS Positive attitude and collaborative mindset.; Willing to work across projects and perform manual \/ automation \/ exploratory testing Highly motivated to keep abreast with the latest development in technology and to acquire deep technical knowledge and skills. Excellent communication, presentation, and interpersonal skills.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85182175","Role":"Big Data Engineer Intern - 2025 Start","Company":"TikTok Pte. Ltd.","Location":"Singapore","Publish_Time":"2025-06-26 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/85182175","job_desc":"Responsibilities; About TikTok TikTok is the leading destination for short-form mobile video. At TikTok, our mission is to inspire creativity and bring joy. TikTok's global headquarters are in Los Angeles and Singapore, and its offices include; New York, London, Dublin, Paris, Berlin, Dubai, Jakarta, Seoul, and Tokyo. Why Join Us Creation is the core of TikTok's purpose.; Our products are built to help imaginations thrive. This is doubly true of the teams that make our innovations possible. Together, we inspire creativity and enrich life; a mission we aim towards achieving every day.; To us, every challenge, no matter how ambiguous, is an opportunity; to learn, to innovate, and to grow as one team. Status quo? Never.; Courage? Always. At TikTok, we create together and grow together.; That's how we drive impact-for ourselves, our company, and the users we serve. Join us. Team Introduction; Our Data Platform Business Partnering team is at the core of TikTok E-Commerce business, responsible for generating tremendous amount of data and providing accessing services and applications. We empower many business & engineering teams to analyze and develop innovative strategies and products driving business growth. We are looking for passionate and talented backend engineers to join us to drive the future of E-Commerce together.; As a project intern, you will have the opportunity to engage in impactful short-term projects that provide you with a glimpse of professional real-world experience. You will gain practical skills through on-the-job learning in a fast-paced work environment and develop a deeper understanding of your career interests. Successful candidates must be able to commit to a minimum of 3 months full-time.; Responsibilities; Translate business requirements & end to end designs into technical implementations and responsible for building batch and real-time data warehouse.; Manage data modeling design, writing, and optimizing ETL jobs.; Collaborate with the business team to building data metrics based on data warehouse.; Responsible for building and maintaining data products.; Involvement in rollouts, upgrades, implementation, and release of data system changes as required for streamlining of internal practices.; Develop and implement techniques and analytics applications to transform raw data into meaningful information using data-oriented programming languages and visualisation software.; Apply data mining, data modelling, natural language processing, and machine learning to extract and analyse information from large structured and unstructured datasets.; Visualise, interpret, and report data findings and may create dynamic data reports as well.; Qualifications; Minimum qualifications:; Final year or undergraduate with a background in Software Development, Computer Science, Computer Engineering, or a related technical discipline.; Solid computer basic knowledge (e.g. data structure & algorithms, SQL and networks).; Strong coding capabilities and mastering at least one programming language (e.g. C\/C++\/Java\/Python\/Golang).; Preferred qualifications:; Passionate about data warehouse, ETL development, data analysis and eCommerce.; Good communication skills and a fast learner of new business and technology knowledge.; Strong collaboration skills with the ability to build rapport across teams and stakeholders.; TikTok is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives. Our platform connects people from across the globe and so does our workplace. At TikTok, our mission is to inspire creativity and bring joy.; To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach. We are passionate about this and hope you are too. By submitting an application for this role, you accept and agree to our global applicant privacy policy, which may be accessed here: https:\/\/careers.tiktok.com\/legal\/privacy.; If you have any questions, please reach out to us at apac-earlycareers@tiktok.com","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85215301","Role":"Customer Engineer, Data Analytics and AI, Google Cloud","Company":"Google","Location":"Singapore River","Publish_Time":"2025-06-27 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/85215301","job_desc":"Google will be prioritizing applicants who have a current right to work in Singapore, and do not require Google's sponsorship of a visa.; ; Minimum qualifications:; Bachelor's degree or equivalent practical experience.; 10 years of experience with cloud native architecture in a customer-facing or support role.; Experience with practical application and architectural considerations of AI\/ML, including Machine Learning (ML) lifecycle, ML frameworks (e.g., TensorFlow, PyTorch) and MLOps principles.; Experience in developing data warehouses, data lakes, batch\/real-time event processing, streaming, data processing (ETL\/ELT), data migrations, data visualization and data governance on cloud native architectures.; Experience in conducting technical discovery, business and technical requirements and translating them into efficient and innovative technical architectures that leverage data and AI services.; ; Preferred qualifications:; Experience implementing MLOps best practices, CI\/CD pipelines for ML models and infrastructure as code for deploying data and AI solutions.; Experience in integrating data and AI solutions with existing enterprise systems, on-premises infrastructure and third-party applications.; Experience in optimizing performance, cost-efficiency and scalability of large-scale data processing pipelines and ML inference systems.; Experience with hybrid cloud architectures and multi-cloud strategies, particularly in data integration and migration.; Experience with architectural design and implementation of solutions involving LLMs, Generative AI models or AI agents (e.g., Vertex AI Agent Engine, ADK, LangChain, LlamaIndex).; About the job; The Google Cloud Platform team helps customers transform and build what's next for their business \u2014 all with technology built in the cloud. Our products are developed for security, reliability and scalability, running the full stack from infrastructure to applications to devices and hardware. Our teams are dedicated to helping our customers \u2014 developers, small and large businesses, educational institutions and government agencies \u2014 see the benefits of our technology come to life. As part of an entrepreneurial team in this rapidly growing business, you will play a key role in understanding the needs of our customers and help shape the future of businesses of all sizes use technology to connect with customers, employees and partners.; As a Customer Engineer, you will collaborate with technical sales teams as a data analytics and Artificial Intelligence expert to showcase Google Cloud's value.You will help customers and partners leverage Google Cloud by designing solutions, conducting proofs-of-concept and resolve technical issues related to data migrations and lifecycle management. You will understand customer needs, present tailored solutions and demonstrate technical and communications skills while working with a team of Googlers that fosters equal opportunities for success.; Google Cloud accelerates every organization\u2019s ability to digitally transform its business and industry. We deliver enterprise-grade solutions that leverage Google\u2019s cutting-edge technology, and tools that help developers build more sustainably. Customers in more than 200 countries and territories turn to Google Cloud as their trusted partner to enable growth and solve their most critical business problems.; Responsibilities; Design and implement solutions that cover the entire data lifecycle from ingestion, storage and processing in both batch and real-time to advanced analytics and Machine Learning (ML) model deployment.; Guide customers in building and integrating collaborative AI agents using tools like Vertex AI agent engine and Agent Development Kit (ADK) and understand the Agent2Agent protocol can facilitate seamless communication and data exchange between these agents across systems.; Identify opportunities where AI agents can automate tasks, enhance decision-making and create value, ensuring solutions are technically sound and deliver tangible business outcomes.; Communicate architectural concepts effectively to technical stakeholders and executive leadership, demonstrate Google Cloud's data and AI offerings address and unlock new possibilities.; Leverage the latest advancements in Google Cloud's data and AI portfolio to provide solutions and competitive differentiation.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85603062","Role":"AVP, Data Engineer, Group Asset Management - Business Technology","Company":"United Overseas Bank Limited (UOB)","Location":"Singapore","Publish_Time":"2025-07-09 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/85603062","job_desc":"United Overseas Bank Limited (UOB) is a leading bank in Asia with a global network of more than 500 branches and offices in 19 countries and territories in Asia Pacific, Europe and North America. In Asia, we operate through our head office in Singapore and banking subsidiaries in China, Indonesia, Malaysia and Thailand, as well as branches and offices.; Our history spans more than 80 years. Over this time, we have been guided by our values \u2013 Honorable, Enterprising, United and Committed. This means we always strive to do what is right, build for the future, work as one team and pursue long-term success. It is how we work, consistently, be it towards the company, our colleagues or our customers.; Established in 1986, UOB Asset Management (UOBAM) is a wholly owned subsidiary of United Overseas Bank. Headquartered in Singapore, UOBAM has grown extensively across Asia with local presence in Brunei, Indonesia, Japan, Malaysia, Taiwan, Thailand and Vietnam. Our network includes UOB Islamic Asset Management in Malaysia and a joint venture with China\u2019s Ping An Trust to form Ping An Fund Management Company. We have also forged a strategic alliance with Wellington Management Singapore.; Our experienced team of more than 90 investment professionals conduct rigorous fundamental research within a proven investment framework to provide our clients with innovative investment solutions. The strength of our team lies in our commitment to investment excellence. Our performance has been recognised by the industry and we have garnered over 340 awards regionally since 1986.; Through our regional network, we offer global investment management expertise to individuals, institutions and corporations. Our comprehensive suite of products ranges from retail unit trusts and exchange-traded funds to customised portfolio management services for institutional clients. A leader in innovation, UOBAM offers a digital option to manage investments with UOBAM Invest robo-adviser, making investing simpler, smarter and safer.;   UOBAM Technology provides software and system development, as well as information technology support services and banking operations.;   We have centralized and standardized the technology components into Singapore, creating a global footprint which can be utilized for supporting our regional subsidiaries and the branches around the world. We operate and support 8 countries with this architecture to provide a secure and flexible Asset Management infrastructure.; Develop and maintain infrastructure for enterprise data platforms and machine learning.; Collaborate with business stakeholders to gather requirements and translate them into effective data visualizations and reporting solutions.; Design, build, and maintain scalable and interactive dashboards using BI tools such as Power BI, Tableau, or Looker.; Implement and manage data governance frameworks, including data cataloging, lineage, quality, and access controls using cloud-native tools (e.g., AWS Glue, Azure Purview, Google Cloud Data Catalog).; Data Platform Development & Management:; Design, develop, and maintain data platforms that support large-scale data ingestion, storage, and processing using cloud-based data infrastructure.; Implement and manage data warehousing and centralized data solutions tailored for asset management.; Evaluate and integrate new data technologies and tools to enhance data platform capabilities.; Data Pipeline Development:; Build and maintain robust and efficient data pipelines for data ingestion, processing, and transformation.; Develop and implement data quality checks and validation processes to ensure data accuracy, timeliness, and consistency.; Utilize ETL\/ELT tools and techniques to transform and load data into target systems.; Employ exceptional problem-solving skills, with the ability to see and solve issues before they snowball into problems.; Learn and share knowledge and experience in a multi-disciplinary team.; Bachelor's or Master's degree in Computer Science, Data Science, Engineering, or a related field.; At least 5 years of experience in data engineering, business intelligence, machine learning engineering, or a related role in a production environment.; Familiarity with data governance frameworks (e.g., DAMA-DMBOK) and regulatory compliance (e.g., GDPR, CCPA).; Hands-on experience in Python and SQL. Experience with other programming languages (e.g., Java, Scala, C++) is a plus.; Experience in best practices such as DataOps and MLOps; Experience with big data technologies and cloud platforms such as BigQuery, Kafka, GCP, AWS and their data engineering and machine learning products and services.; Strong understanding of software development best practices, including version control (Git), testing, and CI\/CD.; Excellent communication and organizational skills, and the ability to stay focused on completing tasks and meeting goals within a busy workspace.; Skilled at working in tandem with a team of engineers, or alone as required.; Strong troubleshooting and analytical skills.; Cloud and data certifications are a plus.; UOB is an equal opportunity employer. UOB does not discriminate on the basis of a candidate's age, race, gender, color, religion, sexual orientation, physical or mental disability, or other non-merit factors. All employment decisions at UOB are based on business needs, job requirements and qualifications. If you require any assistance or accommodations to be made for the recruitment process, please inform us when you submit your online application.; ; Apply now and make a difference.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85176733","Role":"Data Engineer - Singapore","Company":"Shopline","Location":"Singapore","Publish_Time":"2025-06-17 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/85176733","job_desc":"Data Engineer - SHOPLINE Singapore; SHOPLINE is Asia\u2019s largest smart commerce platform. With our customers in mind, we strive to deliver scalable commerce solutions to merchants of all sizes. We\u2019re a full-featured platform with services including online store opening, O2O solution, retail POS systems, advertising placement, business strategy consultation, marketing, and more to empower merchants to succeed in omnichannel retailing and cross-border commerce.; What you\u2019ll be doing:; Responsible for collecting, designing, storing and processing payments data in the eCommerce business. In charge of unifying and standardizing data to create holistic business digital assets.; Play a leading role in constructing business evaluation metrics, developing tactics in data services and creating data-driven tools in alignment with Products and Operations objectives.; Define underlying business data requirements. Build fitted models to enhance data quality and stability. Standardize and maintain data integrity to provide an efficient way in accessing data.; Who we are looking for:; 5+ years of experience as a data engineer with a bachelor or advanced in Computer Science. Proven track record in data warehousing.; Demonstrated strength in data modeling, development and governance. Preferred experience with building ETL pipelines.; Expert skills in SQL and Python. Familiarity with big data technologies and solutions (Spark, Hadoop, Hive, etc.). Nice to have a background in Machine Learning.; Proven success in communicating across different functions and synthesizing resources to push through projects in a dynamic environment.; Preferred experience in tech firms, especially in the payments industry.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85155117","Role":"Big Data Engineer (Libra) - Data Platform","Company":"TikTok Pte. Ltd.","Location":"Singapore","Publish_Time":"2025-06-17 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/85155117","job_desc":"Responsibilities; Libra is a large-scale online one-stop A\/B testing platform developed by TikTok Data Platform. Some of its features include:; Provides experimental evaluation services for all product lines within the company, covering solutions for complex scenarios such as recommendation, algorithm, function, UI, marketing, advertising, operation, social isolation, causal inference, etc.; Provides services throughout the entire experimental lifecycle from experimental design, experimental creation, indicator calculation, statistical analysis to final evaluation launch.; Supports the entire company's business on the road of rapid iterative trial and error, boldly assuming and carefully verifying.; Responsibilities; Responsible for data system of experimentation platform operation and maintenance.; Construct PB-level data warehouses, participate in and be responsible for data warehouse design, modeling, and development, etc.; Build ETL data pipelines and automated ETL data pipeline systems.; Build an expert system for metric data processing that combines offline and real-time processing.; Qualifications; Minimum Qualifications; Bachelor's degree in Computer Science, a related technical field involving software or systems engineering, or equivalent practical experience.; Proficiency with big data frameworks such as Presto, Hive, Spark, Flink, Clickhouse, Hadoop, and have experience in large-scale data processing.; Minimum 1 year of experience in Data Engineering.; Experience writing code in Java, Scala, SQL, Python or a similar language.; Experience with data warehouse implementation methodologies, and have supported actual business scenarios.; Preferred Qualifications; Knowledge about a variety of strategies for ingesting, modeling, processing, and persisting data, ETL design, job scheduling and dimensional modeling.; Expertise in designing, analyzing, and troubleshooting large-scale distributed systems is a plus (Hadoop, M\/R, Hive, Spark, Presto, Flume, Kafka, ClickHouse, Flink or comparable solutions).; Work\/internship experience in internet companies, and those with big data processing experience are preferred.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"84124393","Role":"Data Engineer - Global E-Commerce (Governance Service - Security Platform)","Company":"TikTok Pte. Ltd.","Location":"Singapore","Publish_Time":"2025-06-17 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/84124393","job_desc":"Responsibilities; Global E-Commerce is a content e-commerce business with international short video product as the carrier. It is committed to becoming the first choice for users to discover and purchase good products with affordable prices. Global E-Commerce business team hopes to provide users with more tailored and efficient consumption experience, enabling merchants to receive reliable platform services in different scenarios such as live e-commerce, short video content e-commerce, thereby making more affordable and high-quality products easily accessible and improving lives.; The Global E-Commerce's Governance and Experience is a global team responsible for ensuring a safe and trustworthy marketplace not only for our buyers but also for our sellers and creators. We continually work on areas such as risk detection abilities, fairness, and sustainability of the E-Commerce ecosystem, content and commodity quality, and friction-free experiences to drive improvement. Responsibilities:; Responsible for the offline data warehouse construction of global e-commerce security, including data layering, model design, ETL process, etc.; Establish data warehouse standards for global e-commerce security to ensure data accuracy and consistency.; Collaborate with the security team in optimising their offline tasks to ensure system stability and improving resource efficiency.; Assist in identifying risk indicators through data analysis to support the development of key security projects.; Support real-time data processing pipelines, including tagging systems and real-time ETL infrastructure.; Visualise, interpret, and report data findings and may create dynamic data reports as well.; Qualifications; Minimum Qualifications:; Bachelor's or higher degree in Computer Science, Information Technology, Programming & System Analysis, Science (Computer Studies) or related discipline.; Candidates should have at least 5 years of experience in big data ecosystem development, familiar with technologies such as Spark, Flink, Clickhouse, Hadoop, and practical experience with Lambda\/Kappa architectures.; Proficient in data warehouse implementation methodologies, with a deep understanding of data warehouse systems, and experience supporting real-world business scenarios.; Experience in SQL performance tuning, with an understanding of Hive SQL development.; Preferred Qualifications:; Candidates with deep experience in real-time data warehouse construction.; Data-sensitive with strong business understanding, excellent logical reasoning skills, and some data analysis capabilities.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85632704","Role":"APAC Cloud Solution Architect (Big Data)","Company":"Huawei International Pte. Ltd.","Location":"Changi","Publish_Time":"2025-07-10 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/85632704","job_desc":"Responsibilities: ; Responsible for data architecture, solution design, and product planning, in Big Data projects and product management.; Responsible for the technical side of big data projects, including planning, implementation, and management of the process. Includes both pre-sales (demos & POCs) and post-sales (delivery & support).; Effectively articulate the value of our Big Data products and solutions via a keen understanding of competitor Big Data product limitations and drawbacks. Able to tailor discussions to stakeholders of varying kinds, levels and cultures across APAC.; Deeply understand the challenges and direction of Digital Transformation in various industries, identify key requirements of customers, gain insight into various industry development trends, within the Data domain; Gain deep familiarity with the capabilities and competitive advantages of Huawei Cloud EI Products (Big Data and AI). Master the technical aspects of products such as Pros & Cons, ideal use-cases, optimize the data architecture based on different requirements, organize technical POC support, and ensure the feasibility of product solutions. ; Promote the continuous improvement of Big Data products through product requirement management as the go-between customer-facing colleagues and Product Owners to improve competitiveness. ; Who you are:; Independent, hands-on, willing to learn, and takes accountability to deliver results.; Excellent customer facing skills. Well-developed spoken and written communication skills and the ability to tailor style to relevant audiences, and successfully liaise with people at different levels. ; Strong communication skills with the ability to collaborate & communicate across teams within the organization.; Strong analytical and problem-solving skills, solution driven, highly organized and detail oriented.; Requirements: ; Minimum Bachelors degree with at 1-3 years experience in the technology industry. ; Required to travel based on the assigned projects. ; Hands-on experience with technologies in Data Analytics, such as SQL, Python; Preferred Skills; Experience & knowledge of Big Data technologies & concepts such as Data Warehouse, ETL, Data Governance, Airflow, Spark, Hive, Flink, SQL, Python, Data Catalog, BI Dashboards, etc.; Good understanding of mainstream cloud products and services including Amazon Web Services (AWS), Microsoft Azure and Google Cloud Platform (GCP). ; Knowledge of related cloud computing domains, especially AI, is a plus. Relevant domains include Database, Compute (K8s, VMs), Networking, Security, Landing Zone etc.; Professional customer-facing experience in Data, tech pre-sales, Consulting","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85068770","Role":"Lead Full Stack Data Engineer (2 year contract)","Company":"StarHub Ltd","Location":"Singapore","Publish_Time":"2025-06-17 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/85068770","job_desc":"We seek a seasoned Full Stack developer with strong interest and hands-on data engineering skills to design, develop, and deploy AI-powered, cloud-based products. You will own frontend\/backend development, database\/schema design, data pipelines, and integrate user-facing features with data services, collaborating closely with data science and infrastructure teams to deliver production-grade solutions.; As a Lead Full Stack Data Engineer, you will:; \u00b7 Architect & Build Full Stack Applications; \u2022 Design responsive UIs using Next.js, React, Vue, or Angular; \u2022 Implement server-side logic, REST\/GraphQL APIs, and microservices with Nest.js, Node.js, or Express; \u2022 Ensure seamless data flow, authentication (JWT\/OAuth2), and caching (Redis); \u00b7 Design & Maintain Data Pipelines & Databases \u2022 Define data models for relational (PostgreSQL, Redshift) or document stores (MongoDB); \u2022 Develop ETL\/ELT pipelines (PySpark, Airflow) to load data into warehouses; \u2022 Implement scalable storage (tables, indexes, partitions, materialized views) and tune queries; \u00b7 Integrate with Data Services & APIs; \u2022 Consume analytics\/ML endpoints and real-time streams (Kafka, Kinesis); \u2022 Implement efficient data-fetching on the frontend (pagination, caching, debouncing); \u2022 Design endpoints serving aggregated or pre-processed data; manage API versioning\/documentation (Swagger); \u00b7 Implement CI\/CD & DevOps Collaboration; \u2022 Define CI\/CD pipelines (GitLab CI\/CD) for both applications and data workflows; \u2022 Containerize components with Docker; orchestrate with Kubernetes, Docker Compose, or ECS Fargate; \u2022 Collaborate on cloud provisioning (Terraform, CloudFormation) and manage secrets (AWS Secrets Manager); \u00b7 Develop Dashboards & Visualizations; \u2022 Build dynamic charts with D3.js, ECharts, or Recharts to surface key metrics; \u2022 Create real-time data displays using WebSockets or polling; \u2022 Implement frontend data validation (date pickers, filters, drill-downs); \u00b7 Mentorship & Collaboration; \u2022 Mentor junior engineers; conduct code reviews and share best practices; \u2022 Work with product, infra, delivery\/sales specialist teams to refine requirements, automate tests, and enforce security standards; \u00b7 Bachelor\u2019s or Master\u2019s in CS, Software Engineering, Data Science, or equivalent experience; \u00b7 6+ years as a Full Stack developer with demonstrable data engineering involvement; \u00b7 Proficient in JS\/TypeScript frameworks: Next.js, React, Vue, or Angular; \u00b7 Strong Server-side Rendering (Next.js). Node.js experience with Nest.js or Express; RESTful\/GraphQL API design;; \u00b7 ETL\/ELT pipeline development (PySpark, Airflow) and data modeling for PostgreSQL, Redshift, or MongoDB; \u00b7 Experience integrating real-time streams (Kafka, Kinesis) and consuming ML\/analytics endpoints; \u00b7 Define scalable storage (tables, indexes, partitions, materialized views) and perform query tuning (sort keys, distribution keys, vacuum); \u00b7 CI\/CD pipeline creation (GitLab CI\/CD) and containerization (Docker, Kubernetes); \u00b7 Infrastructure as Code: Terraform or CloudFormation; secret management (AWS Secrets Manager); \u00b7 Cloud experience (AWS) deploying full stack apps and data pipelines (S3, EMR, Redshift); \u00b7 Unit\/integration testing (Jest, Mocha, pytest) and E2E testing (Cypress, Playwright); \u00b7 Strong problem-solving, attention to detail, and ability to mentor and collaborate cross-functionally; Nice to Have; \u00b7 Familiarity with serverless architectures (AWS Lambda) and PWA principles; \u00b7 Exposure to vector databases, data mesh or lakehouse architectures; \u00b7 Participation in GenAI POCs (RAG pipelines, Agentic AI demos); \u00b7 Passion for UI\/UX patterns, accessibility, and developer productivity; \u00b7 Client-facing experience in data-driven or AI\/ML projects; \u00b7 Ability to travel 10\u201330%; This is a Malaysia-based role collaborating closely with Singapore team","salary":"","work_type":"Kontrak\/Temporer","country":"singapore"}
{"Job_ID":"85068650","Role":"Associate, Full Stack Data Engineer (Singapore)","Company":"Nomura Singapore Limited","Location":"Singapore","Publish_Time":"2025-06-20 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/85068650","job_desc":"Job title:        Full Stack Data Engineer; Corporate Title:    Associate; Department:        Chief Data Office; Location:        Singapore; Company overview; Nomura is an Asia-based financial services group with an integrated global network spanning over 30 countries. By connecting markets East & West, Nomura services the needs of individuals, institutions, corporates and governments through its three business divisions: Retail, Asset Management, and Wholesale (Global Markets and Investment Banking). Founded in 1925, the firm is built on a tradition of disciplined entrepreneurship, serving clients with creative solutions and considered thought leadership. For further information about Nomura, visit www.nomura.com; Department overview:; The Chief Data Office plays a key role in defining and implementing the firm's data, cloud and AI strategy, driving change through these capabilities, enforcing data, cloud and AI governance for the firm, and elevating Nomura's data culture. Governance remains a critical focus area, and the Chief Data Office, in partnership with Business and Corporate functions, is responsible for ensuring that the firm's data assets are managed in line with the firm's data management framework, policy and standards.; Role Description:; Job Responsibilities:; \u2022    Information delivery & analytics. State-of-the-art expertise across, data\/information preparation, data insight & visualization using BI (or similar tools), and advanced data prediction using AI, ML, DL, etc.; \u2022    AI\/ML Ops. Responsible for integration, deployment and monitoring of AI\/ML products and solutions,; \u2022    Data management. Demonstrate expertise in data management to ensure the analytics products are appropriate\/ethical and well-controlled. Enabling data architecture and delivery of data-analytics platforms and Solutions- on-premises, cloud, and hybrid ensuring adherence and conformance to Nomura standards and policies; \u2022    Be a trusted partner. Shape the information & analytics agenda at Nomura, and work with all of Nomura\u2019s businesses in laying out their information & analytics adoption roadmaps.; \u2022    Risk Mindset: Familiar with risk and controls frameworks and ability to operate with a control mindset; Skills, experience, qualifications and knowledge required:; Core Skills requirement:; \u2022    Designing and developing scalable data pipelines to collect and process large volumes of data from multiple sources.; \u2022    Building physical data models and ETL processes to ensure data quality, integrity, and accessibility.; \u2022    Microservices Development: Building and maintaining highly scalable and fault tolerant microservice, including efficient server-side APIs.; \u2022    Deployment: Hands on with CI\/CD, Jenkins, Ansible, DevOps process, Enterprise integration patterns.; \u2022    Hands-on with programming languages (Python, SQL, Java, Unix scripting etc.) and with orchestration tools like Airflow or Autosys; \u2022    Experience with cloud technologies such as EC2, EMR, Snowflake or similar tools with ability to drive design and data model discussions, hybrid data architecture. ; \u2022    Proficiency in React with hands-on experience in UI development a plus.; \u2022    Ability to understand and integrate cultural differences and work effectively with virtual cross-cultural, cross-border teams.; \u2022    Flexibility to adjust to multiple demands, shifting priorities, ambiguity, and rapid change.; \u2022    Experience with senior stakeholder management will be an added advantage.; \u2022    Excellent communication (verbal, written, listening), presentation, and interpersonal skills.; \u2022    Able to analyze complex situations and derive workable actions.; \u2022    Able to constructively challenge requirements and current state to increase overall value to the firm.; Education and experience; Wide variety of degrees will be considered, however work experience will be of equal, if not greater importance; \u2022    At least 4-year Bachelor\u2019s degree in quantitative fields with minimum of 5 years of relevant data experience in data engineering \/ MLOps, full stack engineering, preferably in financial organizations or Masters in quantitative fields (Computer Science, Statistics or similar) ; \u2022    Experience of working with a multi-cultural, multi-disciplined, globally dispersed teams; \u2022    Certifications in relevant technologies or frameworks are a plus.; Diversity Statement; Nomura is committed to an employment policy of equal opportunities, and is fundamentally opposed to any less favourable treatment accorded to existing or potential members of staff on the grounds of race, creed, colour, nationality, disability, marital status, pregnancy, gender or sexual orientation.; ; DISCLAIMER:  This Job Description is for reference only, and whilst this is intended to be an accurate reflection of the current job, it is not necessarily an exhaustive list of all responsibilities, duties, skills, efforts, requirements or working conditions associated with the job.  The management reserves the right to revise the job and may, at his or her discretion, assign or reassign duties and responsibilities to this job at any time.  ;                                                                                                         Nomura is an Equal Opportunity Employer","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85478295","Role":"Data Engineer - Applied AI - Data Cycling Centre","Company":"TikTok Pte. Ltd.","Location":"Singapore","Publish_Time":"2025-07-04 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/85478295","job_desc":"Responsibilities; About the team The Machine Learning Engineering (MLE) team focused on the application of multimodal LLMs, unsupervised learning, and clustering algorithms. The ideal candidate will work closely with product, operations, and engineering teams to apply advanced natural language processing, computer vision, and deep learning technologies to solve business challenges and extract actionable insights. Responsibilities; Collaborate closely with data scientists, machine learning engineers, and software developers to build scalable data pipelines.; Support the development and deployment of machine learning models by ensuring high-quality, reliable data infrastructure.; Drive innovation in data processing, storage, and retrieval to optimize model training and inference.; Work in an agile environment focused on continuous integration and delivery of ML solutions.; Contribute to the design and implementation of data governance and security best practices.; Qualifications; Minimum Qualifications: 1. Bachelor\u2019s degree in Computer Science, Engineering, or a related field along with proven experience in data engineering, preferably within a machine learning or AI-focused team. 2. Strong proficiency in programming languages such as Python, Java, or Scala.; 3. Hands-on experience with big data technologies (e.g., Hadoop, Spark, Kafka) along with strong expertise in designing, building, and maintaining ETL\/ELT pipelines. 4. Familiarity with cloud platforms (AWS, GCP, or Azure) and their data services. 5. Solid understanding of database systems, both SQL and NoSQL.; 6. Experience optimizing performance in big data and fully understanding data skew along with familiarity with data governance, lineage, and real-time data processing practices. 7. Knowledge of machine learning workflows and data requirements along with excellent problem-solving skills and ability to work collaboratively in cross-functional teams.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"84356922","Role":"Senior Data Engineer","Company":"StarHub Ltd","Location":"Singapore","Publish_Time":"2025-06-18 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/84356922","job_desc":"JOB PURPOSE; The Data Platform Team is responsible for designing, implementing, and managing a modern data platform that embraces the principles of data mesh, empowering teams to create and manage their own data products. Our mission is to deliver high-quality, scalable data solutions that drive business value across the organization.; As a key member of this team, you will be responsible for building scalable, stable, and secure data pipelines that support both batch and streaming workloads. Your work ensures reliable data delivery across domains and supports the development of reusable, self-serve data products.; In this role, you will collaborate with business owners, engineers, and data stewards to implement ingestion frameworks and transformation jobs that align with the data-as-a-product vision. You will apply best practices in data engineering to enable efficient data integration across cloud and on-prem environments.; KEY RESPONSIBILITIES; Design and develop scalable, secure, and efficient data ingestion pipelines for structured and unstructured data from internal and external systems across AWS and on-prem environments.; Work closely with architects and business domain teams to translate data requirements into robust data pipelines and process workflows.; Design, build, and maintain real-time and batch data pipelines using Kafka, Spark Streaming, AWS EMR, Glue, Lambda, and other AWS services to; ingest and process high-frequency data from diverse internal and external sources.; Implement data partitioning, compaction, and optimization techniques to improve data processing performance and reduce cloud storage costs.; Assist in incident investigations, root cause analysis, and resolution of data pipeline failures or performance bottlenecks.; Document data flow designs, ingestion standards, and transformation logic clearly for use by other engineers, data stewards, and auditors.; QUALIFICATIONS; Required:; Minimum 2 years of experience (or 5 years for a senior position) in Data Engineering, Software Engineering or related fields.; Proven experience building and managing real-time and batch data pipelines on AWS using services such as EMR, Glue, Lambda, S3, and EC2.; Strong knowledge of Python and Spark, with hands-on experience designing low latency ETL\/ELT pipelines.; Experience handling large-scale datasets and optimizing cloud storage formats and query performance.; Familiarity with infrastructure components such as IAM roles, Security Groups, and VPC networking to support secure data access and movement.; Hands-on experience with Linux environments, shell scripting, and AWS CLI for managing data and computation resources.; Strong communication and collaboration skills to work across data, engineering, and network teams.; Ability to maintain clean, structured documentation of ingestion logic, transformation steps, and data flow dependencies.; Preferred:; Certifications in cloud technology platforms (such as cloud architecture, container platforms, systems, and\/or network virtualization).; Knowledge of telecom networks, including mobile and fixed networks, will be an added advantage.; Familiarity with data fabric and data mesh concepts, including their implementation and benefits in distributed data environments, is a bonus.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"82255060","Role":"Data Architect (Snowflake\/Microsoft Fabric), AI & Data, Technology Consulting","Company":"Ernst & Young Solutions LLP","Location":"Singapore","Publish_Time":"2025-06-17 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/82255060","job_desc":"At EY, we develop you with future-focused skills and equip you with world-class experiences. We empower you in a flexible environment, and fuel you and your extraordinary talents in a diverse and inclusive culture of globally connected teams.; We work together across our full spectrum of services and skills powered by technology and AI, so that business, people and the planet can thrive together. ; We\u2019re all in, are you?; Join EY and shape your future with confidence.; About the opportunity; EY AI & Data is the data and advanced analytics capability within EY Asia-Pacific, with over 500 specialist employees working across multiple industry sectors. We implement information-driven strategies, data platforms and advanced data analytics solution systems that help grow, optimize and protect client organizations. We go beyond strategy and provide end to end design, build and implementation of real-life data environments and have some of the best architects, project managers, business analysts, data scientists, big data engineers, developers and consultants in the region. ; We are seeking a skilled Data Architect with extensive experience in Snowflake and\/or Microsoft Fabric to join our dynamic team. The ideal candidate will be responsible for designing, implementing, and managing data architecture solutions that meet the needs of our organization. You will work closely with data engineers, analysts, and other stakeholders to ensure that our data systems are robust, scalable, and aligned with business objectives.; Key Responsibilities:; Design and implement data architecture solutions using Snowflake and\/or Microsoft Fabric.; Collaborate with cross-functional teams to gather requirements and translate them into technical specifications.; Develop and maintain data models, data flow diagrams, and other architectural documentation.; Ensure data integrity, security, and compliance with industry standards and regulations.; Optimize data storage and retrieval processes for performance and cost efficiency.; Monitor and troubleshoot data architecture issues, providing timely resolutions.; Stay updated with the latest trends and technologies in data architecture and cloud computing.; Mentor and guide junior team members in best practices for data architecture and management.;   Qualifications:; Bachelor\u2019s degree in Computer Science, Information Technology, or a related field; Master\u2019s degree preferred.; Proven experience as a Data Architect or similar role, with a strong portfolio of successful projects.; Extensive experience with Snowflake and\/or Microsoft Fabric, including data modeling, ETL processes, and data warehousing.; Proficiency in SQL and experience with data integration tools.; Strong understanding of data governance, data quality, and data security principles.; Excellent analytical and problem-solving skills.; Strong communication and collaboration skills, with the ability to work effectively in a team environment.; Preferred Skills:; Experience with other cloud platforms (e.g., AWS, Azure, Google Cloud).; Familiarity with big data technologies (e.g., Hadoop, Spark).; Knowledge of machine learning and data analytics concepts.; What working at EY offers; EY offers a competitive remuneration package where you\u2019ll be rewarded for your individual and team performance. We are committed to being an inclusive employer and are happy to consider flexible working arrangements. Plus, we offer:; Continuous learning: You\u2019ll develop the mindset and skills to navigate whatever comes next.; Success as defined by you: We\u2019ll provide the tools and flexibility, so you can make a meaningful impact, your way.; Transformative leadership: We\u2019ll give you the insights, coaching and confidence to be the leader the world needs.; Diverse and inclusive culture: You\u2019ll be embraced for who you are and empowered to use your voice to help others find theirs.; Company description; EY is building a better working world by creating new value for clients, people, society and the planet, while building trust in capital markets.; Enabled by data, AI and advanced technology, EY teams help clients shape the future with confidence and develop answers for the most pressing issues of today and tomorrow.; EY teams work across a full spectrum of services in assurance, consulting, tax, strategy and transactions. Fueled by sector insights, a globally connected, multi-disciplinary network and diverse ecosystem partners, EY teams can provide services in more than 150 countries and territories.; All in to shape the future with confidence.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85543032","Role":"Senior Data Solutions Architect","Company":"Systems on Silicon Manufacturing Co Pte Ltd","Location":"Singapore","Publish_Time":"2025-07-07 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/85543032","job_desc":"SSMC (Systems on Silicon Manufacturing Company Pte. Ltd.), is a Joint Venture between NXP and TSMC. We offer flexible and cost effective semiconductor fabrication solutions by maintaining fully equipped SMIF cleanroom environment, 100% equipment automation and proven wafer-manufacturing processes.; At SSMC, every career journey is unique and rewarding. We're looking for innovative, passionate, and talented people like you to join our team.; We\u2019re searching for a Senior Data Solutions Architect to be part of our IT Department diverse team of talent. You will be responsible in Engineering Analytic Systems Software development, Design and Support. This is a 1-year contract role.; What you will be working on:; Design and architect scalable data lakehouse and data warehouse solutions. ; Implement and manage data storage formats and table formats using Apache Iceberg and Apache Hudi. ; Integrate and optimize Hadoop-based data processing pipelines. ; Deploy and manage MinIO for object storage on premise or cloud-native environments. ; Collaborate with data engineers, analysts, and business stakeholders to define data models and governance strategies. ; Ensure data quality, lineage, and security across the data lifecycle. ; Evaluate and recommend emerging technologies and tools in the data ecosystem. ; Collaborate with cross-functional teams to understand requirements and provide technical guidance. ; Implement Service Request related to internal applications ; Provide support for Internal IT department and External customers ; More About You:; Minimum 5 years good experiences in design and implementation of ETL framework for data lakehouse and data warehouse or related projects ; Proven experience as a Data Architect or similar role in large-scale data environments. ; Deep understanding of Apache Iceberg and Apache Hudi for managing large-scale datasets. ; Hands-on experience with Hadoop ecosystem (HDFS, YARN, Hive, Spark, etc.). ; Proficiency in deploying and managing MinIO for high-performance object storage. Excellent troubleshooting skills.; Strong knowledge of data modeling, ETL\/ELT processes, and data governance. ; Familiarity with cloud platforms (AWS, Azure, GCP) and containerization (Docker, Kubernetes) is a plus. ; Excellent problem-solving, communication, and collaboration skills. ; Working knowledge of semiconductor industry ; Experience with BI reporting such as tableau or PowerBI or equivalent will be advantageous ; Good team player with pro-activeness and good initiatives Must be able to work independently.; Good communication and interpersonal skill.; SSMC is committed to equal employment opportunities and abides by the Tripartite Guidelines on Fair Employment Practices (TGFEP). All qualified applicants will receive non-discriminatory consideration for employment on the basis of merit and regardless of age, race, gender, religion, marital status and family responsibilities, or disability, or any other attributes as protected by the relevant laws.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"84522772","Role":"Site Reliability Engineer - Data Management Suite","Company":"TikTok Pte. Ltd.","Location":"Singapore","Publish_Time":"2025-06-17 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/84522772","job_desc":"Responsibilities; The Data Management Suite team is building products that cover the whole lifecycle of data pipeline, including data ingestion and Integration, data development, data catalog, data security and data governance. These products support various businesses, so data engineers and data scientists could greatly boost their productivity. As a software engineer in the data management suite team, you will have the opportunity to build, optimize and grow one of the largest data platforms in the world.; You'll have the opportunity to gain hands-on experience on core systems in the data platform ecosystem. Your work will have a direct and huge impact on the company's core products as well as hundreds of millions of users. Responsibilities:; Be responsible for the production stability for big data development and governance systems; Engage in and improve the whole lifecycle of service, from inception and design, through to deployment, operation and refinement; Maintain services once they are live by measuring and monitoring availability, latency and overall system health.; Practice sustainable incident response and blameless postmortems; Establish best engineering practice for engineers as well as non-technical people; Design and implement reliable, scalable, robust and extensible big data systems that support core products and business;; Qualifications; Minimum Qualifications; Bachelor's degree in Computer Science, a related technical field involving software or systems engineering, or equivalent practical experience; Experience with site reliability engineering, monitoring, alerting for big data related systems; Experience writing code in Java, Go, Python or a similar language; Preferred Qualifications; Knowledge about a variety of strategies for ingesting, modeling, processing, and persisting data, ETL design, job scheduling and dimensional modeling;; Familiarity with running production grade services at scale and understanding cloud native technologies and networking;; Experience developing tools and APIs to reduce human interaction with systems and applications using a variety of coding and scripting standards;; Expertise in designing, analyzing, and troubleshooting large-scale distributed systems is a plus (Hadoop, M\/R, Hive, Spark, Presto, Flume, Kafka, ClickHouse, Flink or comparable solutions);; Systematic problem-solving approach, coupled with effective communication skills and a sense of drive;","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85655720","Role":"Engineering, Data Stewardship, Product Data Engineering, Executive Director,...","Company":"Goldman Sachs","Location":"Central Region","Publish_Time":"2025-07-10 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/85655720","job_desc":"YOUR IMPACT; Are you looking to apply your quantitative skills while deepening your understanding across a vast array of products? Product Data Engineering is a high-value risk management and engineering team under Chief Data Officer of the firm in Engineering Division, is responsible for the governance programs that ensure critical securities modelling and pricing data is accurate, complete, and scalable. This data is a critical input and dependency to most trade and client support functions. Product Data Engineering Stewards are part of Data Engineering and is comprised of teams, based in Salt Lake City, Bengaluru, Hyderabad and Singapore.; OUR IMPACT; Engineering is a dynamic, multi-faceted division that partners with all parts of the firm to deliver banking, sales and trading and asset management capabilities to clients around the world. Data Stewards ensure completeness, accuracy, and suitability of their assigned data sets by applying subject matter expertise to make risk based judgement to maintain data. Data Stewards design and execute proactive controls to mitigate risks by detecting and correcting anomalies; they also design solutions to meet new commercial objectives in a way that complies with underlying governance models. They ensure broad understanding of their data models, content and requirements across internal and external data producers, consumers, developers and project managers. Data Stewards translate requirements from stakeholders to help set development priorities, and test developed solutions before they are released into production. This gives them exposure to every business and product types on a global basis. The role requires the team to work closely with each of our supported business units to ensure their data quality needs are met currently, and that their reference data requirements grow as their business evolves.; HOW WILL YOU FULFILL YOUR POTENTIAL; Being a self-starter is essential; Strong negotiation and relationship skills to bring Engineering and Business project sponsors and stakeholders to the table to build complex solutions while balancing competing priorities; Manage team of individual involved in investigating and resolving product and pricing data exceptions and maintaining product data; Proactively solve data quality issues from both tactical and strategic perspectives; Proactively work with market data and external vendors (e.g. Bloomberg, LSEG, S&P) to ensure we have the highest quality data in our systems; The product team covers data related to:; Equities: Common Stock, Preferred Stock, Rights, Warrants, Exchange traded funds, ADRs, GDRs; Fixed income bonds: Corporate, Government, Agency, Emerging Market, Convertible, Structured notes; Listed derivatives: Options, Future Options, Option Baskets, Flex Options, Futures, Flex Futures; Other products: Mutual Funds, Indices, Baskets, Fiduciary Notes, Credit Indices, OTC Derivatives; About the Role: We are seeking a highly skilled and experienced Executive Director to lead our Data Stewardship efforts within the Product Reference Data team. The ideal candidate will have a strong background in data management, data quality, and resolving downstream consumer issues. This role is critical in ensuring the integrity, accuracy, and usability of our product reference data across the organization; Key Responsibilities:; Manage a team of data stewards and analysts, providing guidance and mentorship; Develop and implement data quality standards, policies, and procedures; Oversee the resolution of data-related issues impacting downstream consumers; Collaborate with cross-functional teams to ensure data consistency and accuracy; Monitor and report on data quality metrics and KPI; Drive continuous improvement initiatives to enhance data quality and stewardship practices; Ensure compliance with data governance and regulatory requirements; Act as a subject matter expert on data quality and stewardship; Proactively solve data quality issues from both tactical and strategic perspectives; Develop effective relationships with key market data and external vendors (e.g., Bloomberg, Reuters, S&P) to ensure the highest quality data is available to our systems; Ensure broad understanding of data models, content, and requirements across internal and external data producers, consumers, developers, and project managers; Translate requirements from stakeholders to help set development priorities; Work directly with engineers to develop, test and deliver solutions; Required Skills and Experience:; Bachelor's degree in Computer Science, Information Systems, Data Management, or a related field; Minimum of 8 years of experience in data management, data quality, or data governance roles; Proven track record of leading data stewardship initiatives and resolving data-related issues; Strong understanding of data governance frameworks and best practices; Excellent analytical and problem-solving skills; Proficiency in data management tools and technologies (e.g., SQL, ETL, data profiling tools); Experience with product reference data and its impact on downstream systems; Strong leadership and team management skills; Excellent communication and collaboration skills; Highly motivated with risk-based judgment skills; Ability to communicate complex concepts across diverse groups; Demonstrated ability to prioritize a diverse set of work tasks and work well under pressure to meet deadlines; Effective management of internal stakeholders and external clients through relationship management and high standard of service; Preferred Qualifications:; Experience in the financial services industry; Knowledge of regulatory requirements related to data management; Certification in data management or data governance (e.g., CDMP, DGSP); Risk based judgement \u2013 reviews problems and can research appropriately across systems to determine next steps including appropriate escalation; Communication of ambiguous concepts \u2013 demonstrated ability to describe complex concepts across groups that do not always use the same jargon; General communication \u2013 essential for interacting at all levels, across business disciplines and regions; Workflow Prioritization \u2013 demonstrated ability to appropriately prioritize a diverse set of work tasks, takes ownership of completing tasks and works well under pressure to meet deadlines; Technical ability \u2013 demonstrates ability to grasp complex processes and workflows; Teamwork \u2013 works well within a team of diverse individuals and high standard of execution and delivery towards common goals and objectives; Client service \u2013 demonstrates effective management of internal stakeholders and\/or external clients through effective relationship management and high standard of service; ABOUT GOLDMAN SACHS; At Goldman Sachs, we commit our people, capital and ideas to help our clients, shareholders and the communities we serve to grow. Founded in 1869, we are a leading global investment banking, securities and investment management firm. Headquartered in New York, we maintain offices around the world.; We believe who you are makes you better at what you do. We're committed to fostering and advancing diversity and inclusion in our own workplace and beyond by ensuring every individual within our firm has a number of opportunities to grow professionally and personally, from our training and development opportunities and firmwide networks to benefits, wellness and personal finance offerings and mindfulness programs. Learn more about our culture, benefits, and people at GS.com\/careers.; We\u2019re committed to finding reasonable accommodations for candidates with special needs or disabilities during our recruiting process. Learn more: https:\/\/www.goldmansachs.com\/careers\/footer\/disability-statement.html; \u00a9 The Goldman Sachs Group, Inc., 2024. All rights reserved.; Goldman Sachs is an equal opportunity employer and does not discriminate on the basis of race, color, religion, sex, national origin, age, veterans status, disability, or any other characteristic protected by applicable law.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85482964","Role":"Senior Data Test Engineer","Company":"Tech Mahindra","Location":"Changi","Publish_Time":"2025-07-04 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/85482964","job_desc":"KEY ACCOUNTABILITIES; Maintain and adopt Agile best practices and lifecycles for process workflows (e.g., Kanban, CI\/CD); Collaborate with business users and business analysts to refine and understand both functional and non-functional requirement during SIT & UAT stages.; Develop automated test scripts to validate functional and technical requirements in data processing pipeline and to perform data quality checks; Collaborate with data analysts in profiling data and monitoring data trends; Collaborate with Developers\/DevOps Engineers on code management, peer review, continuous integrated testing in CI\/CD pipelines; Assure quality at different phases of SDLC by adhering to process and strategies defined by Eastspring IT; Execute manual \/ automated \/ exploratory tests and provide QA sign-off to business users for releases; Maintain test process, design and execution artifacts in test management system complying the audit regulations; Prepare testing traceability reports and other testing metrics; QUALIFICATIONS \/ EXPERIENCE; Recognized degree or higher in Computer Science or related Engineering fields.; At least 8 years of working experience in Test Automation, using test frameworks for Database (ETL Testing) and Data analytical testing.; Working knowledge in testing Data management platform tools same-as\/similar-to \u2018Golden Source\u2019; Sound knowledge in Java programming, SQL queries and Cucumber (Java) testing framework.; Good knowledge in testing scheduling\/orchestration tools (like Control-M, Azure Data Factory); Working knowledge of relational databases and comfortable with testing SQL jobs and stored procedures with awareness of data security.; Basic understanding of data quality, profiling, and analytics concepts.; Working experience with test management tools such as Jira with Xray \/ Zephyr; Working knowledge of tools such as bitbucket, Jenkins, confluence and familiar with Git branching model; Working experience in Agile projects, Behavior Driven Development (BDD) approach to software development and testing.; Good to have basic programming knowledge in Python; Good to have knowledge of Azure cloud platform.; Good to have working experience in investment Bank or Asset Management industry.; OTHER TRAITS; Positive attitude and collaborative mindset.; Willing to work across projects and perform manual \/ automation \/ exploratory testing; Highly motivated to keep abreast with the latest development in technology and to acquire deep technical knowledge and skills.; Excellent communication, presentation, and interpersonal skills.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"84983757","Role":"Sales - Software Engineer (Data), PART","Company":"Apple Inc.","Location":"Singapore","Publish_Time":"2025-06-17 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/84983757","job_desc":"Summary; Posted: Jun 16, 2025; Weekly Hours: 38; Role Number:200608513; Imagine what you could do here. The people at Apple don\u2019t just create products \u2014 they create the kind of wonder that\u2019s revolutionized entire industries. It\u2019s the diversity of those people and their ideas that inspires the innovation that runs through everything we do. Bring passion and dedication to your job and there's no telling what you could accomplish! Apple's Finance Process, Analytics, Reporting, and Technology (PART) team is looking for a passionate and highly motivated Software Engineer. As part of this team, you will support Apple\u2019s growth, both top and bottom line, by applying the same level of innovation toward financial matters as we do toward our products and services. The PART team is seeking an experienced Software Engineer to build high quality, scalable and resilient distributed systems that power Apple's cloud analytics platforms and data pipelines. Apple's Enterprise Data Warehouse landscape caters to a wide variety of real-time, near real-time and batch analytical solutions. These solutions are an integral part of business functions like Sales, Operations, Finance, AppleCare, Marketing and Internet Services, enabling business drivers to make critical decisions. We use proprietary and open source technologies, Kafka, Spark, Iceberg, Airflow, Presto, etc. If you are looking to tackle infrastructure problems at scale, both on-prem or in cloud, focusing on ease of use, ease of maintenance and most importantly implement solutions that are scalable, you will enjoy working in PART!; Description; We engineer high-quality, scalable and resilient distributed systems on cloud that power data exploration, analytics, reporting and production models. Our core systems are diverse and come with an unusual intersection of high data volumes with systems distributed across cloud and on-premise infrastructure. This role will build solutions that integrate open source software with Apple\u2019s internal ecosystem. You will drive development of new components and features from concept to release: design, build, test, and ship at a regular cadence. You will work closely with internal customers to understand their requirements and workflows, and propose new features and ecosystem changes to streamline their experience of using the solutions on our platform. This is a challenging software engineering role, where a large part of an engineer's time is spent in writing code and designing\/developing applications on cloud, with the remainder being spent on tuning and debugging codebase, supporting production applications and supporting our application end users. This role requires in-depth knowledge of innovative technologies and cloud data platform with the ability to independently learn new technologies and contribute to the success of various initiatives.; Minimum Qualifications; 4 or more years of experience building enterprise-level data applications on distributed systems; Knowledge of BI concepts and Implementation experience on Cloud with databases like SnowFlake or Big Query; Programming experience with Python, Scala or Java.; Experience in developing highly optimized SQLs, procedures & semantic process for distributed data applications; Bachelor\u2019s degree in Computer Science or equivalent experience; Preferred Qualifications; Hands-on experience in designing and development of cloud-based applications that include compute services, database services, APIs to design RESTful services, ETL, queues and notification services.; Experience in cloud data warehousing platforms like Snowflake is highly valued; Hands-on knowledge of Spark cluster-computing framework & Kubernetes or similar containerization technologies.; Experience developing Big Data applications using Java, Spark, Kafka is a huge plus; Understanding of fundamentals of object-oriented design, data structures, algorithm design, and problem solving; Cloud technology experience on platforms like AWS, Microsoft Azure, Google Cloud; Data Visualization Tools: experience in software such as Streamlit, Superset, Tableau, Business Objects, and Looker; Data Insights and KPIs: Working experience on generating and visualizing data insights, metrics, and KPIs. Usage of basic ML models in the space of anomaly detection, forecasting, GenAI.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"84791746","Role":"VP, Data Quality & Governance, Data Management Office","Company":"United Overseas Bank Limited (UOB)","Location":"Singapore","Publish_Time":"2025-06-17 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/84791746","job_desc":"United Overseas Bank Limited (UOB) is a leading bank in Asia with a global network of more than 500 branches and offices in 19 countries and territories in Asia Pacific, Europe and North America. In Asia, we operate through our head office in Singapore and banking subsidiaries in China, Indonesia, Malaysia and Thailand, as well as branches and offices.; Our history spans more than 80 years. Over this time, we have been guided by our values \u2013 Honorable, Enterprising, United and Committed. This means we always strive to do what is right, build for the future, work as one team and pursue long-term success. It is how we work, consistently, be it towards the company, our colleagues or our customers.; Data Management Office (DMO) ; DMO believes that data is key to responsible innovation, growth, and competitive advantage in the banking industry. Our mission is to leverage our data assets to create value for our customers, stakeholders and employees. We achieve this by defining, and executing to, enterprise data strategies and transformation, delivering valuable and trusted data to the organisation via the most efficient and responsible manner, ensuring robust data governance via strong enforcement, literacy, and management of emerging data risks and data regulatory compliance, as well as bringing Artificial Intelligence (\u201cAI\u201d) solutions to our business stakeholders to drive monetisation of our data assets. ; This role is a key part of the Enterprise Data Governance Team, supporting the Head of Enterprise Data Governance in shaping and delivering data quality initiatives across the Group, aligned with the Bank\u2019s strategy. In this role, you will work closely with Business, Support, and Technology teams to initiate projects, gather requirements, design solutions, and deliver on agreed project outcomes within stipulated timelines.; Your main responsibilities are to:; Support the UOB Group-wide Data Governance strategy and the adoption of the data governance framework, policy and processes; incorporating best practices and industry standards across different jurisdictions where the Bank operates; Drive the adoption and improvement of operational governance of enterprise data, through activities such as defining data quality requirements, guiding data lineage exploration and designing data quality monitoring mechanisms and related controls; Project management and execute Critical Data Monitoring initiatives via Project Steering Committee and Project Working Groups; Develop strong partnerships with key internal and external parties ensuring that project expectations and data governance expectations are understood and achieved  ; Perform data profiling using SQL and\/or Python on large datasets to identify potential data quality issues and create intelligent data quality rules; Validate and document technical implementation of data quality rules and processes to ensure data quality assurance activities are fit for requirement with users; Execute technical analysis on data management processes including root cause analysis and data lineage tracing, working with relevant stakeholders across the data lifecycle; Actively support remediation coordination including tactical and strategic solutioning of data quality issues; Define, socialize, seek endorsement, and maintain key Data Governance artefacts and processes including: frameworks, policies, standards, guidelines and target operating models; Participate in the development of data standards incorporating industry best practices across Data Quality, Metadata, Reference data and Master data; Be an agent of change and inculcate data governance culture and discipline across the Bank\u2019s business and technical processes; Degree in Computer Science, Information Technology, Business Computing or related disciplines with at least 8-10 years of working experience in the banking industry, or consulting industry with strong exposure to Financial Services; Extensive knowledge and hands-on experience within Data Governance discipline including data governance frameworks (e.g. DAMA-DMBOK), data stewardship\/ownership models and data quality management; Knowledge of data governance industry best practice and regulatory requirements such as BCBS 239, MAS 626, AML\/KYC, FATCA, GDPRA etc.  ; Strong communication and stakeholder engagement skills, with experience in project management or change management; Strong technical knowledge in relation to Python, SQL, Big Data Platforms, ETL Tools and Metadata tools; Evidenced project delivery with cross functional stakeholders from both technical and business domains; Good understanding of data management and related disciplines, such as data architecture, data quality and metadata; Professional certification in data governance (e.g., DAMA CDMP, Collibra, DGSP, CIMP), risk management (e.g., GARP, PRIMIA) are a plus; Demonstrated ability as team player and self starter, ability to conduct initiatives independently.; UOB is an equal opportunity employer. UOB does not discriminate on the basis of a candidate's age, race, gender, color, religion, sexual orientation, physical or mental disability, or other non-merit factors. All employment decisions at UOB are based on business needs, job requirements and qualifications. If you require any assistance or accommodations to be made for the recruitment process, please inform us when you submit your online application.; ; Apply now and make a difference.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85816706","Role":"Associate Data Engineer - ETL (Engineering & Ops)","Company":"Synapxe","Location":"Singapore","Publish_Time":"2025-07-16 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/85816706","job_desc":"Bachelor's degree in Computer Science, Information Technology, or a related field; At least 4 years of experience in the IT industry, including:; a. Development, implementation, and maintenance of IT systems, preferably in Data Warehousing, ETL rules, data  modeling, and BI applications; b. Operations support and business analysis experience.; c. Strong MS-SQL and Oracle Database scriptin; Experience in diagnosing, troubleshooting, and performing root cause analysis; Ability to diagnose and troubleshoot problems with BI reports and ETL processes; Experience with AWS, Data Lake, Databricks, and the healthcare domain is a plus; Able to work independently and as an effective team player with a strong desire to deliver results; Adaptable, meticulous, and possess strong analytical skills; Good communication skills (both written and spoken); Strong team player","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85845477","Role":"Data Engineer","Company":"TOPPAN Ecquaria Pte Ltd","Location":"Braddell","Publish_Time":"2025-07-17 03:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/85845477","job_desc":"Responsibilities:; Design and implement robust, scalable data pipelines and architectures to support data ingestion, processing, and storage. Including performance optimizations for data modeling and ingestion; Develop and optimize complex SQL queries and stored procedures for data extraction, transformation, and analysis.; Model data to meet different use casesng applications and automate data workflows.; Collaborate with data scientists and analysts to understand data requirements and deliver high-quality data solutions.; Lead the integration of data from various sources into data lakes and warehouses, ensuring data quality and consistency.; Monitor and troubleshoot data pipelines and workflows to ensure optimal performance and reliability.; Communicate with and support data users; Document data processes, data models, and architectural designs to ensure knowledge sharing and compliance with best practices.; Prerequisites:; Experience: Minimum 3 years in data engineering fields with system integration, and at least 1 year in system integration and implementation in cloud\/web-based environments.; Proven Solutions: Demonstrated experience in providing effective, working solutions and implementations, particularly in cloud-based environments.; Technical Skills:; Solid understanding of ETL processes, data warehousing concepts, and data modeling best practices.; Proficiency in Databricks, Azure Data lake, PowerBI, Tableau and related data processing and visualisation software.; Familiarity with Windows, Linux, AWS and\/or Azure platforms.; Strong programming skills in languages such as Python and R is a must; Proficiency in other programming languages such as Java, Scala and C# will be advantages; Experience in data processing frameworks (e.g., Apache Spark, Apache Flink); Preferred Exposure:; Experience with large-data management system with visualisation tools.; Experience with Data Integration and ETL Pipelines, Data Warehousing and BI reporting projects.; Experience with Singapore Government Project will be advantages; Personal Attributes:; Excellent problem-solving skills; Ability to work independently; Collaborative in a fast-paced environment; Why Join Us?; Be part of a forward-thinking team that is transforming government digital services. If you are passionate about technology and innovation, and thrive in a dynamic environment, we want to hear from you!; If you are passionate about building partnerships and driving growth, we would love to hear from you!; TOPPAN Ecquaria is an equal opportunity employer and values diversity within our company. We welcome all interested candidates to apply for this position, however, we regret to inform that only shortlisted candidates will be contacted by us for an interview.; Find us at www.topppanecquaria.com or www.linkedin.com\/company\/toppan-ecquaria; For more career opportunities, please visit our career site at:- https:\/\/toppanecquaria.com\/careers\/job-openings?utm_source=Jobstreet&utm_medium=Page&utm_campaign=2020 (Please copy & paste the above link onto your browser)","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85783746","Role":"Data Engineer (Informatica\/Python)","Company":"ASTEK Singapore Innovation Technology Pte. Ltd.","Location":"Central Region","Publish_Time":"2025-07-15 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/85783746","job_desc":"Job Title: Data Engineer - Mid-level\/Senior; Location: Singapore (CBD); Job Type: Full-Time Permanent; Key Responsibilities:; Design and maintain scalable ETL pipelines using Informatica.; Optimize SQL queries and stored procedures for data manipulation.; Lead data migration projects ensuring data integrity and quality.; Collaborate with business teams to create data solutions.; Integrate data into .NET applications.; Qualifications (MUST HAVE):; Min 4 years of working experience in data engineering.; Proficient in Informatica.; Strong knowledge of Python.; Solid SQL skills, including experience with Stored Procedures.; Proven experience in data migration and engineering.; Experience with .Net is highly advantageous.; *Apply only if you meet all the must-have requirements.*","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85187400","Role":"ETL \/ SSIS Data Engineer","Company":"Goldtech Resources Pte Ltd","Location":"Central Region","Publish_Time":"2025-06-26 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/85187400","job_desc":"ETL \/ SSIS Data Engineer (Contract); Location In Singapore: Central; Salary(SGD): 5500; Apply; Job Highlights:; Developer will perform the development and L2\/L3 support activities for the DataMart system and the corresponding reporting system using C#.NET, SQL Server and SSIS\/SSRS; Job Description:; Follow the organizational SDLC processes to deliver the project or enhancement requests, create\/update the SDLC documents including functional and non-functional specifications, technical design documents, test plan, test cases, release procedures, system operational documents, user manuals, etc.; Development of the task based on the written Technical Specifications; Write out Technical Specifications based on Functional Specifications provided by client; Able to estimate effort of tasks based on Technical Specifications and Functional Specifications; Perform the impact analysis; Provide ad-hoc support for other IT service requests, e.g., data extraction, data alteration, extract system logic, answer to users' inquiry about the data\/logic in the system, etc.; \u00b7     ; Requirements:; Recognized university degree or Masters in Information Technology, Computer Science, Manage Information Science, Banking and Finance or equivalent; 4 years or more working experience in SSIS, SSRS and SQL Server development,; Familiar with Software Development Life Cycle (SDLC) process; Experience on business requirement gathering, functional review and feasibility studies; Sound problem solving ability and work well in a challenging technical environment, with capability to multi-task; Strong communication skill; Proficient in SQL Server development, able to construct complex Stored Procedures, User-Defined Functions, Triggers,etc.; In-depth understanding of database management systems, online analytical processing (OLAP) and ETL (Extract, transform, load) framework; Familiar with SQL Server installation and configuration, able to analyze and fine-tune SQL Query performance; Strong influencing skill on technical solution and software design; Strong ownership and proactive working attitude; Positive attitude and disposition; Ability to function independently or as a team player; Technical aptitude; Organized, systematic, logical; Pay attention to details; Apply","salary":"","work_type":"Kontrak\/Temporer","country":"singapore"}
{"Job_ID":"85578382","Role":"DATA ENGINEER","Company":"Continental Technology Solutions Pte Ltd","Location":"Kampong Ubi","Publish_Time":"2025-07-07 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/85578382","job_desc":"Responsibilities:; Design, implement, and maintain scalable and reliable data pipelines.; Build and optimize data architectures (data lakes, data warehouses, ETL\/ELT processes).; Collaborate with data scientists, analysts, and other stakeholders to understand data needs.; Develop data models and structures for reporting and analytics.; Ensure data quality, integrity, security, and compliance with governance policies.; Monitor and troubleshoot performance issues with data systems.; Automate data workflows using modern orchestration tools.; Document processes, systems, and data flows.; Requirements:; Bachelor's degree in Computer Science, Engineering, or a related field.; 2+ years of experience in a data engineering or similar role.; Strong SQL skills and experience with relational databases (e.g., PostgreSQL, MySQL).; Proficiency in Python, Scala, or Java for data manipulation and pipeline development.; Experience with ETL\/ELT tools (e.g., Apache Airflow, dbt, Talend).; Familiarity with data warehouse solutions (e.g., Snowflake, Redshift, BigQuery).; Experience with cloud platforms (AWS, GCP, or Azure) and their data services.; Understanding of data modeling, data governance, and best practices.","salary":"","work_type":"Kontrak\/Temporer","country":"singapore"}
{"Job_ID":"85823545","Role":"Data Engineer","Company":"Innowave Tech Pte Ltd","Location":"Paya Lebar East","Publish_Time":"2025-07-17 01:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/85823545","job_desc":"About Innowave Tech Singapore ; Innowave Tech is an Artificial Intelligence (AI) company offering solutions for the Semiconductor and Advanced Manufacturing industry. Utilizing deep industrial domain knowledge, proven experience, and innovation, we provide expert AI solutions and systems to address various industry pain points. ;  Roles & Responsibilities ; We are seeking Data Engineer to establish and lead our data infrastructure. The successful candidate will be responsible for building our data engineering practice from the ground up, implementing robust data systems for industrial AI applications, and establishing best practices that will power our semiconductor manufacturing AI solutions. ;  Your Role and Impact ; As our first Data Engineer, you will have a foundational role in building robust data infrastructure to handle manufacturing data and LLM applications, while establishing secure data practices that power our AI solutions for advanced manufacturing operations. ;  What You\u2019ll Do ; Select and manage on-premises technologies suitable for secure and efficient operations. ; Build robust pipelines to collect, clean, and transform diverse datasets including process data, sensor data, image data, and human annotations. ; Ensure secure, maintainable, and scalable deployment of data infrastructure. ; Define and enforce best practices in data governance, privacy, and access control. ; Collaboration & Deployment. ;  What We\u2019re Looking For ; Educational Background: ; Minimum Poly or Bachelor Degree in Computer Science, Engineering, or a related field. ; * We welcome applications from Singapore Citizens, Permanent Residents (PRs), Malaysians, and local graduates bonded for local employment, in accordance with MoM regulations.;  Technical Expertise: ; 3+ years of experience in data engineering roles, ideally with on-premises or hybrid infrastructure. ; Proven track record of building scalable data systems from ground up in a startup environment. ; Proficiency in Python and\/or Java for data pipeline development. ; Solid experience with ETL frameworks (e.g., Apache Airflow, Dagster) and streaming systems (e.g., Kafka). ; Experience designing and maintaining SQL and NoSQL databases. ; Experience building and operating data lakes and data catalog. ; Familiarity with containerization (Docker), version control (Git), and CI\/CD practices. ;  Soft Skills: ; Excellent communication skills and ability to collaborate with cross-functional technical and non-technical teams. ; Excellent problem-solving and debugging abilities. ; Ability to balance engineering tradeoffs. ;  Bonus Skills: ; Experience with manufacturing data systems, especially SPC, SCADA, and industrial sensor protocols (e.g., OPC UA, MQTT, Modbus). ; Familiarity with AI\/ML pipelines and tools (e.g., MLflow). ; Knowledge in vector databases and LLM data infrastructure. ; Prior experience working in or with regulated industries (e.g., semiconductor, automotive, aerospace). ; What we Offer ; \u2022 A leading role in cutting-edge AI projects within the semiconductor industry. ; \u2022 The opportunity to work with an learn from experts in the field of AI and data science. ; \u2022 A dynamic, innovative, and supportive work environment. ; \u2022 Competitive salary and benefits package. ; \u2022 Career growth opportunities in a fast-paces technology company.","salary":"$5,333 \u2013 $8,000 per month (SGD)","work_type":"Full time","country":"singapore"}
{"Job_ID":"85751373","Role":"Snowflake Data Engineer","Company":"Blue Ocean Systems Infotech Lte Ltd","Location":"Central Region","Publish_Time":"2025-07-14 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/85751373","job_desc":"Hi Immediate hiring; Job Overview:; We are seeking a highly skilled Snowflake Data Engineer to join our data engineering team. The ideal candidate will have hands-on experience with Snowflake, strong SQL skills, and a solid understanding of cloud-based data warehousing, ETL processes, and data modeling. The role will focus on designing, developing, and optimizing scalable data pipelines using Snowflake and other modern data stack tools.; Key Responsibilities:; Design and implement scalable and efficient data pipelines using Snowflake.; Develop and maintain ETL\/ELT workflows to ingest, transform, and deliver clean data.; Optimize Snowflake performance (query tuning, resource management, clustering).; Collaborate with data analysts, data scientists, and business stakeholders to gather requirements and deliver robust data solutions.; Implement data governance, security, and access control best practices within Snowflake.; Monitor and troubleshoot data pipeline issues and performance bottlenecks.; Create and maintain documentation related to data processes and architecture.; Required Skills & Qualifications:; 5+ years of experience in data engineering or a similar role.; 2+ years of hands-on experience with Snowflake (SnowSQL, Streams, Tasks, Cloning, etc.).; Proficient in SQL, with deep understanding of analytical and complex queries.; Experience with ETL\/ELT tools such as dbt, Informatica, Talend, Matillion, or Apache Airflow.; Strong knowledge of data modeling concepts (Star\/Snowflake schemas).; Experience with cloud platforms (AWS, Azure, or GCP) and integrating Snowflake with cloud storage (S3, Blob, GCS).; Familiarity with programming languages like Python or Scala for scripting and automation.; Strong problem-solving skills and attention to detail.; Preferred Qualifications:; Snowflake certification (SnowPro Core or Advanced Architect) is a plus.; Experience with CI\/CD, version control (Git), and DevOps practices.; Familiarity with BI tools like Power BI, Tableau, or Looker.; Regards; Kshama; +91 9833964181; kshama.raj@blueocean.systems","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85784453","Role":"Data Engineer (Cloud Migration)","Company":"ANTAS PTE. LTD.","Location":"Singapore","Publish_Time":"2025-07-15 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/85784453","job_desc":"Role Overview:; Seeking a seasoned data engineer (7+ years) to design, build, and optimize scalable data solutions using AWS (S3, RDS, Redshift, Glue, Lambda), Databricks (Delta Lake, Spark), and Informatica IDMC.; Role Overview:; Seeking a seasoned data engineer (7+ years) to design, build, and optimize scalable data solutions using AWS (S3, RDS, Redshift, Glue, Lambda), Databricks (Delta Lake, Spark), and Informatica IDMC.; Key Responsibilities:; Architect data platforms including lakes, warehouses, and databases on AWS and Databricks; Develop and automate ETL\/data pipelines with AWS Glue, Lambda, Databricks, and Informatica IDMC; Integrate and transform data from various sources ensuring quality and governance; Monitor and tune performance of data workflows and queries; Implement security, compliance, and cost optimization best practices; Maintain documentation and collaborate across teams to meet data needs; Skills Required:; 7+ years of experience in data engineering with hands-on work in AWS, Databricks, and\/or Informatica IDMC; Skilled in Python, Java, or Scala; strong SQL and NoSQL knowledge; Experience in data modeling, performance tuning, and debugging; Strong problem-solving, communication, and collaboration skills; Certifications in AWS, Databricks, or Informatica are a plus; Experience with Apache Spark, Hadoop, and Informatica IDMC for data governance; Knowledge of Tableau or Power BI; Familiarity with Docker, Kubernetes, Git, CI\/CD, and DevOps principles","salary":"","work_type":"Kontrak\/Temporer","country":"singapore"}
{"Job_ID":"85607529","Role":"Cloud Data Engineer Lead (AWS, Databricks, and Informatica IDMC)","Company":"Synapxe","Location":"One North","Publish_Time":"2025-07-09 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/85607529","job_desc":"Roles And Responsibilities:; \u2022 Design and architect data storage solutions, including databases, data lakes, and warehouses, using AWS services such as Amazon S3, Amazon RDS, Amazon Redshift, and Amazon DynamoDB, along with Databricks' Delta Lake. Integrate Informatica IDMC for metadata management and data cataloging.; \u2022 Create, manage, and optimize data pipelines for ingesting, processing, and transforming data using AWS services like AWS Glue, AWS Data Pipeline, and AWS Lambda, Databricks for advanced data processing, and Informatica IDMC for data integration and quality.; \u2022 Integrate data from various sources, both internal and external, into AWS and Databricks environments, ensuring data consistency and quality, while leveraging Informatica IDMC for data integration, transformation, and governance.; \u2022 Develop ETL (Extract, Transform, Load) processes to cleanse, transform, and enrich data, making it suitable for analytical purposes using Databricks' Spark capabilities and Informatica IDMC for data transformation and quality.; \u2022 Monitor and optimize data processing and query performance in both AWS and Databricks environments, making necessary adjustments to meet performance and scalability requirements. Utilize Informatica IDMC for optimizing data workflows.; \u2022 Implement security best practices and data encryption methods to protect sensitive data in both AWS and Databricks, while ensuring compliance with data privacy regulations. Employ Informatica IDMC for data governance and compliance.; \u2022 Implement automation for routine tasks, such as data ingestion, transformation, and monitoring, using AWS services like AWS Step Functions, AWS Lambda, Databricks Jobs, and Informatica IDMC for workflow automation.; \u2022 Maintain clear and comprehensive documentation of data infrastructure, pipelines, and configurations in both AWS and Databricks environments, with metadata management facilitated by Informatica IDMC.; \u2022 Collaborate with cross-functional teams, including data scientists, analysts, and software engineers, to understand data requirements and deliver appropriate solutions across AWS, Databricks, and Informatica IDMC.; \u2022 Identify and resolve data-related issues and provide support to ensure data availability and integrity in both AWS, Databricks, and Informatica IDMC environments.; \u2022 Optimize AWS, Databricks, and Informatica resource usage to control costs while meeting performance and scalability requirements.; \u2022 Stay up-to-date with AWS, Databricks, Informatica IDMC services, and data engineering best practices to recommend and implement new technologies and techniques.; Requirements \/ Qualifications; \u2022 Bachelor\u2019s or master\u2019s degree in computer science, data engineering, or a related field.; \u2022 Minimum 10 years of experience in data engineering, with expertise in AWS services, Databricks, and\/or Informatica IDMC.; \u2022 Proficiency in programming languages such as Python, Java, or Scala for building data pipelines.; \u2022 Evaluate potential technical solutions and make recommendations to resolve data issues especially on performance assessment for complex data transformations and long running data processes.; \u2022 Strong knowledge of SQL and NoSQL databases.; \u2022 Familiarity with data modeling and schema design.; \u2022 Excellent problem-solving and analytical skills.; \u2022 Strong communication and collaboration skills.; \u2022 AWS certifications (e.g., AWS Certified Data Analytics - Specialty, AWS Certified Data Analytics - Specialty), Databricks certifications, and Informatica certifications are a plus.; Preferred Skills:; \u2022 Experience with big data technologies like Apache Spark and Hadoop on Databricks.; \u2022 Knowledge of data governance and data cataloguing tools, especially Informatica IDMC.; \u2022 Familiarity with data visualization tools like Tableau or Power BI.; \u2022 Knowledge of containerization and orchestration tools like Docker and Kubernetes.; \u2022 Understanding of DevOps principles for managing and deploying data pipelines.; \u2022 Experience with version control systems (e.g., Git) and CI\/CD pipelines","salary":"","work_type":"Kontrak\/Temporer, Full time","country":"singapore"}
{"Job_ID":"85578684","Role":"DATA ENGINEER","Company":"Continental Technology Solutions Pte Ltd","Location":"Kampong Ubi","Publish_Time":"2025-07-07 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/85578684","job_desc":"Roles&Responsibilities:; Design, build, and maintain scalable ETL\/ELT pipelines and data workflows.; Develop and manage job schedules using Control-M to automate complex data workflows.; Collaborate with data analysts, data scientists, and business users to understand data requirements.; Ensure reliable data ingestion from internal and external sources into data lakes and data warehouses.; Monitor job execution, troubleshoot failures, and optimize system performance.; Ensure data quality, consistency, and security across systems.; Document data flows, Control-M job configurations, and operational procedures.; Participate in on-call rotations and incident response as needed.; Requirements:; Bachelor\u2019s degree in Computer Science, Engineering, Information Systems, or related field.; 3+ years of experience in a data engineering or data integration role.; Strong hands-on experience with Control-M (job scheduling, development, troubleshooting, and automation).; Proficiency in SQL and scripting languages (Python, Shell, or PowerShell).; Solid understanding of ETL processes, data pipelines, and workflow orchestration.; Experience with relational databases (e.g., Oracle, SQL Server, PostgreSQL).; Familiarity with data warehouse platforms (e.g., Snowflake, Redshift, BigQuery).; Experience with cloud platforms (AWS, GCP, or Azure) and their data services.; Knowledge of CI\/CD practices and version control systems (Git).","salary":"","work_type":"Kontrak\/Temporer","country":"singapore"}
{"Job_ID":"85818027","Role":"Staff Data Engineer","Company":"Network Guard","Location":"North Region","Publish_Time":"2025-07-16 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/85818027","job_desc":"We are seeking a highly skilled and hands-on Staff Data Engineer to architect and maintain modern data infrastructure and pipelines. This is an individual contributor role focused on building scalable data platforms that power analytics, insights, and data-driven decisions across the company. You will work closely with analysts, data scientists, product, and engineering teams to design end-to-end data solutions using tools like AWS Redshift, Athena, Snowflake, and other leading cloud platforms.; ; ; What You\u2019ll Do; Design and build scalable data pipelines to ingest, process, and store large volumes of structured and unstructured data from diverse sources.; Develop and maintain robust data warehouse architectures leveraging tools such as AWS Redshift, Athena, and Snowflake.; Optimize data models, queries, and storage strategies for performance, scalability, and cost-effectiveness.; Collaborate with cross-functional stakeholders (analytics, product, ML) to gather requirements and deliver data solutions that support business goals.; Ensure data quality, security, and privacy through best practices in governance, testing, and monitoring.; Own and operate production data workflows, resolving incidents and ensuring reliability.; Stay up-to-date with the latest trends and advancements in data engineering and analytics.; ; ; Tech Stack; Languages & Tools: Python, SQL; Data Warehousing & Query Engines: Snowflake, AWS Redshift, Athena; Pipeline & Orchestration: Apache Airflow, AWS Glue, dbt; Cloud & DevOps: AWS (Lambda, S3, IAM), Docker, Terraform; Streaming : Kafka, Kinesis; BI & Analytics: Tableau, Power BI; ; ; What You\u2019ll Need To Succeed; Bachelor's or Master\u2019s degree in Computer Science, Engineering, or related field.; 5+ years of experience as a Data Engineer, with a strong focus on data pipeline development and data warehousing.; Deep proficiency in AWS Redshift, Athena, Snowflake, and experience working with large-scale data systems.; Strong programming and scripting skills in Python, SQL, and Shell.; Experience with pipeline orchestration tools (e.g., Airflow, Glue, dbt).; Solid understanding of data modeling, schema design, and ETL processes.; Familiarity with cloud infrastructure and DevOps practices (especially AWS).; Experience with BI platforms like Tableau or Power BI.; Excellent analytical and problem-solving skills.; Strong communication and the ability to work across functions and mentor others.; ; ; Nice to Have; Knowledge of data privacy and security best practices (e.g., GDPR, SOC2).; Exposure to MLOps and ML data pipelines.; Experience in high-growth, product-led or SaaS environments.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85688508","Role":"Cloud Data Engineer","Company":"Unison Consulting Pte. Ltd.","Location":"Singapore","Publish_Time":"2025-07-11 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/85688508","job_desc":"Design and architect data storage solutions, including databases, data lakes, and warehouses, using AWS services such as Amazon S3, Amazon RDS, Amazon Redshift, and Amazon DynamoDB, along with Databricks' Delta Lake. Integrate Informatica IDMC for metadata management and data cataloging.; Create, manage, and optimize data pipelines for ingesting, processing, and transforming data using AWS services like AWS Glue, AWS Data Pipeline, and AWS Lambda, Databricks for advanced data processing, and Informatica IDMC for data integration and quality.; Integrate data from various sources, both internal and external, into AWS and Databricks environments, ensuring data consistency and quality, while leveraging Informatica IDMC for data integration, transformation, and governance.; Develop ETL (Extract, Transform, Load) processes to cleanse, transform, and enrich data, making it suitable for analytical purposes using Databricks' Spark capabilities and Informatica IDMC for data transformation and quality.; Monitor and optimize data processing and query performance in both AWS and Databricks environments, making necessary adjustments to meet performance and scalability requirements. Utilize Informatica IDMC for optimizing data workflows.; Requirements; Good experience in data engineering, with expertise in AWS services, Databricks, and\/or Informatica IDMC.; Proficiency in programming languages such as Python, Java, or Scala for building data pipelines.; Evaluate potential technical solutions and make recommendations to resolve data issues especially on performance assessment for complex data transformations and long running data processes.; Strong knowledge of SQL and NoSQL databases.; Familiarity with data modeling and schema design.; AWS certifications (e.g., AWS Certified Data Analytics - Specialty, AWS Certified Data Analytics - Specialty), Databricks certifications, and Informatica certifications are a plus.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85815135","Role":"Cloud Engineer","Company":"Virtusa Singapore Private Limited","Location":"Downtown Tanjong Pagar","Publish_Time":"2025-07-16 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/85815135","job_desc":"Roles & responsibilities:; Work as the go to person inside Data & Analytics organization in HSBC Wealth and Personal Banking for any banking data request from Data Scientist team and senior business stakeholders.; Work as DataOps transformation to enable self-serve, high performing data ETL.; Hands-on development for data ETL pipeline, data quality check and be responsible for peer coding review.; Participate in regular stands-up to share and speak up the progress and key blocker to ensure project is going smooth; Skills & Qualifications:; 2-5 years of data related working experience in Financial Institute with special preference in banking.; High proficiency in SQL, Python, PostgreSQL and Cloud native data pipeline automation tooling, e.g. BigQuery; Knowledge of SAS coding will be an advantages; Proven working experience in delivering large scale data projects\/data products.; Project experience in designing & developing high-performance solution for handling extremely large volume data processing, storage and wrangling in real-time manner.; Good understanding of DevOps, DataOps or MLOps.; Experience in RPA (Robotic Process Automation) is an advantages; Experience in Business visualization tool Looker, including LookML; Cloud-native data ETL development in GCP is preferred; Technology stack provisioned in the team include:; DataProc plus Python on Google Cloud; Airflow DAG for scheduling; Multiple databases including PostgreSQL, BigQuery, graph databases, etc.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85271341","Role":"Data Engineer Lead","Company":"Unison Consulting Pte. Ltd.","Location":"Singapore","Publish_Time":"2025-06-30 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/85271341","job_desc":"5 years of experience in data management, data engineering, or data visualization.; Experience with Python, AWS, Glue, spark, redshift, Lambda, S3 and strong Sql (DWH).; Excellent with AWS side of data engineering.; Amazon S3 for storage, AWS Glue for ETL, Amazon Redshift for data warehousing, Amazon Kinesis for real-time data processing, and AWS Lambda for serverless computing.; Amazon EMR for large-scale processing and Amazon Athena for querying data, form the core of a data engineering workflow on AWS.; Excellent problem-solving, organization, debugging, and analytical skills.; Ability to work independently and in a team environment.; Excellent communication skills for effectively expressing ideas to team members and clients.; Understanding of relational database concepts and SQL querying.; Experience with visualization tools - Power BI, will be advantage.; Debug and optimize existing data infrastructure and processes as needed.; Strong hands-on experience and to lead the team.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85617821","Role":"Data Platform Engineer","Company":"International Baccalaureate Organization","Location":"Singapore","Publish_Time":"2025-07-10 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/85617821","job_desc":"The Data Platform Engineer will be responsible for the design, build, maintenance, security and performance of the data analytics platform, serving both Digital Office stakeholders and lines of business.  The work will range from larger technology delivery initiatives, working on a shared Digital Office Data Analytics product backlog, providing maintenance support, and ensuring the smooth operation of live applications and platforms. This role requires hands-on development experience implementing new features and functionalities within the platform.; The International Baccalaureate provides world-class educational services to over 5500 schools across 159 countries. A career at IB is not just a job; it\u2019s an opportunity to work with an innovative world leader of education services and contribute to our 50-year mission of creating a better and more peaceful world. Apply now to join our global organization where we empower our employees to thrive and make a difference.; About the Job; Technical Design ; Analyze business requirements, understand underlying data sources, transformation requirements, data mapping, data modelling and metadata for reporting solutions.; Translate these business needs to a simple, scalable and secure technical design.; Design EDW, data mart layers with appropriate enterprise considerations like architectural fit, performance, flexibility, maintainability, automation etc.; Technical Build ; Build infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources, applications and platforms.; Play an active role in the development scrum team, delivering features with appropriate quality and velocity in the product backlog according to IB\u2019s DevOps practices including refining of user stories, acceptance criteria, write code, conduct unit testing, documentation and troubleshooting.; Maintenance & continuous improvement; Perform ITIL Incident, Problem, and Change Management practices in accordance to SLAs and follow processes.; Identify key problem areas within the application and implement improvements. Evaluate and improve existing data analytics systems.; Data Expertise ; Understand the IB\u2019s main business processes and how it relates to data that is generated or captured.; Understand associated data flows and dependencies between different enterprise systems.; About You; BSc\/BA in Computer Science, Engineering or relevant field.; Able to integrate multiple data sources & user-end applications with databases into one system. (to store the data and its retrieval from the databases).; Solid experience in designing and implementing robust data pipelines and ETL\/ELT framework.; Proven experience as a data warehouse architect & developer, including full implementation of data warehousing solution.; Experience in data engineering solutions built on modern data lake or Lakehouse architectures, including Delta Lake or equivalent frameworks e.g. Microsoft Fabric.; In-depth understanding of database management systems, online analytical processing (OLAP), SQL queries (Azure SQL DB).; Expertise with Azure Resource Management and templates is an added advantage.; Exposure to cloud technologies (MS Azure, AWS) & desire to learn and deliver new things on a needs-basis. (big data, BI, data science, etc.).; Strong expertise in data warehouse design methodologies and technologies, data modelling (Data Vault modelling methodology experience is preferable), data quality and metadata.; Ability to work within a fast-paced environment to meet deadlines, multitask and cope with multiple activities.; In addition to your salary, we offer an attractive range of benefits including: ; 20% employer's CPF contribution ; S$1,200 yearly flexible credits; 20 Days annual leave, plus public holidays, with the choice to buy or sell up to 3 days additional annual leave\u202fusing flexible credits; Life assurance 2x annual salary ; Flexible working hours due to nature of work; Organisation sponsored learning opportunities for professional development; Corporate passes to Singapore Zoo, River Wonders and Gardens By The Bay","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85607945","Role":"Data Engineer (Cloud Migration Projects)","Company":"Synapxe","Location":"One North","Publish_Time":"2025-07-09 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/85607945","job_desc":"Position Summary \/ Project Description; Data Engineer for cloud migration projects (Primarily utilizing AWS, IDMC, Databricks and Tableau); Role and Responsibilities; \u2022 Develop ETL (Extract, Transform, Load) processes to cleanse, transform, and enrich data, making it suitable for analytical purposes using Databricks' Spark capabilities and Informatica IDMC for data transformation and quality.; \u2022 Monitor and optimize data processing and query performance in both AWS and Databricks environments, making necessary adjustments to meet performance and scalability requirements. Utilize Informatica IDMC for optimizing data workflows.; \u2022 Implement security best practices and data encryption methods to protect sensitive data in both AWS and Databricks, while ensuring compliance with data privacy regulations. Employ Informatica IDMC for data governance and compliance.; \u2022 Implement automation for routine tasks, such as data ingestion, transformation, and monitoring, using AWS services like AWS Step Functions, AWS Lambda, Databricks Jobs, and Informatica IDMC for workflow automation.; \u2022 Maintain clear and comprehensive documentation of data infrastructure, pipelines, and configurations in both AWS and Databricks environments, with metadata management facilitated by Informatica IDMC.; \u2022 Collaborate with cross-functional teams, including data scientists, analysts, and software engineers, to understand data requirements and deliver appropriate solutions across AWS, Databricks, and Informatica IDMC.; \u2022 Identify and resolve data-related issues and provide support to ensure data availability and integrity in both AWS, Databricks, and Informatica IDMC environments.; \u2022 Optimize AWS, Databricks, and Informatica resource usage to control costs while meeting performance and scalability requirements.; \u2022 Stay up-to-date with AWS, Databricks, Informatica IDMC services, and data engineering best practices to recommend and implement new technologies and techniques.; Requirements \/ Qualifications; \u2022 Bachelor\u2019s or master\u2019s degree in computer science, data engineering, or a related field.; \u2022 Minimum 4 years of experience in data engineering, with expertise in AWS services, Databricks, and\/or Informatica IDMC.; \u2022 Proficiency in programming languages such as Python, Java, or Scala for building data pipelines.; \u2022 Evaluate potential technical solutions and make recommendations to resolve data issues especially on performance assessment for complex data transformations and long running data processes.; \u2022 Strong knowledge of SQL and NoSQL databases.; \u2022 Familiarity with data modeling and schema design.; \u2022 Excellent problem-solving and analytical skills.; \u2022 Strong communication and collaboration skills.; \u2022 AWS certifications, Databricks certifications, and Informatica certifications are a plus.","salary":"","work_type":"Kontrak\/Temporer, Full time","country":"singapore"}
{"Job_ID":"85600186","Role":"Data Engineer Intern","Company":"Innowave Tech Pte Ltd","Location":"Paya Lebar","Publish_Time":"2025-07-09 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/85600186","job_desc":"About Innowave Tech; Innowave Tech is an Artificial Intelligence (AI) company offering solutions for the Semiconductor and Advanced Manufacturing industry. Utilizing deep industrial domain knowledge, proven experience, and innovation, we provide expert AI solutions and systems to address various industry pain points.; Job Description; We are seeking a highly motivated and detail-oriented Data Engineer Intern to join our data team. In this role, you will assist in designing, building, and maintaining data pipelines and infrastructure that support data analytics and decision-making across the organization. This is an excellent opportunity to gain hands-on experience with real-world data engineering tools, workflows, and cloud platforms.; Key Responsibilities; \u00b7 Assist in developing and maintaining data pipelines using ETL\/ELT tools.; \u00b7 Support data integration from various internal and external sources into data warehouses.; \u00b7 Work with structured and unstructured data to transform it into usable formats.; \u00b7 Help ensure data quality, consistency, and availability across systems.; \u00b7 Collaborate with data scientists, analysts, and engineers to support data needs.; \u00b7 Document data workflows, schemas, and system processes.; \u00b7 Monitor data pipelines and resolve any issues or failures.; Requirements; \u00b7 Currently pursuing a degree in Computer Science, Data Science, Engineering, or a related field.; \u00b7 Basic knowledge of SQL and experience with a programming language such as Python, Java, or Scala.; \u00b7 Familiarity with relational databases (e.g., MySQL, PostgreSQL) and\/or data warehousing solutions (e.g., BigQuery, Redshift, Snowflake).; \u00b7 Experience in C# development.; \u00b7 Understanding of ETL concepts and data pipeline architecture.; \u00b7 Strong analytical thinking and problem-solving skills.; \u00b7 Willingness to learn and work in a team-oriented environment.; \u00b7 Internship duration should be at least 3 months full time.; \u00b7 Resume should indicate your forecasted internship dates.; Preferred Qualifications:; \u00b7 Exposure to cloud platforms like AWS, GCP, or Azure.; \u00b7 Experience with data processing frameworks (e.g., Apache Spark, Airflow).; \u00b7 Knowledge of version control tools (e.g., Git).","salary":"$800 \u2013 $1,000 per month (SGD)","work_type":"Kontrak\/Temporer","country":"singapore"}
{"Job_ID":"85702523","Role":"Senior Data Engineer","Company":"Singapore Telecommunications Limited","Location":"Singapore","Publish_Time":"2025-07-09 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/85702523","job_desc":"An empowering career at Singtel begins with a Hello. Our purpose, to Empower Every Generation, connects people to the possibilities they need to excel. Every \"hello\" at Singtel opens doors to new initiatives, growth, and BIG possibilities that takes your career to new heights. So, when you say hello to us, you are really empowered to say\u2026\u201cHello BIG Possibilities\u201d.; Be a Part of Something BIG!  ; Design, build, and maintain scalable data pipelines and processing systems that power analytics and AI use cases across a hybrid data platform; Contribute to design conversation of AIDA\u2019s new data and AI platform; Collaborate with platform, analytics, and governance teams to deliver high-quality, secure, and well-documented data assets; Lead team of data engineers to ensure timely availability of accurate, well-documented data for use in AI use case; Make An Impact By; Design and implement batch and streaming data ingestion pipelines from diverse sources (e.g., files, APIs, Kafka, databases); Develop real-time and near-real-time data workflows using tools like Apache Flink, Kafka Streams; Optimize performance for high-volume and high-velocity datasets; Implement data quality checks and automatic monitoring system to ensure consistently accurate and available data; Design and manage storage solutions such as Microsoft Fabric, Delta Lake, and Databricks; Apply best practices for schema design, partitioning, and data lifecycle management; Support data discovery and cataloguing in coordination with governance tools; Work with data scientists, analysts, and business users to understand data needs and translate them into technical solutions; Partner with DevSecOps and platform engineers to automate deployment and orchestration of data pipelines; Document data flows, transformations, and quality checks in accordance with governance standard; Skills to Succeed; Bachelor\u2019s or Master\u2019s degree in Computer Science, Engineering, or a related field; 5 years of experience in data engineering; 1 year in a senior technical role delivering data engineering projects; Deep expertise in Spark, Databricks, and data processing frameworks; Strong knowledge of streaming technologies such as Apache Kafka, Apache Flink, or Azure Event Hub; Experience working with data lake and\/or lakehouse architectures such as Hadoop, Delta Lake, Iceberg and Microsoft OneLake; Proficient in Python and SQL; Familiar with workflow orchestration (e.g., Apache Airflow) and CI\/CD principles; Analytical mindset with a focus on data quality, performance, and maintainability; Able to work independently and collaboratively in a dynamic environment; Strong communication and documentation skills to support cross-functional collaboration; Knowledge of data types across network and IT in telco environment; Rewards that Go Beyond; Full suite of health and wellness benefits  ; Ongoing training and development programs  ; Internal mobility opportunities; Your Career Growth Starts Here. Apply Now!; We are committed to a safe and healthy environment for our employees & customers and will require all prospective employees to be fully vaccinated.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85043774","Role":"Data Engineer","Company":"Tabernacle Pte Ltd","Location":"Central Region","Publish_Time":"2025-06-19 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/85043774","job_desc":"Role Overview:; We are looking for a skilled Remote Fabric Data Engineer to support our ongoing data platform migration and transformation efforts. This role focuses on migrating existing Databricks workloads into Microsoft Fabric, implementing efficient transformation logic, and ensuring complete lineage documentation and performance optimization.; Key Responsibilities:; Migrate existing Databricks Silver\/Gold layer SQL transformations into Microsoft Fabric SQL and Dataflows.; Design and implement transformation logic, including deduplication, joins, field mappings, and business rules in Fabric Dataflows.; Develop dimensional data models with surrogate key generation, row-level security (RLS) attributes, and role-based filtering.; Build and maintain metadata lineage pipelines to track data flow across the lifecycle \u2014 from source systems through transformation to target layers.; Tune performance of Fabric solutions by applying partitioning strategies, indexing, and aggregations where applicable.; Produce clear, concise documentation including:; Dataflow structures and logic; Dimensional model definitions; End-to-end lineage path summaries for audit and traceability; Qualifications:; Diploma or Degree in Information Technology, Computer Science, or related field.; Any other related certifications;  Experience and Skills:; Strong experience in Microsoft Fabric (Dataflows, Pipelines, Fabric SQL, and Lakehouses).; Prior hands-on experience migrating Databricks SQL logic or similar workloads into Microsoft environments.; Solid understanding of data modeling, data warehousing, and ETL\/ELT transformations.; Proficiency in performance tuning and working with large-scale data systems using partitioning and indexing.; Experience with metadata management, data lineage, and documentation practices.; Comfortable working in a fully remote setting with distributed teams.; Excellent written communication skills to produce technical documentation and collaborate asynchronously.","salary":"","work_type":"Kontrak\/Temporer","country":"singapore"}
{"Job_ID":"84822778","Role":"G08 - Data Engineer","Company":"FPT Asia Pacific Pte Ltd","Location":"Singapore","Publish_Time":"2025-06-17 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/84822778","job_desc":"We are looking for experienced data engineers to join our team who will be responsible for:; Data Engineering and Platform Integration; Design, develop, and maintain data pipelines and ETL processes using AWS services (Glue, Athena, S3, RDS); Work with data virtualisation tools like Denodo and develop VQL queries; Ingest and process data from various internal and external data sources; Perform data extraction, cleaning, transformation, and loading operations; Implement automated data collection processes including API integrations when necessary; Data Architecture; Design and implement data models (conceptual, logical, and physical) using tools like ER Studio; Develop and maintain data warehouses, data lakes, and operational data stores; Develop and maintain data blueprints; Create data marts and analytical views to support business intelligence needs using Denodo, RDS; Implement master data management practices and data governance standards; Technical Architecture and Integration; Ensure seamless integration between various data systems and applications; Implement data security and compliance requirements; Design scalable solutions for data integration and consolidation; Development and Analytics; Develop Python scripts in AWS Glue for data processing and automation; Write efficient VQL\/SQL queries and stored procedures; Design and develop RESTful APIs using modern frameworks and best practices for data services; Work with AWS Sagemaker for machine learning model deployment and integration; Manage and optimise database performance, including indexing, query tuning, and maintenance; Work in an Agile environment and participate in sprint planning, daily stand-ups, and retrospectives; Implement and maintain CI\/CD pipelines for automated testing and deployment; Participate in peer code reviews and pair programming sessions; Documentation and Best Practices; Create and maintain technical documentation for data models and systems; Follow industry-standard coding practices, version control, and change management procedures; Stakeholder Collaboration; Partner with cross-functional teams on data engineering initiatives; Gather requirements, conduct technical discussions, implement solutions, and perform testing; Collaborate with Product Managers, Business Analysts, Data Analysts, Solution Architects, UX Designers to build scalable, data-driven products; Provide technical guidance and support for data-related queries; Qualifications and Experience:; At least 3 years of experience in data engineering or similar role; Strong proficiency in Python, VQL, SQL; Experience with AWS services (Glue, Athena, S3, RDS, Sagemaker); Knowledge of data virtualisation concepts and tools (preferably Denodo); Experience with BI tools (preferably Tableau, Power BI); Understanding of data modelling and database design principles; Familiarity with data governance and master data management concepts; Experience with version control systems (Gitlab) and CI\/CD pipelines; Experience working in Agile environments with iterative development practices; Strong problem-solving skills and attention to detail; Excellent communication skills and ability to work in a team environment; Knowledge of AI technologies (AWS Bedrock, Azure AI, LLMs) would be advantageous","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85006790","Role":"Data Engineer-Consultant","Company":"WSH Experts Pte Ltd","Location":"Singapore","Publish_Time":"2025-06-17 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/85006790","job_desc":"Job responsibilities; Create and manage a single master record for each business entity, ensuring data consistency, accuracy, and reliability.; Implement data governance processes, including data quality management, data profiling, data remediation, and automated data lineage.; Create and maintain multiple robust and high-performance data processing pipelines within Cloud, Private Data Centre, and Hybrid data ecosystems.; Assemble large, complex data sets from a wide variety of data sources.; Collaborate with Data Scientists, Machine Learning Engineers, Business Analysts, and Business users to derive actionable insights and reliable foresights into customer acquisition, operational efficiency, and other key business performance metrics.; Develop, deploy, and maintain multiple microservices, REST APIs, and reporting services.; Design and implement internal processes to automate manual workflows, optimize data delivery, and re-design infrastructure for greater scalability.; Establish expertise in designing, analyzing, and troubleshooting large-scale distributed systems.; Support and work with cross-functional teams in a dynamic environment.; Job Requirements; Experience building and operating large-scale data lakes and data warehouses.; Experience with Hadoop ecosystem and big data tools, including Spark and Kafka.; Experience with Master Data Management (MDM) tools and platforms such as Informatica MDM, Talend Data Catalog, Semarchy xDM, IBM PIM & IKC, or Profisee.; Familiarity with MDM processes such as golden record creation, survivorship,reconciliation, enrichment, and quality.; Experience in data governance, including data quality management, data profiling, data remediation, and automated data lineage.; Experience with stream-processing systems including Spark-Streaming.; Experience working with Cloud services using one or more Cloud providers such as Azure, GCP, or AWS.; Experience with Delta Lake and Databricks.; Advanced working experience with relational SQL and NoSQL databases, including Hive, HBase, and Postgres.; Deep understanding of SQL and the ability to optimize data queries.; If the requirement matches with your profile, kindly share your updated CV\/resume to Aparna at aparna@wshexperts.com.sg","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"83448441","Role":"ART 1284 Data Engineer","Company":"FPT Asia Pacific Pte Ltd","Location":"Singapore","Publish_Time":"2025-06-17 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/83448441","job_desc":"Responsibilities:; Develop and maintain data pipelines and ETL\/ELT processes, and ensuring maintainability through unit and integration testing.; Collaborate with data teams to understand requirements and automate deployment and monitoring.; Optimize data storage and troubleshoot issues to enhance performance.; Skillset:; Possess a degree in Computer Science\/Information Technology or related fields.; At least 5 years of relevant working experience in software or data engineering with proficiency in Python.; Strong experience in unit and integration testing.; Familiarity with DevOps practices and Agile methodologies.; Strong software engineering, analytical and problem-solving skills.; Experience in AWS and Kubernetes (K8s).; Familiarity with data platforms such as Snowflake, Apache Spark, or Apache Hive, as well as orchestration tools like Apache Airflow, Dagster, or Prefect.; Familiarity with GitHub workflows and Datadog.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85072996","Role":"Data Engineer (Azure Synapse Analytics, PySpark)","Company":"SembCorp Utilities Pte. Ltd.","Location":"East Region","Publish_Time":"2025-06-20 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/85072996","job_desc":"About Sembcorp; Sembcorp is a leading energy and urban solutions provider headquartered in Singapore. Led by its purpose to drive energy transition, Sembcorp delivers sustainable energy solutions and urban developments by leveraging its sector expertise and global track record.; Play a role in Powering Asia\u2019s Energy Transition; Drive Asia\u2019s energy transition with us! Our Gas & Related Services segment is a key growth engine, delivering reliable and efficient energy to industries and communities across multiple countries. We support Asia\u2019s growing energy needs while advancing the shift to a lower-carbon future.; Purpose and Scope; We are seeking a highly skilled and self-driven Azure Data Engineer with expertise in PySpark, Python, and modern Azure data services including Synapse Analytics and Azure Data Explorer. The ideal candidate will design, develop, and maintain scalable data pipelines and architectures, enabling effective data management, analytics, and governance.; Key Roles and Responsibilities; Design, develop, and maintain scalable and efficient data pipelines (both batch and real-time streaming) using modern data engineering tools; Build and manage data lakes, data warehouses, and data marts using Azure Data Services; Integrate data from various sources including APIs, structured\/unstructured files, IoT devices, and real-time streams; Develop and optimize ETL\/ELT workflows using tools such as Azure Data Factory, Databricks, and Apache Spark; Implement real-time data ingestion and processing using Azure Stream Analytics, Event Hubs, or Kafka; Ensure data quality, availability, and security across the entire data lifecycle; Collaborate with analysts, data scientists, and engineering teams to deliver business-aligned data solutions; Contribute to data governance efforts and ensure compliance with data privacy standards; Establish and manage source system connectivity (on-prem, APIs, sensors, etc.); Handle deployment and migration of data pipeline artifacts between environments using Azure DevOps; Design, develop, and troubleshoot PySpark scripts and orchestration pipelines; Perform data integration using database joins and other transformations aligned with project requirements; Any assigned ad-hoc duties.; Requirements:; Bachelor\u2019s Degree in Computer Science, Engineering, or related field; 3\u20135 years of experience in Azure-based data engineering, PySpark, and Big Data technologies; Strong hands-on experience with Azure Synapse Analytics for pipeline orchestration and data handling; Expertise in SQL, data warehousing, data marts, and ingestion using PySpark and Python; Solid experience building and maintaining cloud-based ETL\/ELT pipelines, especially with Azure Data Factory or Synapse; Familiarity with cloud data environments such as Azure and optionally AWS; Experience with Azure DevOps for CI\/CD and artifact deployment; Excellent communication, problem-solving, and interpersonal skills; Good to Have:; 1\u20132 years of experience working with Azure Data Explorer (including row-level security and access controls).; Experience with Azure Purview for metadata management, data lineage, governance, and discovery; Ability to work independently and take full ownership of assignments; Proactive in identifying and resolving blockers and escalating when needed; Exposure to real-time processing with tools like Azure Stream Analytics or Kafka; Our Culture at Sembcorp; At Sembcorp, our culture is shaped by a strong set of shared behaviours that guide the way we work and uphold our commitment to driving the energy transition.; We foster an institution-first mindset, where the success of Sembcorp takes precedence over individual interests. Collaboration is at the heart of what we do, as we work seamlessly across markets, businesses, and functions to achieve our goals together. Accountability is a core principle, ensuring that we take ownership of our commitments and deliver on them with integrity and excellence. These values define who we are and create a workplace where our people can thrive while making a meaningful impact on driving energy transition.; Join us in making a real impact!","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85603113","Role":"Platform Engineer - Data & AI","Company":"Equinix Asia Pacific","Location":"Singapore","Publish_Time":"2025-07-09 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/85603113","job_desc":"Who are we?; Equinix is the world\u2019s digital infrastructure company\u00ae, operating over 260 data centers\u202facross the globe. Digital leaders harness Equinix's trusted platform to bring together and interconnect foundational infrastructure at software speed. Equinix enables organizations to access all the right places, partners and possibilities to scale with agility, speed the launch of digital services, deliver world-class experiences and multiply their value, while supporting their sustainability goals. ; Our culture is based on collaboration and the growth and development of our teams.\u202f We hire hardworking people who thrive on solving\u202fchallenging\u202fproblems and give them opportunities to hone new skills and try new approaches, as we grow our product portfolio with new software and network architecture solutions. We embrace diversity in thought and contribution and are committed to providing\u202fan equitable work environment that is foundational to our core values as a company and is vital to our success.; Job Summary; We\u2019re looking for a Senior Platform Engineer with a strong foundation in data architecture, distributed systems, and modern cloud-native platforms to architect, build, and maintain intelligent infrastructure and systems that power our AI, GenAI and data-intensive workloads.; You\u2019ll work closely with cross-functional teams, including data scientists, ML & software engineers, and product managers & play a key role in designing a highly scalable platform to manage the lifecycle of data pipelines, APIs, real-time streaming, and agentic GenAI workflows, while enabling federated data architectures. The ideal candidate will have a strong background in building and maintaining scalable AI & Data Platform, optimizing workflows, and ensuring the reliability and performance of Data Platform systems.; Responsibilities; Platform & Cloud Engineering; Develop and maintain real-time and batch data pipelines using tools like Airflow, dbt, Dataform, and Dataflow\/Spark; Design and develop event-driven architectures using Apache Kafka, Google Pub\/Sub, or equivalent messaging systems; Build and expose high-performance data APIs and microservices to support downstream applications, ML workflows, and GenAI agents; Architect and manage multi-cloud and hybrid cloud platforms (e.g., GCP, AWS, Azure) optimized for AI, ML, and real-time data processing workloads; Build reusable frameworks and infrastructure-as-code (IaC) using Terraform, Kubernetes, and CI\/CD to drive self-service and automation; Ensure platform scalability, resilience, and cost efficiency through modern practices like GitOps, observability, and chaos engineering; Data Architecture & Governance; Lead initiatives in data modeling, semantic layer design, and data cataloging, ensuring data quality and discoverability across domains; Implement enterprise-wide data governance practices, schema enforcement, and lineage tracking using tools like DataHub, Amundsen, or Collibra; Guide adoption of data fabric and mesh principles for federated ownership, scalable architecture, and domain-driven data product development; AI & GenAI Platform Integration; Integrate LLM APIs (OpenAI, Gemini, Claude, etc.) into platform workflows for intelligent automation and enhanced user experience; Build and orchestrate multi-agent systems using frameworks like CrewAI, LangGraph, or AutoGen for use cases such as pipeline debugging, code generation, and MLOps; Experience in developing and integrating GenAI applications using MCP and orchestration of LLM-powered workflows (e.g., summarization, document Q&A, chatbot assistants, and intelligent data exploration); Hands-on expertise building and optimizing vector search and RAG pipelines using tools like Weaviate, Pinecone, or FAISS to support embedding-based retrieval and real-time semantic search across structured and unstructured datasets; Engineering Enablement; Create extensible CLIs, SDKs, and blueprints to simplify onboarding, accelerate development, and standardize best practices; Streamline onboarding, documentation, and platform implementation & support using GenAI and conversational interfaces; Collaborate across teams to enforce cost, reliability, and security standards within platform blueprints; Work with engineering by introducing platform enhancements, observability, and cost optimization techniques; Foster a culture of ownership, continuous learning, and innovation; Qualifications; 5+ years of hands-on experience in Platform or Data Engineering, Cloud Architecture, AI Engineering roles; Strong programming background in Java, Python, SQL, and one or more general-purpose languages; Deep knowledge of data modeling, distributed systems, and API design in production environments; Proficiency in designing and managing Kubernetes, serverless workloads, and streaming systems (Kafka, Pub\/Sub, Flink, Spark); Experience with metadata management, data catalogs, data quality enforcement, and semantic modeling & automated integration with Data Platform; Proven experience building scalable, efficient data pipelines for structured and unstructured data; Experience with GenAI\/LLM frameworks and tools for orchestration and workflow automation; Experience with RAG pipelines, vector databases, and embedding-based search; Familiarity with observability tools (Prometheus, Grafana, OpenTelemetry) and strong debugging skills across the stack; Experience with ML Platforms (MLFlow, Vertex AI, Kubeflow) and AI\/ML observability tools; Prior implementation of data mesh or data fabric in a large-scale enterprise; Experience with Looker Modeler, LookML, or semantic modeling layers; Why You\u2019ll Love This Role; Drive technical leadership across AI-native data platforms, automation systems, and self-service tools; Collaborate across teams to shape the next generation of intelligent platforms in the enterprise; Work with a high-energy, mission-driven team that embraces innovation, open-source, and experimentation; Equinix is committed to ensuring that our employment process is open to all individuals, including those with a disability.  If you are a qualified candidate and need assistance or an accommodation, please let us know by completing this form.; Equinix is an Equal Employment Opportunity and, in the U.S., an Affirmative Action employer.  All qualified applicants will receive consideration for employment without regard to unlawful consideration of race, color, religion, creed, national or ethnic origin, ancestry, place of birth, citizenship, sex, pregnancy \/ childbirth or related medical conditions, sexual orientation, gender identity or expression, marital or domestic partnership status, age, veteran or military status, physical or mental disability, medical condition, genetic information, political \/ organizational affiliation, status as a victim or family member of a victim of crime or abuse, or any other status protected by applicable law.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85702527","Role":"Senior Data & Integration Architect","Company":"Singapore Telecommunications Limited","Location":"Singapore","Publish_Time":"2025-07-09 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/85702527","job_desc":"An empowering career at Singtel begins with a Hello. Our purpose, to Empower Every Generation, connects people to the possibilities they need to excel. Every \"hello\" at Singtel opens doors to new initiatives, growth, and BIG possibilities that takes your career to new heights. So, when you say hello to us, you are really empowered to say\u2026\u201cHello BIG Possibilities\u201d.; Be a Part of Something BIG!  ; The Senior Data & Integration Architect plays a critical role in designing and implementing the data and integration layer of Singtel\u2019s next-generation AI platform. This role focuses on orchestrating secure, scalable, and interoperable data architectures and API integration frameworks that support advanced AI and ML workloads across the enterprise.; You will work closely with the AI architecture, security, platform, and business teams to ensure seamless data movement, governance alignment, and real-time system interoperability for serving as the bridge between distributed data systems, cloud services, and AI-enabled applications.; Make An Impact By; Design, build and implement enterprise-wide data and API integration frameworks to support AI\/ML platforms across hybrid cloud and on-premise environments; Work with system owners and data domain leads to design and deliver scalable end-to-end data flows across operational, analytical, and AI systems; Define and develop secure, reusable API interfaces (REST, GraphQL, event-driven) and data interface (batch or streaming) that enable seamless interoperability between internal systems and AI services; Oversee and evaluate new data integration approaches and pipeline designs to ensure efficient, secure, and scalable data flow between data sources and AI platforms.; Collaborate with Security and Data Governance teams to ensure integration designs align with compliance, privacy, and policy requirements (e.g., PDPA, data classification); Design and enable data access strategies for LLMs and agent-based workflows, ensuring context-rich, real-time connectivity to distributed enterprise systems; Implement and maintain integration middleware and tooling (e.g. Kafka, Azure ML\/Foundry, Databricks, etc) to support data orchestration, synchronization, and reliability; Contribute integration expertise to data or AI experimentation, PoCs, and platform upgrades, ensuring architectural consistency and production-readiness; Define and enforce data and integration design standards, focusing on scalability, resilience, observability, and system decoupling; Work closely with business units, IT, and Networks to align integration plans with enterprise priorities and ensure successful data exchange across functional boundaries; Skills to Succee; Bachelor\u2019s in Computer Science, Engineering, Data, AI\/ML, or related field.; At least 3 years of experience in data architecture, system and API integration engineering.; Demonstrated experience in designing integration flows for large-scale, real-time systems across cloud and legacy environments.; Experience in designing and implementing data integration frameworks across hybrid cloud and on-premise environments, including building scalable and secure data pipelines for AI\/ML platforms.; Proficient in data integration design, with solid knowledge of data pipelines, data lakes, data warehouses, and data lakehouse architectures.; Good knowledge of modern data orchestration and middleware tools such as Apache Kafka, Azure Data Factory, Databricks, Airflow, and experience in managing data flow between operational, analytical, and AI environments.; Working knowledge of data security, data protection and data quality management including implementation of encryption, RBAC, masking, and alignment with regulatory frameworks such as PDPA and internal data classification policies.; Proven experience integrating data systems with AI\/ML workflows, including model training, serving, monitoring, and enabling context-aware access for LLMs and agent-based automation.; Effective collaboration skills to work across data, platform, machine learning engineering and API integration teams, with a clear communication style to bridge business and technical stakeholders; Good internal (IT, Networks, business) and external (suppliers, government) stakeholders management skills; Strong technical writing and presentation skills, with the ability to communicate complex concepts clearly to both technical and non-technical stakeholders.; Proactive and fast learner with a strong drive to stay current on emerging technologies and industry trends.; Rewards that Go Beyond; Full suite of health and wellness benefits  ; Ongoing training and development programs  ; Internal mobility opportunities; Your Career Growth Starts Here. Apply Now!; We are committed to a safe and healthy environment for our employees & customers and will require all prospective employees to be fully vaccinated.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"83482854","Role":"Data Engineer","Company":"SMRT Corporation Ltd","Location":"Singapore","Publish_Time":"2025-06-17 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/83482854","job_desc":"The duties and responsibilities for Data Engineer, are as listed below. The list is not comprehensive and related duties and responsibilities may be assigned from time to time.; Data Engineering & Processing:; Develop and maintain data pipelines for efficient data ingestion and transformation.; Work with structured and unstructured data to ensure optimal storage and retrieval.; Perform data analysis and report on results.; Database Design & Management:; Design and implement relational and NoSQL database schemas for scalability.; Optimize database performance through indexing, partitioning, and query tuning.; Implement data security and compliance best practices.; API Development & Backend Engineering:; Design and develop APIs for data access and application integration.; Implement authentication, authorization, and API security best practices.; Cloud Infrastructure & Deployment (Supporting Role):; Assist in design Azure cloud architectures; Work with IT infrastructure team to set up cloud infrastructure for application hosting, data storage and processing. ; Collaboration & Best Practices:; Collaborate with internal stakeholders to understand their business needs.; Work with software engineers, data scientist, frontend developer to understand the data requirement and design architecture of the data platform.; Implement CI\/CD pipelines for automated testing, deployment and monitoring.; Write testable and maintainable code and documentation to deploy to production.; Engage continuously with end-user for feedback and improvements.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85840488","Role":"Sr. Data Engineer","Company":"VISA WORLDWIDE PTE. LIMITED","Location":"Singapore","Publish_Time":"2025-07-17 09:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/85840488","job_desc":"Company Description; Visa is a world leader in payments and technology, with over 259 billion payments transactions flowing safely between consumers, merchants, financial institutions, and government entities in more than 200 countries and territories each year. Our mission is to connect the world through the most innovative, convenient, reliable, and secure payments network, enabling individuals, businesses, and economies to thrive while driven by a common purpose \u2013 to uplift everyone, everywhere by being the best way to pay and be paid.; Make an impact with a purpose-driven industry leader. Join us today and experience Life at Visa.; Job Description; Visa\u2019s Technology Organization is a community of problem solvers and innovators reshaping the future of commerce. We operate the world\u2019s most sophisticated processing networks, capable of handling more than 65k secure transactions a second across 80M merchants, 15k Financial Institutions, and billions of everyday people. You\u2019ll work on complex distributed systems and solve massive scale problems centered on new payment flows, business and data solutions, cybersecurity, and B2C platforms.; In addition, Value Added Services (VAS) - VAS Digital Marketing is a key growth strategy for Visa globally, aimed at diversifying Visa\u2019s revenue with products and solutions that differentiate its network and deliver valuable solutions across other networks.; ; The Opportunity:; We are developing and executing a shared strategic vision for Digital Marketing platforms and products that enable Visa to be the world-leading data-driven payments company. As a Senior Data Engineer, you will be part of a world-class team of Engineers to define, drive and execute on this vision. We are looking for a self-motivated, versatile and energetic individual with software engineering skills and expertise with Java, Big data & Web technologies, who embraces solving complex challenges on a global scale. The candidate will be extensively involved in hands-on activities including POCs, design, development, testing, and managing applications globally used by Visa cardholders. Candidate must be flexible and willing to switch tasks based on team's needs.; ; You will use your Java skills and experience with various technologies to design, develop, test, and deploy high-quality code that meets stringent business, security, and resiliency requirements. You will collaborate with other teams, vendors, and stakeholders to ensure the smooth delivery and operation of the application. You will have the opportunity to learn and apply new technologies and frameworks, such as AI and generative AI, to enhance the functionality and performance of the application.; Primary responsibilities will include:; Design, develop, test, document, and implement new applications and enhance existing systems to ensure high performance and reliability.; Write secure, maintainable, and efficient code that adheres to Java\/J2EE best practices, organizational and security standards.; Create and maintain comprehensive technical documentation, including design changes and architectural decisions, using Wiki or similar tools.; Participate in code and design review sessions to ensure high-quality deliverables and adherence to development standards.; Collaborate with architects, product owners, and technical stakeholders to deliver products that meet business requirements and leverage modern technologies.; Identify and recommend opportunities for process improvements, enhancements, and adoption of best practices within the development team.; Mentor and support junior developers, fostering knowledge sharing and contributing to the development of departmental procedures and standards.; Coordinate and contribute to Continuous Integration (CI) activities and the implementation of automated testing frameworks.; Develop proof-of-concepts (POCs) and prototypes to validate ideas and quickly iterate new features or enhancements.; Communicate technical solutions, project status, issues, and risks effectively to both technical and non-technical stakeholders.; Ensure the delivery of high-quality, defect-free code and take accountability for meeting project timelines and quality standards.; This is a hybrid position. Expectation of days in office will be confirmed by your Hiring Manager.; Qualifications; Preferred Qualifications; \u20223 or more years of work experience with a Bachelor\u2019s Degree or more than 2 years of work experience with an Advanced Degree (e.g. Masters, MBA, JD, MD); \u20224\u20137 years of relevant experience in Java\/J2EE enterprise applications.; \u2022Strong skills in Core Java, J2EE, Spring Framework, Spring Boot, Hibernate, and Web services.; \u2022Proficiency in object-oriented design and software design principles.; \u2022Experience with secure coding practices.; \u2022Strong SQL skills with experience in relational (MySQL, PostgreSQL) and NoSQL (MongoDB) databases.; \u2022Understanding of data warehousing concepts and tools.; \u2022Exposure to data engineering frameworks such as Apache Spark, Hadoop, or Kafka is an advantage.; \u2022Basic understanding of ETL processes and data pipeline development.; \u2022Hands-on experience with containerization and orchestration tools (Docker, Kubernetes).; \u2022Proficiency in version control systems (Git\/Stash), build tools (Maven), and CI\/CD tools (Jenkins).; \u2022Familiarity with Unix\/Linux operating systems and shell scripting.; \u2022Experience with UI frameworks and frontend development using Angular or React, Next.js, JavaScript, HTML, and CSS.; \u2022AI and generative AI skills are highly desirable.; \u2022Experience working in all phases of the software development life cycle.; \u2022Experience with Agile methodologies (Scrum, sprints) and tools (Jira).; \u2022Understanding of DevOps practices.; \u2022Solid foundation in computer science, including data structures and algorithms.; \u2022Willingness to learn and improve coding skills, especially in Java or Scala.; Additional Information:; Skills\/Abilities; \u2022Strong analytical and problem-solving abilities.; \u2022Quick to learn and adapt to new technologies and challenges.; \u2022Excellent organizational skills with the ability to manage multiple tasks and deadlines in a fast-paced environment.; \u2022Outstanding written and verbal communication skills for conveying ideas and implementation plans to team members and stakeholders.; \u2022Highly detail-oriented, resourceful, and results-driven.; \u2022Self-motivated with a demonstrated ability to work independently and meet commitments.; \u2022Comfortable collaborating in dynamic, fast-paced, and highly interactive team settings.; \u2022Eager to learn new skills, embrace new initiatives, and contribute to team success.; \u2022Proven ability to maintain a positive attitude and have fun while working as part of a team.; Additional Information; Visa is an EEO Employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability or protected veteran status. Visa will also consider for employment qualified applicants with criminal histories in a manner consistent with EEOC guidelines and applicable local law.","salary":"","work_type":"","country":"singapore"}
{"Job_ID":"85783762","Role":"Collibra Platform & Metadata Engineer","Company":"Goldtech Resources Pte Ltd","Location":"Singapore","Publish_Time":"2025-07-15 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/85783762","job_desc":"We are seeking a highly skilled and detail-oriented Collibra Platform & Metadata Engineer to lead the design, configuration and integration of metadata governance capabilities within our enterprise data environment. This role will be responsible for administering the Collibra platform, managing metadata lifecycle processes, supporting regulatory reporting initiatives and enabling advanced lineage and data quality linkages.; Key Responsibilities:; Metadata Governance & Lifecycle; Manage the full lifecycle of Metadata assets in Collibra, including definitions, certifications, ownership and stewardship.; Oversee and maintain the enterprise business glossary, ensuring alignment with data standards and compliance frameworks.; Data Lineage & KDE Traceability; Build and maintain comprehensive business and technical lineage in Collibra.; Enable traceability of Key Data Elements (KDEs) across data systems and reports, supporting regulatory needs.; Collibra Administration & Configuration; Configure Collibra assets, domains, user roles, responsibilities and workflows.; Administer platform integrations and ensure optimal performance and availability.; Manage access control, asset lifecycles and approval workflows.; Metadata Integration & Automation; Develop metadata ingestion pipelines using APIs and data connectors.; Integrate metadata from tools such as ETL platforms, data warehouses, BI tools (Power BI, Tableau) and DQ tools (Alteryx, Informatica).; Workflow Development & Advanced Capabilities; Design and implement custom workflows for glossary, lineage, and KDE processes.; Automate metadata processes and enhance Collibra usability through term linking, data quality rule associations and embedded metadata strategies.; Stakeholder Engagement & Enablement; Work closely with data stewards, business users, and compliance teams to align metadata governance with enterprise policies.; Provide user onboarding, training and technical documentation to support adoption.; Requirements:; Bachelor\u2019s degree in Computer Science, Data Management or related field.; Certified in Collibra Ranger (mandatory).; 4\u20138 years of experience in data governance or metadata management roles.; Minimum 2\u20133 years of hands-on experience with Collibra, including platform configuration and API-based integration.; Strong knowledge of metadata, business glossary, data lineage, and data governance principles.; Proficiency with scripting and integration tools (e.g., Python, SQL, REST APIs).; Familiar with regulatory requirements (e.g., BCBS 239, MAS, GDPR) and data quality frameworks.; Preferred Qualifications:; Collibra Solution Architect certification is a plus.; Experience in financial services, regulatory reporting or enterprise data governance programs.; Familiarity with cloud platforms (Azure, AWS) and data modeling.; Excellent communication and stakeholder engagement skills.; Please send your updated resume in MS Word format to resume@goldtechrs.com along with:; Education Level; Working experiences; Each employment background; Reason for leaving each employment; Last drawn salary; Expected salary; Date of availability","salary":"","work_type":"Kontrak\/Temporer, Full time","country":"singapore"}
{"Job_ID":"83729756","Role":"Senior Data Engineer","Company":"WSH Experts Pte Ltd","Location":"Singapore","Publish_Time":"2025-06-17 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/83729756","job_desc":"Job Description; Create and manage a single master record for each business entity, ensuring data consistency, accuracy, and reliability.; Implement data governance processes, including data quality management, data profiling, data remediation, and automated data lineage.; Create and maintain multiple robust and high-performance data processing pipelines within Cloud, Private Data Centre, and Hybrid data ecosystems.; Assemble large, complex data sets from a wide variety of data sources.; Collaborate with Data Scientists, Machine Learning Engineers, Business Analysts, and Business users to derive actionable insights and reliable foresights into customer acquisition, operational efficiency, and other key business performance metrics.; Develop, deploy, and maintain multiple microservices, REST APIs, and reporting services.; Design and implement internal processes to automate manual workflows, optimize data delivery, and re-design infrastructure for greater scalability.; Establish expertise in designing, analyzing, and troubleshooting large-scale distributed systems.; Support and work with cross-functional teams in a dynamic environment.; Job Requirement; Experience building and operating large-scale data lakes and data warehouses.; Experience with Hadoop ecosystem and big data tools, including Spark and Kafka.; Experience with Master Data Management (MDM) tools and platforms such as Informatica MDM, Talend Data Catalog, Semarchy xDM, IBM PIM & IKC, or Profisee.; Familiarity with MDM processes such as golden record creation, survivorship,reconciliation, enrichment, and quality.; Experience in data governance, including data quality management, data profiling, data remediation, and automated data lineage.; Experience with stream-processing systems including Spark-Streaming.; Experience working with Cloud services using one or more Cloud providers such as Azure, GCP, or AWS.; Experience with Delta Lake and Databricks.; Advanced working experience with relational SQL and NoSQL databases, including Hive, HBase, and Postgres.; Deep understanding of SQL and the ability to optimize data queries.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85750802","Role":"Uni Internship Jan to May 2026 - HEALIX Data Integration and Validation","Company":"Synapxe","Location":"Singapore","Publish_Time":"2025-07-14 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/85750802","job_desc":"Synapxe is the national HealthTech agency inspiring tomorrow\u2019s health. The nexus of HealthTech, we connect people and systems to power a healthier Singapore. Together with partners, we create intelligent technological solutions to improve the health of millions of people every day, everywhere.;  ; Are you someone who enjoys problem solving, has a creative and curious mind, and strives to create a better and healthier tomorrow? If you say yes to all, do check out our website and find out more about Internship@Synapxe.;  ; Join Synapxe as an intern and see how you can contribute in powering a healthier Singapore. We aim to deliver the best experience for all interns, to create exponential growth and paving your future in the tech industry.; HEALIX is the pioneering cloud-based analytics platform designed specifically for the public healthcare sector. By utilizing state-of-the-art cloud-native tools, it facilitates the swift development of AI models aimed at enhancing patient care. HEALIX consolidates the diverse data requirements of the public healthcare sector into a single, cohesive platform, fostering collaboration and efficiency. This integration allows stakeholders to harness the power of data-driven insights and artificial intelligence, ultimately leading to improved healthcare outcomes.; The selected interns will play a crucial role in the User Acceptance Testing (UAT) and Go-Live validation of BRAIN migration projects. They will also help to ensure data accuracy and verify the functionality of ETL processes and Tableau within the BRAIN system. Furthermore, interns will assist in the preparation and deployment of artifacts, providing essential post-live support such as automation of some testing approaches in support for regression testing when major change\/tech refresh is needed or until the successful handover of the project.; As part of the HEALIX Track initiative, the selected intern will partake in the following duties:; Preparing Table Creation in ETL (DDL) for new systems in collaboration with FAs; Testing new source systems; Preparing for data catch-up from on-prem (Cluster and National Warehouse Systems); Validating data between on-prem and HEALIX (source systems data); Reviewing long-running jobs and suggesting performance improvement solutions; About you:; Be pursuing a Bachelor Degree in Business Analytics, Data Science, Computer Engineering, Computer Science or related discipline; Graduating in May\/Dec 2026 or May 2027; Knowledge of ETL processes (e.g., Informatica); Proficiency in SQL scripting; Experience with data visualization tools (e.g., OBIEE\/Tableau); Familiarity with AWS and Databricks; Basic programming skills in Python; Good team player with strong analytical and communication skills; Ability to multitask and work effectively as part of a multidisciplinary team; Passionate and keen to make a difference to re-imagine the future of HealthTech; #LI-YG1; #LI-LK1","salary":"","work_type":"","country":"singapore"}
{"Job_ID":"85783909","Role":"Data Engineer (Python\/OpenSearch)","Company":"ASTEK Singapore Innovation Technology Pte. Ltd.","Location":"Central Region","Publish_Time":"2025-07-15 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/85783909","job_desc":"Job Title: Data Engineer; Location: Singapore, CBD; Employment Type: Full-time Permanent; We are looking for a Data Engineer to join our team and support the development of a standardized Data Toolkit. This platform will streamline data ingestion and observability efforts across the organization. You will also assist in migrating data and systems from legacy tools to this new framework.; Key Responsibilities; Design, build, and test components of the Data Toolkit.; Integrate data systems to ensure consistency and efficiency in data workflows.; Support the migration of existing data and processes from legacy systems.; Collaborate with engineering and product teams to ensure seamless adoption.; Requirements; Minimum 3 years of full time working experience.; Proficiency in Python.; Experience with search platforms such as OpenSearch, Elasticsearch, or Solr.; Strong software development, analytical, and problem-solving skills.; Knowledge of SQL, ETL\/ELT, and data pipeline architecture will be advantageous.; Experience with CI\/CD, Terraform, and AWS cloud services is a plus.; Good communication skills and the ability to work well in a team.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85019312","Role":"Senior Data Engineer","Company":"GC ASIA DENTAL PTE LTD","Location":"Tampines","Publish_Time":"2025-06-18 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/85019312","job_desc":"Job Responsibilities; Microsoft Fabric Expertise: Serve as the primary subject matter expert for Microsoft Fabric, including but not limited to Lakehouse, Data Factory, Synapse Data Engineering (Spark), Data Warehousing, Real-Time Analytics, and Power BI integration.; Data Solution Design & Development: Lead the end-to-end design, development, and implementation of robust, scalable, and efficient data pipelines and solutions within the Microsoft Fabric ecosystem. This includes data ingestion, transformation, orchestration, and consumption layers.; Architectural Leadership: Contribute to and drive the architectural vision for our data platform on Microsoft Fabric, ensuring best practices for performance, security, reliability, and cost-effectiveness.; Independent Project Management: Take full ownership of data engineering projects, including planning, resource allocation (where applicable), timeline management, risk mitigation, and successful delivery.; Stakeholder Collaboration: Work closely with data analysts, business stakeholders, and other engineering teams to understand data requirements, translate them into technical specifications, and deliver impactful data solutions.; Data Governance & Quality: Implement and enforce data governance, quality, and security best practices within the Microsoft Fabric environment.; Performance Optimization & Troubleshooting: Proactively monitor, troubleshoot, and optimize data pipelines and data models for performance and efficiency.; Mentorship & Best Practices: Champion best practices in data engineering, data modeling, and Microsoft Fabric utilization. Potentially mentor junior team members and foster a culture of continuous learning.; Documentation: Create and maintain comprehensive technical documentation for data pipelines, data models, and architectural designs.; Job Requirements; Minimum 5+ years of experience in data engineering, with a strong focus on building and maintaining enterprise-grade data platforms.; Demonstrable expert-level proficiency in Microsoft Fabric, with hands-on experience across multiple components (Lakehouse, Data Factory, Synapse Data Engineering\/Spark, Data Warehousing, Real-Time Analytics).; Proven track record of successful project management in data-related initiatives, including planning, execution, and delivery.; Extensive experience with Spark (PySpark\/Scala) for data transformation and processing within a distributed environment.; Strong understanding of data warehousing concepts, dimensional modeling, and data lake architectures.; Proficiency in SQL for data manipulation and analysis.; Experience with Azure ecosystem components (e.g., Azure Data Lake Storage, Azure DevOps, Azure Functions) is a strong plus.; Experience with version control systems (e.g., Git).; Excellent problem-solving, analytical, and critical thinking skills.; Exceptional communication and interpersonal skills, with the ability to articulate complex technical concepts to non-technical stakeholders and work effectively in a team-oriented environment.; Highly self-motivated and able to work independently with minimal supervision, taking initiative and ownership of tasks.; Microsoft Certified: Azure Data Engineer Associate, or other relevant Microsoft Azure\/Fabric certifications.; Experience with real-time data streaming technologies.; Familiarity with CI\/CD practices for data pipelines.; Experience with Power BI for data visualization and reporting.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85054557","Role":"Data Engineer","Company":"Assurity Trusted Solutions Pte Ltd","Location":"Singapore","Publish_Time":"2025-06-17 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/85054557","job_desc":"Assurity Trusted Solutions (ATS) is a wholly owned subsidiary of the Government Technology Agency (GovTech). As a Trusted Partner over the last decade, ATS offers a comprehensive suite of products and services ranging from infrastructure and operational services, authentication services, governance and assurance services as well as managed processes. In a dynamic digital and cyber landscape, where trust & collaboration are key, ATS continues to drive mutually beneficial business outcomes through collaboration with GovTech, government agencies and commercial partners to mitigate cyber risks and bolster security postures.; We are seeking a Data Engineer to join our dynamic team to develop and strengthen our data ecosystem for the maritime sector. This role is pivotal in bridging the gap between business\/regulatory needs and our maritime data ecosystem, so that we enhance Singapore\u2019s position as a leading global maritime hub. The ideal candidate will leverage stakeholder insights and agile methodologies to develop data ecosystem products that will be relevant for a range of stakeholders in our maritime ecosystem, which would include our port operators, international shipping lines, technology companies, among others.; The maritime sector in Singapore faces significant challenges, as well as opportunities. It is critical for MPA to position Singapore to capture benefits from growing maritime trade, while addressing various challenges including our manpower constraints and global pressures for maritime decarbonisation. This position offers a unique opportunity to enhance Singapore\u2019s global positioning as a leading maritime hub by advancing new value proposition that can be unlocked via data for our maritime ecosystem.; Responsibilities:; Design, Develop, Test, Deploy and Maintain data pipelines (ETL) on the Enterprise Data Warehouse and Big Data Platform; Design and Develop the API \/Web Services framework for curation of new datasets whether internal or external (Internet), and to interface with other systems (both internal and external); Explore and source new data sets to address emerging business use case needs; Support stakeholder engagement, development, implementation and maintenance of systems for data collection, storage, access, and analytics at scale.; Develop and manage continuous improvement of data architecture and ensure alignment with business requirements, data management and governance policies.; Design, develop, and maintain interactive dashboards that provide insights and data to support business decision-making.; Support the design and definition of the data architecture framework, standards, and principles, including modelling, metadata, privacy, security, reference data and master data; Requirements; 3+ years of related work experience as a Data Engineer; Good grasp of Software Engineering principles such as Requirements Gathering (both functional and non-functional), Modular & Re-usable Design.; Proficient in ETL using programming language \/tools such as Python and\/or SSIS and\/or Informatica Power Centre; Able to develop data applications including integration with ICT systems, build APIs and web applications via .Java and\/or Python; Familiarity with MS SQL, PostgreSQL or Oracle is preferred.; Proficient in Data Modelling and Data Mining.; Experience in designing and building scalable database schema for applications.; Understanding of Object-Oriented Design.; Knowledge of or prior work experience on Big Data platforms such as Hadoop or using Spark.; Experience in the cloud environment setup; Excellent organizational, analytical, and problem-solving skills.; Experience collaborating with business and product teams.; Join us and discover a meaningful and exciting career with Assurity Trusted Solutions!; The remuneration package will commensurate with your qualifications and experience. Interested applicants, please click \"Apply Now\".; We thank you for your interest and please note that only shortlisted candidates will be notified.; By submitting your application, you agree that your personal data may be collected, used and disclosed by Assurity Trusted Solutions Pte. Ltd. (ATS), GovTech and their service providers and agents in accordance with ATS\u2019s privacy statement which can be found at: https:\/\/www.assurity.sg\/privacy.html or such other successor site.; Benefits; A wholly-owned subsidiary of GovTech.; We promote a learning culture and encourage you to grow and learn.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"84568838","Role":"Data Strategy & Transformation (VP\/Director)","Company":"Evolution Recruitment Solutions Pte Ltd","Location":"Central Region","Publish_Time":"2025-06-17 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/84568838","job_desc":"The Data Strategy & Transformation (VP\/Director) will be responsible for driving the organization's overarching data strategy and aligning it with business priorities across the region. This individual will lead efforts to streamline data integration between data lakes and data marts, working across technology and business functions to support governance, analytics, and architecture initiatives. The role is instrumental in fostering a data-first culture and ensuring optimal use of data assets for business growth and regulatory compliance.; Key Responsibilities:; Develop and execute a regional data strategy that aligns with global governance principles and business goals.; Collaborate with key stakeholders to gather data requirements related to core business functions such as products, transactions, and reporting.; Architect scalable solutions that unify data from multiple systems into a centralized data environment.; Define and maintain logical and physical data models to support both operations and analytics.; Convert business needs into data structures optimized for different workloads.; Direct the evolution of data marts, warehouses, and integration layers in line with enterprise architecture.; Work with engineering teams on data model deployment and performance tuning.; Ensure models comply with governance, quality, and metadata standards across key domains.; Produce technical documentation, including data dictionaries and lineage visualizations, for audits and business use.; Lead efforts to improve metadata, data quality, and lineage to promote transparency and trust.; Support integration strategies that enable access to operational and analytical systems.; Qualifications:; Bachelor's degree in a related field (Computer Science, Data Science, etc.); Master\u2019s preferred.; Over 15 years of experience in data architecture or data management, ideally within the financial sector.; Deep familiarity with governance frameworks (e.g., DAMA-DMBOK), data modelling, and metadata practices.; Extensive experience in data model design for financial operations, regulatory reporting, and risk.; Skilled in modelling tools such as ERwin, PowerDesigner, or similar platforms.; Proficient in SQL and experienced in handling large, complex financial datasets.; Strong knowledge of data warehousing, integration techniques, and regulatory frameworks in the region.; Effective communicator with a proven ability to engage both technical and business audiences.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85673927","Role":"Data Architect - Technology (SAS VI)","Company":"UNIVERSAL PROCUREMENT SYSTEMS PTE LTD","Location":"Singapore","Publish_Time":"2025-07-11 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/85673927","job_desc":"We are looking for a seasoned Data Architect \/ Data Management Lead with extensive experience in designing data solutions for fraud analytics , investigative platforms , or intelligence and case management systems . The successful candidate will oversee the architecture, development, and implementation of integrated data pipelines and analytics frameworks within SAS-based environments , particularly SAS Viya . This is a hands-on leadership role requiring both technical depth and strong stakeholder coordination across business and technology teams.; Responsibilities:; Partner with business analysts to understand and shape data requirements for fraud or investigative systems.; Architect end-to-end data strategies, from source acquisition and cleansing to storage, preparation, and analytics delivery.; Convert functional needs into technical deliverables across ETL, engineering, and reporting layers.; Design and manage real-time and batch ingestion workflows into SAS platforms or large-scale data lakes.; Build and maintain robust data pipelines, orchestration layers, and visualization components using SAS Viya , DS2 , Python , and other tools.; Coordinate testing, integration, and support activities, including UAT and post-deployment maintenance.; Lead documentation efforts and contribute to overall data governance, metadata, and quality frameworks.; Troubleshoot complex data issues and track project progress against delivery milestones.; Required Experience & Skills:; Minimum 10 years of experience in data architecture, analytics platforms, or BI\/data warehouse implementations.; Delivered at least 5 full-scale projects involving fraud detection, surveillance systems, or case\/investigation platforms.; Strong hands-on capabilities in:; SAS Viya , SAS DS2 , and SQL; Python , Spark , JSON , XML; Linux , job orchestration tools , and real-time technologies like Kafka or SAS ESP; Postman , SOAPUI , REST APIs , and CI\/CD frameworks ( DevOps \/ DataOps ); Familiar with:; Data modeling, source-to-target mapping, real-time processing design; Metadata and data lineage tools, orchestration frameworks; Experience with SAS Intelligent Decisioning, RTDM, PEGA, or similar decision management tools is advantageous.; Prior involvement in public sector, government, or regulatory environments is preferred.; Strong communication and stakeholder alignment skills; capable of leading technical teams.; Bachelor\u2019s degree in a relevant discipline such as Computer Science, Information Technology, or Statistics","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"82357566","Role":"Vice President, Azure Data Technical Lead (JRI-4391)","Company":"Sumitomo Mitsui Banking Corporation","Location":"Singapore","Publish_Time":"2025-06-17 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/82357566","job_desc":"The Japan Research Institute (JRI) Limited is a subsidiary of Sumitomo Mitsui Banking Corporation (SMBC) Group. SMBC Group is a Tokyo-based bank holding company that is ranked among the largest 25 banks globally by assets under management. JRI provides comprehensive, highly value-added information services through the coordinated application of its 3 functions i.e. information systems, consulting and think-tank. As a system integrator, JRI offers services in IT strategy planning, implementation and outsourcing to a broad range of industries and activities. JRI Singapore currently supports the overall IT functions of Sumitomo Mitsui Banking Corporation (SMBC) in the Asia Pacific region.; You will be the Azure data platform Technical Lead to plan out on-premises system migration, include migrate methodology, data requirements and compliance strategy. Also provide insightful knowledge to guide internal team better support on the building of Azure data warehouse, Databrick, data pipelines and Power BI reports. ; Job Responsibilities; Analyse the current practices, processes, and procedures as well as identifying future business opportunities for leveraging Microsoft Azure Data and Analytics Services.; Provide technical and thought leadership as a senior member of the Analytics Practice in areas such as data access & ingestion, data processing, data integration, data modelling, database design & implementation, data visualization, and advanced analytics.; Collaborate with project managers in estimating technical tasks and deliverables.; Develop best practices including reusable code, libraries, patterns, and consumable frameworks for cloud-based data warehousing and ETL.; Maintain best practice standards for the development or cloud-based data warehouse solutioning including naming standards.; Job Requirements; Bachelor\u2019s degree in information technology, Computer Science, Manage Information Science, Banking and Finance or equivalent.; Min. 10 years of experience in Data Warehousing, Data Analytics, Realtime data integration or Business Intelligence; Min. 3 years of experience as a Data Architect, Solution Architect or Technical Lead; Min. 2 years of experience in managing technical team and leading on Azure data platform implementation.; Knowledge in both traditional and modern data architecture and processing concepts such as SQL, Hadoop, Spark, Kafka and business analytics; Proficiency in Databricks is a must.; Experience in technology skillsets:; Azure database platforms e.g. Azure SQL database, Azure Databricks, Azure data Lake Gen2; Azure data integration tools e.g. Azure Synapse, Azure Data factory, Azure Event Hub.; Data visualisation tools e.g. QlikView, Power BI; Programming language: R & Python; Possess architectural sense in connecting data sources, data visualization, structured and unstructured data.; Candidates with traditional ETL knowledge may be considered.; Knowledge in financial sector technologies, products and services will be an advantage.; Able to multi-task in a challenging technical environment to deliver high quality solutions; A meticulous team player who has proactive and positive attitude; Excellent communication, interpersonal and presentation skills","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"83384091","Role":"VP, Senior Azure Data Platform Solution Architect (JRI-4695)","Company":"Sumitomo Mitsui Banking Corporation","Location":"Singapore","Publish_Time":"2025-06-17 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/83384091","job_desc":"The Japan Research Institute (JRI) Limited is a subsidiary of Sumitomo Mitsui Banking Corporation (SMBC) Group. SMBC Group is a Tokyo-based bank holding company that is ranked among the largest 25 banks globally by assets under management. JRI provides comprehensive, highly value-added information services through the coordinated application of its 3 functions i.e. information systems, consulting and think-tank. As a system integrator, JRI offers services in IT strategy planning, implementation and outsourcing to a broad range of industries and activities. JRI Singapore currently supports the overall IT functions of Sumitomo Mitsui Banking Corporation (SMBC) in the Asia Pacific region.; Job Responsibilities; Develop and maintain Azure Data Architecture, ensuring compliance with data-related policies and standards.; Design and implement data solutions using Databricks, including data pipelines, ETL processes, and data lakes.; Analyse data-related system integration challenges and propose appropriate solutions.; Work with data engineer and IT team members to assist with data-related technical issues and support their data infrastructure needs.; Implement data security and privacy protection measures.; Develop and implement strategies for data acquisitions, archive recovery, and database implementation.; Work with data governance teams to ensure data solutions are compliant with regulations.; Monitor system performance and troubleshoot issues.; Maintain updated knowledge of the Azure platform, Databricks, and best practices.; Job Requirements; Bachelor\u2019s degree in computer science, computer engineering, information systems or related discipline.; Extensive experience with Azure services like Azure SQL Database, Data Factory, Data Lake Store, Data Lake Analytics, Stream Analytics, Azure Storage, Cosmos DB, and Databricks.; Proven experience in designing and implementing solutions using Databricks, including Spark, Delta Lake, and MLflow.; Strong analytical and problem-solving skills.; Excellent communication and collaboration skills.; Proven experience in data architecture and management, preferably in a cloud environment","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85207528","Role":"Data Engineer (Intern)","Company":"LHN Group Pte Ltd","Location":"East Region","Publish_Time":"2025-06-25 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/85207528","job_desc":"We are seeking a highly motivated and inquisitive Data Engineering Intern to assist in the design, development, and maintenance of a new data warehouse on the AWS cloud platform. You will play a crucial role in building and optimizing data pipelines to extract, transform, and load (ETL) data from various sources. This is an excellent opportunity to gain hands-on experience with real-world data engineering challenges and contribute to the growth of a dynamic company.; Key Responsibilities:; \u00b7 Assist in the design and development of the data warehouse architecture on AWS.; \u00b7 Develop and maintain data pipelines using AWS services like AWS Glue, AWS S3, AWS Redshift\/Aurora.; \u00b7 Extract, transform, and load data from various sources (NetSuite, Salesforce, SharePoint, other SaaS & internal systems) into the data warehouse.; \u00b7 Ensure data quality and integrity throughout the data pipeline.; \u00b7 Collaborate with data analysts and business users to understand their data needs and requirements.; \u00b7 Assist in the development of data quality checks and monitoring processes.; \u00b7 Learn and apply best practices in data engineering and cloud computing.; \u00b7 Document all data pipelines and processes.; \u00b7 Support the development and maintenance of data models.; Requirements; \u00b7 Currently pursuing a Bachelor's or Master's degree in Computer Science, Data Science, or a related field.; \u00b7 Strong understanding of data warehousing concepts and principles.; \u00b7 Experience with SQL and a scripting language like Python.; \u00b7 Basic understanding of cloud computing concepts and AWS services (preferred).; \u00b7 Familiarity with data modeling and data visualization tools (a plus).; \u00b7 Excellent analytical and problem-solving skills.; \u00b7 Strong communication and interpersonal skills.; \u00b7 Ability to work independently and as part of a team.; \u00b7 Passion for learning new technologies and solving challenging problems.","salary":"","work_type":"Kontrak\/Temporer","country":"singapore"}
{"Job_ID":"85232560","Role":"Reporting and Analytics Developer\/Data Engineer 0642B","Company":"USER EXPERIENCE RESEARCHERS PTE. LTD","Location":"Singapore","Publish_Time":"2025-06-27 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/85232560","job_desc":"We are seeking a highly skilled and experienced Big Data Engineer to join our team. The ideal candidate will have a minimum of 4 years of experience managing data engineering jobs in big data environment e.g., Cloudera Data Platform. The successful candidate will be responsible for designing, developing, and maintaining the data ingestion and processing jobs. Candidate will also be integrating data sets to provide seamless data access to users.; Responsibilities; Analyse the Authority's data needs and document the requirements.; Refine data collection\/consumption by migrating data collection to more efficient channels.; Plan, design and implement data engineering jobs and reporting solutions to meet the analytical needs.; Develop test plan and scripts for system testing, support user acceptance testing.; Build reports and dashboards according to user requirements; Work with the Authority's technical teams to ensure smooth deployment and adoption of new solution.; Ensure the smooth operations and service level of IT solutions.; Support production issues.; What we are looking for:; Good understanding and completion of projects using waterfall\/Agile methodology.; Strong SQL, data modelling and data analysis skills are a must.; Hands-on experience in big data engineering jobs using Python, Pyspark, Linux, and ETL tools like Informatica.; Hands-on experience in a reporting or visualization tool like SAP BO and Tableau is must.; Hands-on experience in DevOps deployment and data virtualisation tools like Denodo will be an advantage.; Track record in implementing systems using Hive, Impala and Cloudera Data Platform will be preferred.; Good understanding of analytics and data warehouse implementations.; Ability to troubleshoot complex issues ranging from system resource to application stack traces.; Track record in implementing systems with high availability, high performance, high security hosted at various data centres or hybrid cloud environments will be an added advantage.; Passion for automation, standardization, and best practices.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85808751","Role":"Big Data Engineer","Company":"Unison Consulting Pte. Ltd.","Location":"Singapore","Publish_Time":"2025-07-16 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/85808751","job_desc":"We are seeking a highly skilled and experienced Big Data Engineer to join our team. The ideal candidate will have a minimum of 5 years of experience managing data engineering jobs in big data environment e.g., Cloudera Data Platform. The successful candidate will be responsible for designing, developing, and maintaining the data ingestion and processing jobs. Candidate will also be integrating data sets to provide seamless data access to users.; SKILLS SET AND TRACK RECORD; * Good understanding and completion of projects using waterfall\/Agile methodology.; * Analytical, conceptualisation and problem-solving skills.; * Good understanding of analytics and data warehouse implementations; * Hands-on experience in big data engineering jobs using Python, Pyspark, Linux, and ETL tools like Informatica; * Strong SQL and data analysis skills. Hands-on experience in data virtualisation tools like Denodo will be an added advantage; * Hands-on experience in a reporting or visualization tool like SAP BO and Tableau is preferred; * Track record in implementing systems using Cloudera Data Platform will be an added advantage.; * Motivated and self-driven, with ability to learn new concepts and tools in a short period of time; * Passion for automation, standardization, and best practices; * Good presentation skills are preferred; The developer is responsible to:; * Analyse the Client data needs and document the requirements.; * Refine data collection\/consumption by migrating data collection to more efficient channels; * Plan, design and implement data engineering jobs and reporting solutions to meet the analytical needs.; * Develop test plan and scripts for system testing, support user acceptance testing.; * Work with the Client technical teams to ensure smooth deployment and adoption of new solution.; * Ensure the smooth operations and service level of IT solutions.; * Support production issues","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85577454","Role":"Big Data Developer","Company":"KG Sowers Group Pte Ltd","Location":"North Region","Publish_Time":"2025-07-08 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/85577454","job_desc":"Roles and Responsibilites:; Developing and optimising ETL (Extract, Transform, Load) processes to ingest and transform large volumes of data from multiple sources.; Must have experience in investment banking, payment and transaction banking domains.; Developing and deploying data processing applications using Big Data frameworks such as Hadoop, Spark, Kafka, or similar technologies.; Proficiency in programming languages and scripting (e.g., Java, Scala, Python, SQL) for data processing and analysis.; Experience with cloud platforms and services for Big Data (e.g., AWS, Azure, Google Cloud); Requirements:; Primary Skills:; Designing, building, and maintaining systems that handle large volumes of data, enabling businesses to extract valuable insights and make data-driven decisions.; Creating scalable and efficient data pipelines, implementing data models, and integrating various data sources.; Developing and deploying data processing applications using Big Data frameworks such as Hadoop, Spark, Kafka; Write efficient and optimised code in programming languages like Java, Scala, Python to manipulate and analyse data; Creating scalable and efficient data pipelines, implementing data models, and integrating diverse data sources to enable businesses to extract valuable insights; Secondary Skills:; Designing, developing, and implementing scalable and efficient data processing pipelines using Big Data technologies.; Implementing a Kafka-based pipeline to feed event-driven data into a dynamic pricing model, enabling real-time pricing adjustments based on market conditions and customer; Conduct testing and validation of data pipelines and analytical solutions to ensure accuracy, reliability, and performance.; Strong experience in Spring Boot and microservices architecture.; Strong experience in distributed computing principles and Big Data ecosystem components (e.g., Hadoop, Spark, Hive, HBase).; More than 8 years of working experience in IT industry; More than 5 years of relevant experience","salary":"","work_type":"Kontrak\/Temporer","country":"singapore"}
{"Job_ID":"85500842","Role":"Integration Developer (WebMethods_Talend_Kafka_DevOps)","Company":"Maltem Asia Pte. Ltd.","Location":"Raffles Place","Publish_Time":"2025-07-05 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/85500842","job_desc":"Maltem Singapore is seeking an Integration Developer to join our wonderful community.; Responsibilities :; Design, develop and deliver data integration\/data extraction solutions using Talend and ControlM for flow scheduling.; Migration experience from ETL IBM DataStage to Talend is plus.; Design and Develop Service Oriented Architecture SOA based architecture approaches, design methodologies and design patterns.; Develop Web Services (SOAP & REST) and complex XML schemas and data mapping; Extensive Working Experience on WebMethods Components\/concepts : WebMethods Designer, Integration Server, BPM (Business Process Model), MWS, WMDeployer, Web Services, Broker, SOA, CentraSite and Mediator; Develop Flow Services using adapters such as JDBC Adapter, Salesforce, SAP Adapter,AS400, Flat Files and XML.; Work with communication protocol like FTP, SFTP, FTP-PGP, HTTPS, AS2.; Interact with IT and business partners across global teams as required for support or projects.; Work on API gateways API portal, setting up security policies using OAUTH, SSO, JWT.; Familiar with CI\/CD tools, such as Jenkins, Maven\/Gradle, ANT script and SVN\/Git.; Good to have knowledge on Bitbucket repository and docker concepts.; Must-Have-Skillsets:; Proven track record in using Apache Kafka for managing real-time data feeds building data pipelines.; ETL Talend 8, Talend 7.x, Unix, SQL; Passion in advocating and implementing best practices in Software Engineering and DevOps.; Mandatory relevant experience in BPM & API (SOAP and REST) with Mediator & CentraSite; Experience in design, develop and support WebMethods implementation projects (WebMethods 9.x , 10.x) product suite","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85043745","Role":"Data Engineer","Company":"Tabernacle Pte Ltd","Location":"Central Region","Publish_Time":"2025-06-19 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/85043745","job_desc":"We are seeking a hands-on Fabric Onsite Data Engineer to support our data modernization initiatives by leveraging Microsoft Fabric. This role is critical in transitioning legacy ADF pipelines to Fabric Pipelines and Dataflows Gen2 while ensuring high data quality, robust metadata management, and effective collaboration with business and technical stakeholders.; Key Responsibilities:; Rebuild and optimize existing Azure Data Factory (ADF) ingestion processes using Microsoft Fabric Pipelines and Dataflows Gen2.; Coordinate with SL10\/EQS source system owners to align data schemas, field mappings, and ingestion specifications.; Perform onsite data validation and integrity checks across ingestion workflows to ensure accuracy and completeness.; Capture and log file-level and field-level metadata (e.g., file name, source system, ingestion timestamp) to enhance traceability and auditability.; Support User Acceptance Testing (UAT), triage user-reported issues, and provide technical support to business users during validation phases.; Train and guide end users on accessing and using Power BI reports, data glossary resources, and Baobao Q&A tools for self-service analytics.; Proactively respond to data queries from business users, ensuring data understanding and alignment with operational needs.; Qualifications:; Diploma or Degree in Information Technology, Computer Science, or related field.; Proven experience with Microsoft Fabric, including Pipelines and Dataflows Gen2.; Strong knowledge of Azure Data Factory, data ingestion strategies, and ETL\/ELT best practices.; Familiarity with data modeling, schema design, and metadata logging.; Experience working with business users and system owners to align technical data structures with business requirements.; Solid understanding of data quality assurance, validation techniques, and troubleshooting in production environments.; Excellent communication skills to support user training and documentation.; Ability to work onsite and collaborate across cross-functional teams in a dynamic environment.","salary":"","work_type":"Kontrak\/Temporer","country":"singapore"}
{"Job_ID":"85575223","Role":"Data Engineer","Company":"Synpulse Singapore Pte. Ltd.","Location":"Singapore","Publish_Time":"2025-07-08 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/85575223","job_desc":"We are an established, globally active management consulting company with offices in Switzerland, Germany, Austria, UK, USA, Singapore, Hong Kong, the Philippines, Australia, Indonesia and India. We are a valued partner to many of the world\u2018s largest international financial services and insurance firms. We support our clients at all project management stages from the development of strategies and operational frameworks to the technical implementation and handover. Our expertise in business and technology combined with our methodic approach enable us to create sustainable added value for our clients business.; About the job: ; Develop processes of the ingestion of data using various programming languages, techniques and tools from systems implemented using Oracle, Teradata, SAP, and Hadoop technology stack ; Evaluate and make decisions around dataset implementations designed and proposed by peer engineers ; Build large consumer database models for financial planning & analytics including Balance Sheet, Profit and Loss, Cost Analytics and Related Ratios ; Develop ETL, real time and batch data processes feeding into in-memory data infrastructure ; Perform and document data analysis, data validation, and data mapping\/design ; Work with clients to solve business problems in fraud, compliance and financial crime and present project results ; Use emerging and open-source technologies such as Spark, Hadoop, and Scala ; Collaborate on scalability issues involving access to massive amounts of data and information ; You should be comfortable with working with high profile clients on their sites ; About you:; Requirements; Bachelor's degree in computer science, Physics, Mathematics, or similar degree or equivalent ; Experience with open source big-data tools, such as Spark, Hadoop, and specially Scala ; 2 to 6 years of experience working in the Financial Services sector on big data project implementations ; Demonstrate strong analytical and problem-solving skills and the ability to debug and solve technical challenges with sometimes unfamiliar technologies ; Client facing experience, good communication and presentation skills ; Strong technical communication skills with demonstrable experience of working in rapidly changing client environments ; Quantexa Certification preferred ; Why us:; Flexible working hours with part-time working models and hybrid options; Attractive fringe benefits and salary structures in line with the market; Modern and central office space with good public transport connections; Can-do mentality and one-spirit culture; Varied events and employee initiatives; Your documents to start the process:; Resume; Job references; Qualifications (bachelor\/ master diploma, etc.) with certificate of grades; Motivation letter: Why Synpulse? Why you? Why this function?; Recommendation letters (optional); Do you approach your tasks with commitment and enjoyment and are you convinced that teamwork achieves better results than working alone? Are you proactive and willing to go the extra mile for your clients? Are you motivated not only to design solutions but also to implement them? As a flexible and goal-oriented person, you will quickly assume entrepreneurial responsibility with us.;  ; Do you appreciate the spirit of a growing international company with Swiss roots and a strong corporate culture? Then we look forward to receiving your online application at http:\/\/synpulse.com","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85482196","Role":"Intern, Geospatial Data Engineering & Platform team","Company":"Public Service Division","Location":"Singapore","Publish_Time":"2025-07-03 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/85482196","job_desc":"[What the role is]; The interns will be supporting the Geospatial Data Engineering & Platform team in the data operations revolving around the SLA Enterprise Data Platform. The operations involve: (1) onboarding of SLA data sets onto EDP Data Warehouse for publishing of data sets on EDP Data Portal (2) development of data engineering workload (data transformation pipelines) and dashboards\/reports for monitoring of system operating performance, auditorial insights, data quality and performance metrics.; [What you will be working on]; Assist in the development of building robust data ingestion pipelines to collect, clean, and merge data from diverse source systems; Develop, test, and maintain efficient, scalable, and reusable code for data pipelines; Develop, test, and maintain dashboards\/reports to serve the daily data operational needs.; Implement comprehensive validation processes and data quality checks to ensure data accuracy, consistency, and reliability.; Contribute to the documentation of data processes; [What we are looking for]; Pursuing tertiary qualifications in Data Science, Computer Science, or a related technical field; Strong analytical skills with demonstrable experience in data analysis tools, preferably Python and SQL; Ability to write efficient, well-documented code and think critically about edge cases and error handling; Familiarity with data management concepts, data engineering techniques, and ETL processes; Strong communication skills and ability to work effectively in a team environment; Able to commit between July 2025 to December 2025; Please include your availability in your CV.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"84995480","Role":"Senior Data Engineer - A25054","Company":"Activate Interactive Pte Ltd","Location":"Central Region","Publish_Time":"2025-06-18 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/84995480","job_desc":"About the job; Activate Interactive Pte Ltd (\"Activate\") is a leading technology consultancy headquartered in Singapore with a presence in Malaysia and Indonesia. Our clients are empowered with quality, cost-effective, and impactful end-to-end application development, like mobile and web applications, and cloud technology that remove technology roadblocks and increase their business efficiency.; ; We believe in positively impacting the lives of people around us and the environment we live in through the use of technology. Hence, we are committed to providing a conducive environment for all employees to realise their full potential, who in turn have the opportunity to continuously drive innovation.; ; We are searching for our next team members to join our growing team.; ; If you love the idea of being part of a growing company with exciting prospects in mobile and web technologies that create positive impact on people's lives, then we would love to hear from you.; ; Technology Solutions Office is looking for Senior Data Engineer; ; Internal Code: A25054; ; What will you do?; Lead a team of data engineers to design and develop robust data architectures that meet key business needs; Design, build, and maintain scalable data pipelines to handle large volumes of data; Collaborate with cross-functional teams to define data requirements and deliver quality solutions; Implement data governance and data quality methodologies to ensure data integrity; Utilize advanced analytics and data transformation techniques to derive insights and provide strategic recommendations; Mentor and train junior data engineers to enhance their skills and knowledge in data technologies; Requirements; ; What are we looking for?; At least 5 years of experience in data engineering, with a proven track record of leading data projects; Strong proficiency in data engineering tools and technologies including AWS, Azure, SQL, Python, and Spark; Extensive experience with data modeling, ETL processes, and big data technologies; Familiarity with data visualization tools like Tableau, Power BI, or similar; Experience with machine learning concepts and frameworks is a plus; Demonstrated ability to communicate effectively with technical and non-technical stakeholders; Strong problem-solving skills and ability to work in a fast-paced environment; Bachelor's or Master's degree in Computer Science, Data Science, or a related field; Benefits; ; What do we offer in return?; Fun working environment; Employee Wellness Program; Does it sound like something you are interested in exploring further? Please be in touch with our team for an initial chat at careers@activate.sg; ; Activate Interactive Singapore is an equal opportunity employer. Employment decisions will be based on merit, qualifications and abilities. Activate Interactive Pte Ltd does not discriminate in employment opportunities or practices on the basis of race, colour, religion, national origin, age, disability, marital status or any other characteristics protected by law.; ; Protecting your privacy and the security of your data are longstanding top priorities for Activate Interactive Pte Ltd.; ; Your personal data will be processed for the purposes of managing Activate Interactive Pte Ltd's recruitment related activities, which include setting up and conducting interviews and tests for applicants, evaluating and assessing the results, and as is otherwise needed in the recruitment and hiring processes.; ; Please consult our Privacy Notice (https:\/\/www.activate.sg\/privacy-policy) to know more about how we collect, use, and transfer the personal data of our candidates. Here you can find how you can request for access, correction and\/or withdrawal of your Personal Data.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85437508","Role":"Data Engineer for URA with 4 years experience (Contract)","Company":"Websparks Pte Ltd","Location":"East Region","Publish_Time":"2025-07-03 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/85437508","job_desc":"About Design and Planning Lab (DPLab); Be part of URA\u2019s DPLab, which spearheads URBEX efforts in the experimentation and development of smart planning technologies through Policy-Ops-Tech collaborations. Our teams leverage on their capabilities in data policy, governance, strategy, and engineering, product strategy, business analysis, software engineering, cloud engineering, urban design technologies, data science, AI\/ML, and modelling and simulation to collaborate with planners, architects, and policymakers to create insights and digital solutions for Singapore\u2019s urban planning challenges.; Data Engineering Work:; Support data engineering tasks, including the implementation and enhancement of data pipelines, as well as the rectification of broken pipelines.; Manage the data platform and perform regular software version upgrades across all environments, ensuring thorough testing and detailed documentation.; Support daily operational needs by handling application configuration changes, managing user access request to application, addressing general function queries, and troubleshooting on issues.; Perform source code review and configuration review periodically to ensure code quality and verify that sensitive information, such as secrets, is not hardcoded or embedded in source codes or configuration files.; Periodic patching of Azure Cloud Servers; Develop business processes by engaging stakeholders to understand use cases for building data pipelines, performing data modeling, completing data collection forms, documenting use cases, defining data attributes within various data quality zones, and establishing naming conventions for datasets.; Requirements:; Degree in Computer Science, Engineering, or related disciplines; Experienced in data engineering, including the implementation of data pipelines, development of data models and schemas, and pipeline monitoring and management.; Experienced in architecting, designing, and developing data platform.; Experience with:; \u25cb Python, PySpark, or similar programming languages; \u25cb Python packages for data manipulation and analysis (e.g. Pandas, GeoPandas, Shapely); \u25cb Database design and management (e.g. PostgreSQL, MS SQL, Oracle, Geodatabase); \u25cb SQL programming (e.g. writing complex queries, optimizing performance, data manipulation); \u25cb Scripting and version control (e.g. Bash, PowerShell, Git); \u25cb ETL (Extract, Transform, Load) processes; \u25cb Technologies such as JupyterHub, RStudio, PowerBI; \u25cb CI\/CD tools such as Jenkins, GitLab, YAML; \u25cb GIS technology (e.g. ArcGIS Server, PostGIS); \u25cb API development and SFTP for secure data transfer; \u25cb Cloud Platforms (e.g. Microsoft Azure, AWS); \u25cb Cloud Technologies (e.g. Azure Data Factory, Databricks, Azure Functions, Azure Key Vault, AWS Lambda); \u25cb Containerization technologies (e.g. Docker, Kubernetes); Able to work well with a team and be willing to learn.; Effective presentation, communication and writing skills.; Added advantage with the following:; Infrastructure-as-code Tools \u2013 Terraform, CloudFormation; Log Management tools (e.g. Azure Monitor, AWS CloudWatch, Splunk); Agile Management Tools \u2013 Confluence, Jira, Kanban board","salary":"","work_type":"Kontrak\/Temporer","country":"singapore"}
{"Job_ID":"85273433","Role":"Data Engineer Lead","Company":"DCS Card Centre Pte. Ltd.","Location":"East Region","Publish_Time":"2025-06-30 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/85273433","job_desc":"Key Responsibilities:; Lead the development of a financial-grade real-time data platform from 0 to 1 (covering data collection, storage, and computation layers), designing an architecture that supports daily processing of petabyte-scale transactional data.; Establish a cloud-native (AWS) data warehouse system integrated with Spark\/Flink for unified stream and batch processing, enabling millisecond-level metric computation for payment data.; Interface with the company\u2019s full-stack business (e.g., acquiring and issuing) and design data masking workflows and audit solutions that comply with PCI DSS.; Build and maintain data dashboards to support business teams in data-driven decision-making.; Integrate with payment systems to ensure seamless data flow and consolidation.; Responsible for data collection, processing, modeling, and structuring to improve data quality and usability.; Support cross-departmental data needs and promote data standardization and automation.; Work closely with teams such as risk control, compliance, and product to provide data support.; Requirements:; Over 5 years of experience in data engineering or data warehousing, with the capability to independently build data platforms.; Familiar with mainstream big data technology stacks, and proficient in performance tuning for Hadoop\/Spark\/Flink (e.g., shuffle optimization, checkpoint mechanism).; Skilled in data modeling (Dimensional\/Star Schema), ETL processes, and data governance.; Preferred background in payments, finance, or internet industries; familiarity with payment data structures is a plus.; Strong communication, collaboration, and business understanding skills, with the ability to work effectively with product, risk, and compliance teams.; Preferred Qualifications:; Experience in data-related roles within fintech or payment companies; Hands-on experience building data platforms or dashboards from scratch.; Familiarity with compliance-related data processing policies and workflows.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85293565","Role":"BIG DATA PLATFORM ENGINEER","Company":"Matrix Process Automation Pte Ltd","Location":"Central Region","Publish_Time":"2025-07-01 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/85293565","job_desc":"Operating Global Data Platform components (VM Servers, Kubernetes, Kafka) and applications (Apache stack, Collibra, Dataiku and similar).; Implement automation of infrastructure, security components, and Continuous Integration & Continuous Delivery for optimal execution of data pipelines (ELT\/ETL).; You have 5+ years of experience in building or designing large-scale, fault-tolerant, distributed systems, (for example: data lakes, delta lakes, data meshes, data lake houses, data platforms, data streaming solutions\u2026); In-depth knowledge and experience in one or more large scale distributed technologies including but not limited to: Hadoop ecosystem, Kafka, Kubernetes, Spark; Migration experience of storage technologies (e.g. HDFS to S3 Object Storage); Integration of streaming and file based data ingestion \/ consumption (Kafka, Control M, AWA); Experience in DevOps, data pipeline development, and automation using Jenkins and Octopus (optional: Ansible, Chef, XL Release, and XL Deploy); Expert in Python and Java or another static language like Scala\/R, Linux\/Unix scripting, Jinja templates, puppet scripts, firewall config rules setup; VM setup and scaling (pods), K8S scaling, managing Docker with Harbor, pushing Images through CI\/CD","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"84921566","Role":"Data Engineer\/Senior Data Engineer, DXD (Digital Excellence & Products...","Company":"Public Service Division","Location":"Singapore","Publish_Time":"2025-06-17 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/84921566","job_desc":"[What the role is]; The Government Technology Agency (GovTech) is the lead agency driving Singapore\u2019s Smart Nation initiatives and public sector digital transformation. As the Centre of Excellence for Infocomm Technology and Smart Systems (ICT & SS), GovTech develops the Singapore Government\u2019s capabilities in Data Science & Artificial Intelligence, Application Development, Smart City Technology, Digital Infrastructure, and Cybersecurity.; At GovTech, we offer you a purposeful career to make lives better. We empower our people to master their craft through continuous and robust learning and development opportunities all year round. Our GovTechies embody our Agile, Bold and Collaborative values to deliver impactful solutions.; GovTech aims to transform the delivery of Government digital services by taking an \"outside-in\" view, putting citizens and businesses at the heart of everything we do.; Play a part in Singapore\u2019s vision to build a Smart Nation and embark on your meaningful journey to build tech for public good. Join us to advance our mission and shape your future with us today!; Learn more about GovTech at tech.gov.sg.; [What you will be working on]; ; \"To Mould the Future of Our Nation\"; At MOE, we believe in enabling every learner to thrive in a rapidly changing world. As part of our mission, we are building internal AI capabilities to improve student learning outcomes, enhance educator productivity, and strengthen our ability to innovate sustainably. Through the responsible and meaningful application of AI, we aim to advance personalised learning, support teaching, and transform educational operations.; As a Data Engineer, you will play a key role in shaping MOE\u2019s AI capabilities by leading the evaluation, optimisation, and deployment of AI models for education. You will partner closely with product managers, engineers, curriculum specialists, and policy teams to design solutions that are pedagogically relevant, technically robust, and ready for scale.; Our Team; You will be part of the Digital Excellence & Products Division (DXD), a cross-functional team driving MOE\u2019s digital transformation across platforms, policies, and products. Our Forward-Deployed AI\/Data Science Team focuses on rapidly applying AI to real-world problems in education \u2014 from personalised learning and curriculum support to school operations.; What you will be working on:; Translate data requirements from business users into technical specifications.; Collaborate with partner agency\u2019s IT teams on technology stack, infrastructure and security alignment.; Build out data product as part of a data team:; Architect and build ingestion pipelines to collect, clean, merge, and harmonize data from different source systems.; Day-to-day monitoring of databases and ETL systems, e.g., database capacity planning and maintenance, monitoring, and performance tuning; diagnose issues and deploy measures to prevent recurrence; ensure maximum database uptime;; Construct, test, and update useful and reusable data models based on data needs of end users.; Design and build secure mechanisms for end users and systems to access data in data warehouse.; Research, propose and develop new technologies and processes to improve agency data infrastructure.; Collaborate with data stewards to establish and enforce data governance policies, best practices and procedures.; Maintain data catalogue to document data assets, metadata and lineage.; Implement data quality checks and validation processes to ensure data accuracy and consistency.; Implement and enforce data security best practices, including access control, encryption, and data masking, to safeguard sensitive data; [What we are looking for]; A Bachelor\u2019s Degree, preferably in Computer Science, Software Engineering, Information Technology, or related disciplines. ; Deep understanding of system design, data structure and algorithms, data modelling, data access, and data storage.; Demonstrated ability in using cloud technologies such as AWS, Azure, and Google Cloud.; Experience in architecting data and IT systems.; Experience with orchestration frameworks such as Airflow, Azure Data Factory.; Experience with distributed data technologies such as Spark, Hadoop.; Proficiency in programming languages such as Python, Java, or Scala.; Proficiency in writing SQL for databases\\; Familiarity with building and using CI\/CD pipelines.; Familiarity with DevOps tools such as Docker, Git, Terraform.; Preferred requirements:; Experience in designing, building, and maintaining batch and real-time data pipelines. ; Experience with Databricks.; Experience with implementing technical processes to enforce data security, data quality, and data governance.; Familiarity with government systems and government's policies relating to data governance, data management, data infrastructure, and data security.; Our employee benefits are based on a total rewards approach, offering a holistic and market-competitive suite of perks. These include leave benefits to meet your work-life needs and employee wellness programmes. ; We champion flexible work arrangements (subject to your job role) and trust that you will manage your own time to deliver your best, wherever you are, and whatever works best for you. ; Learn more about life inside GovTech at go.gov.sg\/GovTechCareers.; Stay connected with us on social media at go.gov.sg\/ConnectWithGovTech.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85187458","Role":"Big Data","Company":"Goldtech Resources Pte Ltd","Location":"Singapore","Publish_Time":"2025-06-26 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/85187458","job_desc":"Big Data (Full Time); Location In Singapore: Islandwide; Salary(SGD): 6000 - 10000; Apply; Job Description:; \u00b7 Analysing requirements, designing, developing, testing & supporting application and Data warehouses from build to production (including proof-of-concept); \u00b7   Ability to develop test plans and lead testing cycles.; \u00b7  Provide product and application support and maintenance.; \u00b7   Able to communicate pros and cons of different options to end users.; \u00b7 Leads in the assessment of the customer environment and the design of the solution architecture based on the requirements specifications.; \u00b7     Ensures appropriate documentation, customer involvement and sign-off.; \u00b7    Develop development framework and assign development effort to team members.; \u00b7    Actively leads and manages team members to a successful project.; Requirements:; \u00b7 Diploma\/Degree in Computer Science, Information Technology or equivalent.; \u00b7 Have 5 - 8 years\u2019 experience in installation and configuration of software in Linux\/Microsoft platforms with the understanding of security hardening, firewall and etc.; \u00b7Have 5 - 8 years\u2019 experience in Business Intelligence\/Data Warehouse\/Analytics Projects involving in requirements gathering designing, development, deployment, conducting knowledge transfer and post deployment support.; \u00b7  Have 5 - 8 years\u2019 experience familiar with Data Platform\/ETL\/Business Intelligence technologies such as:; o  SAP BI\/BODS; o  Tableau; o  Talend; o  MapR\/Cloudera\/Hortonworks platform; \u00b7   Preferably have experience actively leading and managing a team of 3 to 5 members.; \u00b7  Independent with ability to work effectively in a team and who takes initiative and engages their colleagues.; \u00b7Excellent communication and interpersonal skills with ability to communicate with clarity and confidence with colleagues and customers.; \u00b7 Likes technology, taking initiative to learn more and share knowledge with juniors and within the team.; \u00b7 Proven abilities to take initiative, innovative and the ability to develop creative solutions for challenging client needs.; \u00b7 Knowledge in any of following tools or technologies are not mandatory but will be an added advantage:; o  Data Modelling using dimensional modelling techniques (e.g. star and snow flake schemas) and designing the metadata layer for self-service analytics; o  SAP related skillsets S\/4 HANA, SAP Analytics Cloud, SAP BW; o  Cloud and network concepts; o  Databases such as MariaDB, MySQL, PostgreSQL; Apply","salary":"","work_type":"","country":"singapore"}
{"Job_ID":"85430135","Role":"Data Architect \u2013 SAS VI","Company":"THAKRAL ONE PTE LTD","Location":"Singapore","Publish_Time":"2025-07-03 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/85430135","job_desc":"Role; Data Architect \/ Data Management Lead \u2013 SAS (Fraud & Investigation Focus); Job Description; We are seeking a highly experienced Data Architect \/ Data Management Lead with strong domain knowledge in fraud analytics, case management, and investigation\/intelligence systems. The ideal candidate will lead the design, development, and deployment of data integration and analytics components within SAS environments, with hands-on exposure to SAS Viya, data pipelines, and real-time streaming tools. This role is ideal for someone who thrives at the intersection of data strategy, architecture, and stakeholder collaboration.; Key Responsibilities; Collaborate with business analysts to gather and interpret data-related requirements for fraud\/case management\/investigation solutions.; Define and implement a comprehensive data architecture, including data sourcing, integration, cleansing, storage, and provisioning strategies.; Translate business requirements into technical work products across the ETL, data engineering, and reporting layers.; Design and lead batch and real-time data ingestion processes from various sources into SAS or big data platforms.; Develop data pipelines, orchestration frameworks, and visualization\/reporting structures using tools such as SAS Viya, DS2, and Python.; Lead integration and testing of developed components, and support UAT activities.; Oversee documentation, project tracking, and technical troubleshooting.; Ensure compliance with data governance, metadata, and quality management standards.; Experience and Skills Requirements; 10+ years of relevant experience in Business Analytics, Data Architecture, or Data Warehousing.; Proven end-to-end delivery of at least 5 enterprise projects, preferably involving SAS solutions in fraud, surveillance, or case management domains.; Strong hands-on experience in:; SAS Viya, SAS DS2, SQL Programming; Python, Spark, JSON, XML; Job scheduling, Linux commands, data streaming tools (Kafka\/SAS ESP); Postman\/SOAPUI, REST APIs, DevOps\/DataOps; Working knowledge of:; Data model design, source-to-target mapping; Real-time event processing, metadata management, and orchestration; Background in fraud analytics, investigative systems, or intelligence\/case management frameworks is essential.; Familiarity with tools such as SAS Intelligent Decisioning, RTDM, PEGA, or similar is a strong plus.; Experience working in government, public sector, or regulated environments is highly preferred.; Strong communication, stakeholder management, and leadership skills.; Degree in Computer Science, IT, Statistics, or related field.; Number of Vacancies; 1; Philippines","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85507921","Role":"Data Engineer","Company":"NOVADE SOLUTIONS PTE LTD","Location":"Singapore","Publish_Time":"2025-06-21 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/85507921","job_desc":"Headquartered in Singapore, Novade is a leading field management platform designed to digitalize processes on construction and industrial sites. We help clients manage progress, quality, and safety with our cutting-edge solutions, empowering teams to achieve better performance and compliance. With millions of records processed daily, Novade transforms how teams collaborate and leverage data to drive success.; About Our Data Team; Our Data Team is at the forefront of innovation, with missions including:; Developing scalable data pipelines that process millions of records, offering powerful, customizable analytics to help clients improve their performance; Training predictive models to detect risks; Developing Generative AI use cases to push boundaries in digital transformation; Managing internal analytics to enable data-driven decisions for our teams and delivering top-tier consulting to our clients; Key Responsibilities; Design, develop, and maintain data pipelines to ensure efficient data flow and high data quality; Implement scalable data models and visualizations for our clients; Perform data analysis on client and business data; Communicate findings and insights effectively through presentations and reports; Conduct feature engineering for predictive analytics; Create beautiful and efficient dashboards to visualize data and derive actionable insights; Occasionally, train machine learning models once other tasks have been mastered; What You Need for This Position; Technical skills:; Proficiency in Python (including PySpark, Pandas, NumPy); Strong SQL skills; Experience with building ETL pipelines; Experience with BI tools, preferably Power BI; Familiarity with machine learning techniques; Strong analytical skills with an understanding of statistical principles; Knowledge of Databricks or Apache Airflow is a plus; Computer science skills are a plus; Soft skills:; Autonomy and initiative \u2014 you know when to dive deep and when to seek help; Reliability, particularly in managing production pipelines and reacting swiftly to issues; Team spirit and adaptability; Eagerness to learn and grow; Communication and presentation abilities; Experience (internship) with SaaS platforms is a plus; What You Will Get With Us; Learn and contribute to the development of a state-of-the-art, scalable data platform; Be part of a fun and rapidly growing company making waves in the industry; Work alongside a dynamic team of young but experienced professionals; Experience a culture of respect, collaboration, and innovation; Make a tangible impact\u2014join us in revolutionizing the digital landscape of the construction industry; Gain access to a unique dataset within the construction sector, unlocking invaluable learning opportunities; If you\u2019re passionate about data engineering, eager to work on impactful projects, and ready to learn from a team of experts, we\u2019d love to hear from you!","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85190498","Role":"Data Quality Assurance Engineer - Data Platform 2025 Start","Company":"BYTEDANCE PTE. LTD.","Location":"Singapore","Publish_Time":"2025-06-26 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/85190498","job_desc":"Data Quality Assurance Engineer - Data Platform 2025 Start; Singapore Regular R&D - Testing Job ID: A184326; Responsibilities; About ByteDance Founded in 2012, ByteDance's mission is to inspire creativity and enrich life. With a suite of more than a dozen products, including TikTok as well as platforms specific to the China market, including Toutiao, Douyin, and Xigua, ByteDance has made it easier and more fun for people to connect with, consume, and create content. Why Join; Us Creation is the core of ByteDance's purpose. Our products are built to help imaginations thrive. This is doubly true of the teams that make our innovations possible.; Together, we inspire creativity and enrich life; a mission we aim towards achieving every day.; To us, every challenge, no matter how ambiguous, is an opportunity; to learn, to innovate, and to grow as one team. Status quo? Never.; Courage? Always. At ByteDance, we create together and grow together.; That's how we drive impact-for ourselves, our company, and the users we serve. Join us. About the team; The mission of the Data Business Partner (BP) Quality Assurance Team is to build a highly robust(standardized\/professional\/efficient) data quality system, ensuring the better delivery of data. Our goal is to safeguard the quality of data throughout the entire lifecycle in terms of timeliness, accuracy, and stability, empowering the enterprise's decision-making capabilities, market competitiveness, and operational efficiency. Team members possess rich backgrounds in big data technology and quality experience, along with keen data insight and a forward-looking technological perspective.; We look forward to your contribution and joining our team What you will be doing:; Responsible for quality assurance work in offline\/real-time data warehouses and data engineering, building a quality assurance system.; Develop comprehensive test plans and testing strategies based on actual business requirements, ensuring data accuracy and maintaining data engineering quality.; Quickly develop testing tools or platforms based on testing needs to improve business delivery efficiency.; Able to identify data system risks comprehensively, ensuring data system stability through means such as performance testing and online monitoring.; In-depth understanding of the business, communicate and collaborate with various roles in data business, create value for the business, and work without boundaries.; Update software, enhances existing software capabilities and develops and direct software testing and validation procedures.; Qualifications; Minimum Qualifications:; Bachelor's degree or above in computer science, mathematics, statistics, or related fields.; At least 3 years of experience in testing development\/development work, with a preference for experience in big data testing and data engineering service testing.; 3 years experience in using one programming language (JAVA\/Go\/python) for tool development, skilled in using SQL, and preferably with independent experience in platform tool development.; Familiarity with common big data technologies such as Yarn, Spark, HDFS, understanding of data ETL processes, good literacy and foundation in mathematical analysis; Strong initiative, sense of responsibility, and ability to work under pressure are required.; Preferred Qualifications:; 5 years of experience in testing development\/development work, with a preference for experience in big data testing and data engineering service testing.; 5+ years of experience working in a complex, matrixed organization involving cross-functional, and\/or cross-business projects; Experiences in data platform related product development or big data technologies (such as Hadoop, Clickhouse, Flink etc.); Sensitivity to data, excellent logical thinking, analytical skills, business understanding, communication skills, and presentation abilities.; Solid background in statistics, data mining, and modeling are preferred.; ByteDance is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives. Our platform connects people from across the globe and so does our workplace. At ByteDance, our mission is to inspire creativity and enrich life. To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach. We are passionate about this and hope you are too.; Apply; Share to","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85172239","Role":"Lead Big Data Engineer","Company":"PLOY ASIA PTE. LTD.","Location":"Singapore","Publish_Time":"2025-06-25 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/85172239","job_desc":"What's on offer:; Location: Singapore; Client: End client user environment; ; Job Summary:; We are seeking a highly skilled and motivated Lead Big Data Engineer to join our data team. The ideal candidate will play a key role in designing, developing, and maintaining scalable big data solutions while providing technical leadership. This role will also support strategic Data Governance initiatives, ensuring data integrity, privacy, and accessibility across the organization.; Key Responsibilities:; Design, implement, and optimize robust data pipelines and ETL\/ELT workflows using SQL and Python.; Lead architecture discussions, including the creation and review of Entity Relationship Diagrams (ERDs) and overall system design.; Collaborate closely with Data Engineers, Analysts, and cross-functional engineering teams to meet evolving data needs.; Deploy and manage infrastructure using Terraform and other Infrastructure-as-Code (IaC) tools.; Develop and maintain CI\/CD pipelines for deploying data applications and services.; Leverage strong experience in AWS services (e.g., S3, Glue, Lambda, RDS, Lake Formation) to support scalable and secure cloud-based data platforms.; Handle both batch and real-time data processing effectively.; Apply best practices in data modeling and support data privacy and data protection initiatives.; Implement and manage data encryption and hashing techniques to secure sensitive information.; Ensure adherence to software engineering best practices including version control, automated testing, and deployment standards.; Lead performance tuning and troubleshooting for data applications and platforms.; ; Required Skills & Qualifications:; Strong proficiency in SQL for data modeling, querying, and transformation.; Advanced Python development skills with an emphasis on data engineering use cases.; Hands-on experience with Terraform for cloud infrastructure provisioning.; Proficiency with CI\/CD tools, particularly GitHub Actions.; Deep expertise in AWS cloud architecture and services.; Demonstrated ability to create and evaluate ERDs and contribute to architectural decisions.; Strong communication and leadership skills with experience mentoring engineering teams.; ; Preferred Skills:; Familiar AI\/ML RAG (Retrieval-Augmented Generation) MCP (Multi-Channel Processing) concepts; Understanding of data processing libraries (Pandas, NumPy); Familiarity with cloud platforms (AWS, GCP, or Azure); Knowledge of containerisation (Docker) and orchestration tools; Experience with CI\/CD pipelines; Basic understanding of data structures and algorithms","salary":"","work_type":"Kontrak\/Temporer","country":"singapore"}
{"Job_ID":"85308711","Role":"AI Data Engineer","Company":"InnoCellence Systems Pte Ltd","Location":"Central Region","Publish_Time":"2025-07-01 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/85308711","job_desc":"We are looking for a skilled and experienced AI Data Engineer to join our team. The ideal candidate will be responsible for designing, building, and maintaining robust data pipelines to support the processing and analysis of clinical study and digital device sensor data. As a Data Engineer, you will work closely with data scientists and software engineers to ensure the efficient and reliable flow of data from source systems to analytical tools and platforms.; Responsibilities:; Design, develop, and maintain scalable data pipelines to ingest, transform, and load clinical study data from various sources, including digital device sensors.; Optimize data storage and retrieval processes in cloud-based platforms to ensure high performance and reliability.; Collaborate with data scientists to integrate data processing pipelines with AI-powered algorithms and third-party analytical tools or platforms.; Implement data quality checks and monitoring mechanisms to ensure the integrity and accuracy of the data.; Troubleshoot and resolve issues related to data pipeline performance, reliability, and scalability.; Work closely with software developers, system architects and other cross-functional teams to develop data-driven business solutions.; Stay up-to-date with emerging technologies in AI & Cloud computing and best practices in data engineering to continuously improve data processing pipelines and infrastructure.; Requirements:; Bachelor's or Master's degree in Computer Science, Engineering, or a related field.; Proven experience in designing and building data pipelines using ETL tools and frameworks such as Apache Spark, Apache Beam, or Apache Airflow.; Proficiency in programming languages such as Python, Java, or Scala.; Strong understanding of database systems, data warehousing concepts, SQL and NoSQL.; Experience with AI and Cloud Computing: Hands-on experience with cloud platforms like AWS and familiarity with AI solutions in these environments.; Excellent problem-solving and troubleshooting skills with a strong attention to detail and quality.; Effective communication and collaboration skills with the ability to work in a team environment.; Preferred Qualifications:; Experience with containerization and orchestration tools such as Docker and Kubernetes.; Familiarity with big data technologies such as Hadoop, Hive, or Presto.; Knowledge of distributed computing frameworks such as Apache Hadoop or Apache Spark.; Familiarity with ElasticSearch or AWS OpenSearch is plus; Prior experience working with healthcare or clinical data is a plus.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85675075","Role":"Solutions Engineer - AI Infrastructure","Company":"Cisco Systems (USA) Pte Ltd","Location":"Marina South","Publish_Time":"2025-07-11 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/85675075","job_desc":"What You\u2019ll Do; We are seeking a Solutions Engineer - Artificial Intelligence (AI) to join our dynamic sales team. As an SE (AI), you will drive the adoption of our AI solutions across various industries. You will identify potential clients, understand their specific needs, and provide tailored AI solutions that enhance their business operations. This role requires a deep understanding of AI technologies and experience relaying technical concepts to a diverse audience.; Who You\u2019ll Work With; The Cloud + AI Infrastructure team delivers one scalable strategy with local execution for data center customer transformation and growth. We are the worldwide go-to-market compute and data center networking engine gathering market transitions and allowing sellers to expand growth for customers and Cisco. Alongside our colleagues, Cloud & AI Infrastructure builds the sales strategy, activates sellers and technical communities, and accelerates selling.; Who You Are; You will build awareness, fostering education and driving energy for AI with both internal and external team members, partners, etc. You will also act as a link between technical teams, account teams, executives, partners\/customers. In addition, you will show case AI solutions\/products, its potential and promote its tactical and technical responsibilities. You will play an important role in the design, development and promotion of AI Solutions and address challenges. You will need to travel as needed to meet with clients and attend industry events.; Minimum Qualifications:; 6+ years of technical presales experience selling Compute, Storage, and Network solutions.; Proficiency in Python (must), MySQL, Github, Git, GO, ETL, OLAP, RDBMS, Scribus, AWS, Azure, Google, IBM Cloud, or other platforms and programming languages.; Experienced with databases (Oracle, PostgreSQL, MySQL, MongoDB, Cassandra, Redis, Snowflake, BigQuery) and AI\/ML frameworks (scikit-learn, TensorFlow, PyTorch, Hugging Face).; Ability to provide detailed and consumable documentation of standard methodologies for deployment around application acceleration, automation\/management efficiencies, enterprise edge, and AI\/ML solutions.; Preferred Qualifications; Bachelor's Degree in Computer Science, Computer Engineering, Electrical Engineering, or related field. Advanced degree is a plus.; Excellent presentation skills \u2013 ability to deliver engaging workshops to both technical and non-technical audiences on AI topics; AI experience with Nvidia, IBM, Microsoft, Dell, NetApp, HPE, and\/or other AI vendors; In-depth understanding of language models, including but not limited to GPT-3, BERT, or similar architectures.; Expertise in training and fine-tuning LLMs using popular frameworks such as TensorFlow, PyTorch, or Hugging Face Transformers.; Experience in deploying LLM models in cloud environments (e.g., AWS, Azure, GCP) and on-premises infrastructure.; Familiarity with containerization technologies (e.g., Docker or equivalent experience) and orchestration tools (e.g., Kubernetes) for scalable and efficient model deployment.; We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform crucial job functions, and to receive other benefits and privileges of employment. Please contact us to request accommodation","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"85156727","Role":"Senior Data Architect (Networks)","Company":"Singapore Telecommunications Limited","Location":"Singapore","Publish_Time":"2025-06-18 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/85156727","job_desc":"At Singtel, we believe in the strength of a vibrant, diverse and inclusive workforce where backgrounds, perspectives and life experiences of our people help us innovate and create strong connections with our customers. We strive to ensure all our people practices are non-discriminatory and provide a fair, performance-based work culture that is diverse, inclusive and collaborative. Join us and experience what it\u2019s like to be with an Employer of Choice*. Together, let\u2019s create a brighter digital future for all. *Awarded at the HR Fest Awards 2020.; Singtel Networks, the most established telecommunications infrastructure provider in Singapore is transforming to enable the digital generation of tomorrow. We are introducing new capabilities in 5G, Cloud, Analytics, Digital Commerce, Software Engineering, Cyber Security to enhance our core competencies and deliver innovative and differentiated Mobile and Fixed services (Broadband, TV and Telephony) for our customers. We are committed to celebrating inclusion and diversity and is a strong believer to upskill and nurture all individuals. Come join us today as we build Singtel\u2019s Networks of tomorrow, and Empower Every Generation to live, work and play in new ways!; Make an Impact by:; Lead and manage cloud data lake or solution initiative including design and determine the SaaS or software to be used for the data processing and pipeline for ETL, Streaming, analytics, AI\/ML and APIs.; Establish and lead the Day 2 operation process, SLO\/SLA, data off-premise clearance and security governance for cloud data pipeline.; Manage Networks data governance for the department and support in Group data governance and data protection framework.; Perform as Tier 3 Systems SME\uf0a7    Accountable for overall System Performance and Design.\uf0a7    Accountable for Change Management outcomes, executing minor to major software upgrades as well as solution changes independently.\uf0a7    Responsible to manage vendors and technically debate on optimum solution performance while ensuring robust and cost-efficient architecture. ; Lead and manage Tier 2 Systems Administration & Operation Support.; Accountable for System Security\uf0a7    Management of Anti-Malware systems and perform monthly scanning for any security threats and system administration and perform monthly scanning for systems.\uf0a7    Analyse security reports from Anti-Malware scans, determining best course of action.\uf0a7    Administrator for the firewalls and perform the quarterly review for the rules to ensure the system defence-in-depth and security.; Responsible for Information Security (InfoSec) governance, serving as the Security representative for the department or division. ; Responsible for architecting network and IP address designs, as well as implementing data protection and security measures.; Lead Incident Management and provide timely update to the Management and accountable for the RCA of the managed platforms.; Conduct research and development (R&D) or proof of concept (POC) on new technologies or proposals provided by the vendor.; Ensure operational processes for the respective systems are well documented. This includes system inventories, solution doc, IP\/Network design, SOPs etc.; Lead system audits, oversee security review submissions, and manage the Business Continuity Management (BCM) for the department.;  Skills for Success:; Degree in Engineering or IT.; At least 5 years\u2019 experience in data solution and administration.; Experience in Cloud Data solution (AWS, Microsoft Azure, etc), On-premise data platform on HPE Ezmeral, Stream data platform in Kafka and Confluent, On-premise system implementation, System.; Able to handle Data Architect, mapR, Hadoop and Spark (or equivalent).; Has knowledge in Linux\/Unix, Ansible automation, Shell Scripting, Kubernetes, Docker, serverless functions, APIs and Kafka bus, Network, IP Address and Security design.; Rewards that Go Beyond:; \u2022    Hybrid work arrangements; \u2022    Full suite of health and wellness benefits ; \u2022    Ongoing training and development programs ; \u2022    Internal mobility opportunities; Are you ready to say hello to BIG Possibilities?; Take the leap with Singtel to unlock new opportunities and accelerate your growth. Apply now and start your empowering career!","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"83649507","Role":"Senior Data Engineer","Company":"Mediacorp Pte Ltd","Location":"Singapore","Publish_Time":"2025-06-17 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/83649507","job_desc":"Job Description; PURPOSE OF THIS ROLE; The purpose of the Senior Data Engineer role is to provide strategic leadership in data engineering, overseeing the development and maintenance of data infrastructure, and ensuring the availability, reliability, and performance of data solutions. This position plays a critical role in supporting data-driven decision-making across the organization.; Key responsibilities include:; Providing strategic direction and technical leadership in data engineering.; Leading the design, development, and deployment of ETL processes and data pipelines.; Integrating data from various sources and transforming it for analytics.; Maintaining data quality and implementing data governance best practices.; Managing and mentoring a team of data engineers for professional growth.; Collaborating with stakeholders to understand and fulfil data requirements.; Documenting data processes, architectures, and best practices.; Challenges in this role may include:; Ensuring data security and compliance in a rapidly evolving regulatory environment.; Optimizing data infrastructure for performance and scalability.; Resolving data-related issues efficiently.; Managing a dynamic team in a fast-paced data environment.; FOUNDATIONAL\/LEADERSHIP COMPETENCIES; Strong leadership and team management skills.; Excellent problem-solving and communication skills.; FUNCTIONAL COMPETENCIES; Proficiency in programming languages such as Java, or Scala, SQL, Python.; Extensive experience with ETL tools, data integration, and data transformation.; In-depth knowledge of data storage technologies (relational databases, NoSQL, distributed file systems).; Expertise in data modelling, database design and data warehousing.; Familiarity with data governance, security, and compliance standards.; Experience with big data technologies (e.g., Hadoop, Spark, Hive, Azure, Databricks) is a plus.; Experience in container orchestration framework like Kubernetes is a plus.; Experience in Infrastructure as Code and DevOps, MLOps and DataOps is a plus.; Job Requirements; Bachelor's or Master's degree in Computer Science, Information Technology, or a related field.; Relevant certifications in data engineering and Cloud computing are advantageous.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"84915025","Role":"Vice President, Data Platform Lead, Technology Service Delivery Group","Company":"Sumitomo Mitsui Banking Corporation","Location":"Singapore","Publish_Time":"2025-06-17 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/84915025","job_desc":"Job Responsibilities; \u2022    Oversee the planning and delivery of data platform projects and system changes following the Software Development Lifecycle.; \u2022    Serve as a data architect involved in logical and physical data modeling, collaborating with cross-functional teams to collect requirements, design solutions, and implement data strategies utilizing Azure services such as Data Lake, Synapse, and Databricks.; \u2022    Build data models to support data marts\/enterprise data warehouse.; \u2022    Manage data pipelines and ETL processes to ensure data quality and availability.; \u2022    Ability to influence and steer technical direction.; \u2022    Front technical reviews with technical team.; \u2022    Involve in solutioning discussions with stakeholders.; \u2022    Provide guidance and mentorship to data engineers and analysts.; \u2022    Identify and assess new data technologies and tools, making recommendations for adoption.; \u2022    Ensure adherence to data governance and security policies.; \u2022    Monitor and optimize the performance and scalability of the data platform.; \u2022    Effectively communicate project progress and outcomes to stakeholders.; \u2022    Collaborate with the data management office to identify governance requirements and ensure integration with existing platforms.; \u2022    Implement data management best practices, including data cataloguing, lineage tracking, and metadata management within Azure.; \u2022    Oversee budgeting for data initiatives, ensuring resource allocation aligns with organizational goals.; \u2022    Provide ongoing maintenance and L3 support. ; Job Requirements; \u2022    Bachelor\u2019s degree in information technology, Computer Science, Data Science or equivalent; master\u2019s degree is a plus.; \u2022    Min. 8 years of working experience in management information systems, regulatory reporting, data management or data analytics role within Banks\/Financial Institutions. ; \u2022    Experience in leading mid-sized project implementation.; \u2022    Understand bank product, services, regulatory environment and customer needs;   \u2022    Understand bank traditional product and services for all commercial banking product; \u2022    Proficient in System Development Life Cycle (SDLC)\/Agile project delivery framework.; \u2022    Demonstrated ability to design and implement scalable data solutions on Azure.; \u2022    Excellent analytical, problem-solving, and communication skills.; \u2022    Relevant certifications in Azure (e.g., Microsoft Certified: Azure Data Engineer Associate); \u2022    Experience leading cross-functional teams and managing stakeholder relationships.; \u2022    Experience in Microsoft Azure, data management, data warehousing, data mart, data visualization, design and implementation will be advantageous.; \u2022    Knowledge in Financial Accounting, Management Accounting & Regulatory Reporting, Risk Management, Capital \/ Profitability Management would be a plus.; \u2022    Certifications such as PMP, PRINCE2, SCRUM, ITIL is a plus.","salary":"","work_type":"Full time","country":"singapore"}
{"Job_ID":"82131033","Role":"Platinion Principal Data\/AI Architect","Company":"THE BOSTON CONSULTING GROUP PTE. LTD.","Location":"Singapore","Publish_Time":"2025-06-17 16:18:10","URL":"https:\/\/id.jobstreet.com\/id\/job\/82131033","job_desc":"Who We AreBoston Consulting Group partners with leaders in business and society to tackle their most important challenges and capture their greatest opportunities. BCG was the pioneer in business strategy when it was founded in 1963.; Today, we help clients with total transformation-inspiring complex change, enabling organizations to grow, building competitive advantage, and driving bottom-line impact. To succeed, organizations must blend digital and human capabilities. Our diverse, global teams bring deep industry and functional expertise and a range of perspectives to spark change.; BCG delivers solutions through leading-edge management consulting along with technology and design, corporate and digital ventures\u2014and business purpose. We work in a uniquely collaborative model across the firm and throughout all levels of the client organization, generating results that allow our clients to thrive. About BCG Platinion BCG Platinion's presence spans across the globe, with offices in Asia, Europe, and South and North America.; We achieve digital excellence for clients with sustained solutions to the most complex and time-sensitive challenge. We guide clients into the future to push the status quo, overcome tech limitations, and enable our clients to go further in their digital journeys than what has ever been possible in the past. At BCG Platinion, we deliver business value through the innovative use of technology at a rapid pace.; We roll up our sleeves to transform business, revolutionize approaches, satisfy customers, and change the game through Architecture, Cybersecurity, Digital Transformation, Enterprise Application and Risk functions. We balance vision with a pragmatic path to change transforming strategies into leading-edge tech platforms, at scale. Practice; Area BCG Platinion launched in Germany in 2000 to add deep technical expertise to the Boston Consulting Group\u2019s existing capabilities. Today, our presence spans across the globe, with offices in Asia, Europe, and South and North America. Our; New York-based North American team began in 2014 and in 2017 acquired MAYA Design, a Pittsburgh-based digital design and innovation lab, to grow our capabilities around technology and design.  We support our clients\u2019 total digital transformation through technology, design, cybersecurity, and risk management & financial engineering capabilities. And together with BCG, BCG Platinion\u2019s interdisciplinary team of technical experts enable customized technical solutions and accelerate delivery value through new business platforms, application consolidations, and major system implementations.; What You'll DoPosition OverviewAbout this role We are seeking a highly skilled Senior Data\/AI Architect to provide strategic guidance on designing, developing, and optimizing modern data architectures that support advanced analytics, AI\/ML, and real-time data processing. This role emphasizes advisory and leadership, helping organizations define future-ready data strategies and governance frameworks that enable AI-driven use cases.; The ideal candidate will work closely with data engineers, data scientists, business analysts, and IT leaders to shape scalable, secure, and efficient data ecosystems.  What You; Will Do Data Architecture & StrategyDefine and advise on the design of modern, future-ready data architectures that align with business goals and support AI, analytics, and automation. Guide organizations in adopting cloud-native and hybrid data solutions (AWS, Azure, GCP, Snowflake, Databricks).; Provide thought leadership on best practices for data modeling, data warehousing, and lakehouse architectures to ensure scalability and performance. Shape long-term data strategies that foster flexibility, innovation, and interoperability across platforms.  Data Governance & AdvisoryLead the development of governance frameworks and policies that ensure data security, compliance, and ethical AI use.; Provide guidance on the creation and maintenance of data dictionaries, metadata management, and data cataloging, ensuring consistency, accuracy, and alignment with industry best practices. Advise on data quality management strategies, ensuring robust data lineage, accuracy, and reliability across the organization. Define governance roadmaps that support AI adoption while maintaining compliance with GDPR, CCPA, HIPAA, and other regulations.;  Cloud & Emerging TechnologiesProvide strategic recommendations on leveraging data lakes, data meshes, and serverless architectures to optimize data processing and storage. Advise on implementing real-time streaming solutions (Kafka, Kinesis, Pub\/Sub) to support AI-driven analytics. Assess and recommend AI\/ML-enabled data architectures that facilitate scalable feature engineering and model training pipelines.; Guide organizations in evaluating and adopting graph databases, NoSQL solutions, and modern data integration tools.  Collaboration & LeadershipAct as a trusted advisor to C-level executives and business leaders, translating complex data challenges into strategic initiatives. Collaborate with data scientists, engineers, and business analysts to enhance data accessibility and usability.; Drive innovation in data architecture, ensuring organizations remain competitive and AI-ready. Lead assessments of emerging data technologies and best practices to future-proof organizational data strategies.  What; You'll BringRequired Skills & QualificationsEducation: Bachelor\u2019s or Master\u2019s in Computer Science, Data Science, Information Systems, or a related field. Experience: 10-16 years in data architecture, data engineering, or cloud-based data solutions. Flexibility to travel within SEA\/Asia Pacific region Technical & Advisory Expertise:Deep knowledge of cloud data platforms (AWS Redshift, Azure Synapse, Google BigQuery, Snowflake).; Expertise in data governance, master data management (MDM), and compliance frameworks. Strong understanding of AI-ready data architectures and their impact on feature engineering and ML workflows. Ability to guide organizations on ETL\/ELT strategy, data integration, and workflow automation.; Familiarity with industry-standard data architecture frameworks (TOGAF, Zachman). Experience advising on real-time data streaming (Kafka, Kinesis, Pub\/Sub).  Preferred QualificationsCertifications in cloud data platforms (AWS Certified Data Analytics, Azure Data Engineer, GCP Professional Data Engineer).; Experience advising on Data Mesh and Data Fabric architectures. Knowledge of Graph databases and NoSQL solutions (MongoDB, Neo4j, Cassandra). Background in data ethics, responsible AI, and AI governance frameworks.;  Why Join Us? Influence enterprise data strategies at a global scale.; Work in a collaborative, innovative environment that values advisory expertise. Competitive compensation and benefits. Lead transformational data initiatives that shape AI adoption and digital innovation.;  If you are passionate about advising organizations on future-ready data architectures and driving AI-enabled data strategies, we invite you to apply! Boston Consulting Group is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, age, religion, sex, sexual orientation, gender identity \/ expression, national origin, disability, protected veteran status, or any other characteristic protected under national, provincial, or local law, where applicable, and those with criminal histories will be considered in a manner consistent with applicable state and local laws.; BCG is an E - Verify Employer. Click here for more information on E-Verify.","salary":"","work_type":"Full time","country":"singapore"}
